{"posts":[{"title":"机器学习实战（九）：线性回归","content":"线性回归 一、形式化定义 在统计学中，线性回归是一种线性方法，用于建模标量响应与一个或多个解释变量之间的关系。 当只有一个解释变量时，我们称之为简单线性回归： 数据假设：yi∈R模型假设：yi=xiTw+ϵiϵi∼N(0,σ2),yi∣xi∼N(xiTw,σ2)\\begin{aligned} &amp;数据假设：y_i\\in R\\\\ &amp;模型假设：y_i=x_i^Tw+\\epsilon_i\\\\ &amp;\\epsilon_i\\sim N(0,\\sigma^2),y_i|x_i\\sim N(x_i^Tw,\\sigma^2) \\end{aligned} ​数据假设：yi​∈R模型假设：yi​=xiT​w+ϵi​ϵi​∼N(0,σ2),yi​∣xi​∼N(xiT​w,σ2)​ 根据对分布的模型假设，我们最终得到的条件概率为： P(yi∣xi,w)=12πσ2e−(xiTw−yi)22σ2P(y_i|x_i,w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_i^Tw-y_i)^2}{2\\sigma^2}} P(yi​∣xi​,w)=2πσ2​1​e−2σ2(xiT​w−yi​)2​ 我们假设模型是一条过原点的直线（类似于感知机通过升维即可使得函数过原点）而对于每个特征xix_ixi​，它的标签yiy_iyi​则从一个平均值为wTxiw^Tx_iwTxi​，方差为σ2σ^2σ2的高斯分布中得出，我们在线性回归建模中的任务便是根据数据集估计斜率www。 单标量预测变量xxx和单标量响应变量yyy的最简单情况称为简单线性回归，而多个预测变量xxx参与的预测为多元线性回归，几乎所有现实世界的回归模型都涉及多个预测因子，线性回归的基本描述通常用多元回归模型来表述。 二、参数估计 线性回归模型中的重要参数就是“斜率”www，我们同样可以通过MLEMLEMLE和MAPMAPMAP两种方式对它进行估计。 进行下列推导之前，我们先了解几个常用的矩阵求导公式： ∂Ax∂x=AT,∂xTA∂x=A,∂ATxB∂x=ABT∂ATxTB∂x=BAT,∂ATxxTB∂x=(ABT+BAT)x,∂xTAx∂x=2Ax\\frac{\\partial Ax}{\\partial x}=A^T,\\frac{\\partial x^TA}{\\partial x}=A,\\frac{\\partial A^TxB}{\\partial x}=AB^T\\\\\\frac{\\partial A^Tx^TB}{\\partial x}=BA^T,\\frac{\\partial A^Txx^TB}{\\partial x}=(AB^T+BA^T)x,\\frac{\\partial x^TAx}{\\partial x}=2Ax ∂x∂Ax​=AT,∂x∂xTA​=A,∂x∂ATxB​=ABT∂x∂ATxTB​=BAT,∂x∂ATxxTB​=(ABT+BAT)x,∂x∂xTAx​=2Ax 再了解一些矩阵的相关性质： (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT(A^T)^T=A,(A+B)^T=A^T+B^T,(AB)^T=B^TA^T (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT 2.1 极大似然估计MLE w^MLE=argmaxw P(Y,X∣w)=argmaxw ∏i=1nP(yi,xi∣w) =argmaxw ∏i=1nP(yi∣xi,w)P(xi∣w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡12πσ2e−(wTxi−yi)22σ2 =argminw ∑i=1n(xiTw−yi)2=argminw 1n∑i=1n(xiTw−yi)2\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y,X|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)P(x_i|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(w^Tx_i-y_i)^2}{2\\sigma^2}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2 \\end{aligned} ​w^MLE​=wargmax​ P(Y,X∣w)=wargmax​ i=1∏n​P(yi​,xi​∣w) =wargmax​ i=1∏n​P(yi​∣xi​,w)P(xi​∣w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log2πσ2​1​e−2σ2(wTxi​−yi​)2​ =wargmin​ i=1∑n​(xiT​w−yi​)2=wargmin​ n1​i=1∑n​(xiT​w−yi​)2​ 我们可以发现，线性回归问题的极大似然估计结果与最小二乘法（OLS）的形式完全相同。 此外我们还可以推导出www的闭合形式解，这样会使得计算更加方便：根据上述估计值，我们可以定义损失函数l(w)l(w)l(w) l(w)=(Xw−Y)2\\begin{aligned} &amp;l(w)=(Xw-Y)^2 \\end{aligned} ​l(w)=(Xw−Y)2​ 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www，XXX为n×dn\\times dn×d维矩阵，www为d×1d\\times 1d×1维矩阵，YYY为n×1n\\times1n×1维矩阵： l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂l(w)∂w=2XTXw−2XTY=0则有：w=(XTX)−1XTY\\begin{aligned} &amp;l(w)=(Xw-Y)^T(Xw-Y)=(w^TX^T-Y^T)(Xw-Y)\\\\ &amp;~~~~~~~=w^TX^TXw-w^TX^TY-Y^TXw-Y^TY\\\\ &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY=0\\\\ &amp;则有：w=(X^TX)^{-1}X^TY \\end{aligned} ​l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂w∂l(w)​=2XTXw−2XTY=0则有：w=(XTX)−1XTY​ 2.2 极大后验估计MAP 引入一个附加的模型假设： P(w)=12πτ2e−wTw2τ2P(w)=\\frac1{\\sqrt{2\\pi\\tau^2}}e^{-\\frac{w^Tw}{2\\tau^2}} P(w)=2πτ2​1​e−2τ2wTw​ 则极大后验估计的过程为： w^MAP=argmaxw P(w∣Y,X)=argmaxw P(Y,X∣w)P(w)P(Y,X) =argmaxw P(Y,X∣w)P(w)=argmaxw ∏i=1nP(yi,xi∣w)P(w) =argmaxw ∑i=1nlog⁡P(yi,xi∣w)+log⁡P(w) =argmaxw −∑i=1n(xiTw−yi)2−12τ2wTw =argminw ∑i=1n(xiTw−yi)2+λ∣∣w∣∣22\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|Y,X)=\\underset{w}{argmax}~\\frac{P(Y,X|w)P(w)}{P(Y,X)}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~P(Y,X|w)P(w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i,x_i|w)+\\log P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\sum_{i=1}^n(x_i^Tw-y_i)^2-\\frac{1}{2\\tau^2}w^Tw\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_2^2 \\end{aligned} ​w^MAP​=wargmax​ P(w∣Y,X)=wargmax​ P(Y,X)P(Y,X∣w)P(w)​ =wargmax​ P(Y,X∣w)P(w)=wargmax​ i=1∏n​P(yi​,xi​∣w)P(w) =wargmax​ i=1∑n​logP(yi​,xi​∣w)+logP(w) =wargmax​ −i=1∑n​(xiT​w−yi​)2−2τ21​wTw =wargmin​ i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣22​​ 我们将这一回归称为岭回归，它同样存在闭合形式：我们定义损失函数为： l(w)=(Xw−Y)2+λwTwl(w)=(Xw-Y)^2+\\lambda w^Tw l(w)=(Xw−Y)2+λwTw 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www： ∂l(w)∂w=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY+2\\lambda Iw=0\\\\ &amp;则有：w=(X^TX+\\lambda I)^{-1}X^TY \\end{aligned} ​∂w∂l(w)​=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY​ 三、模型实现 对于简单的线性回归模型，我们通过闭合形式可以直接求解，将分别通过手动实现和调库的方式构造模型。 3.1 数据集 我们使用波士顿房价数据集，查看数据集的内容并对其进行存储依靠以下部分代码： 波士顿房价数据集有506个样本，每个样本有13个特征，接下来我们利用线性回归模型对数据集进行拟合。 3.2 手动实现模型 我们的评估指标选择r2r2r2系数，其定义如下： R2=1−∑i=1n(yi−yi^)2∑i=1n(yi−y‾)2=1−MSEVarR^2=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{\\sum_{i=1}^n(y_i-\\overline{y})^2}=1-\\frac{MSE}{Var} R2=1−∑i=1n​(yi​−y​)2∑i=1n​(yi​−yi​^​)2​=1−VarMSE​ 显然，预测越准r2r2r2系数越趋近于1。 3.3 调库实现模型 调库主要用到的是sklearn中的LinearRegression模型，其具体参数请查阅文档 ","link":"https://2006wzt.github.io/post/机器学习实战（九）：线性回归/"},{"title":"机器学习实战（八）：梯度下降","content":"梯度下降 一、基本思想 1.1 知识回顾 我们首先回顾逻辑回归模型的参数估计： 对于给定的数据集D={(x1,y1),(x2,y2),...,(xn,yn)},xi∈Rd,yi∈{0,1}w^MLE=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))w^MAP=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;对于给定的数据集D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\},x_i\\in R^d,y_i\\in\\{0,1\\}\\\\ &amp;\\hat{w}_{MLE}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;\\hat{w}_{MAP}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​对于给定的数据集D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)},xi​∈Rd,yi​∈{0,1}w^MLE​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))w^MAP​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 由此我们得到两个估计过程的损失函数： l(w)MLE=∑i=1nlog⁡(1+e−yi(wTxi+b))l(w)MAP=∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;l(w)_{MLE}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;l(w)_{MAP}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​l(w)MLE​=i=1∑n​log(1+e−yi​(wTxi​+b))l(w)MAP​=i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 我们的目标是最小化一个凸的、连续的、可微的损失函数l(w)l(w)l(w)。 凸函数定义为： 对∀t∈[0,1],若f(x)满足:f(tx1+(1−t)x2)≤tf(x1)+(1−t)f(x2),则称f(x)为凸函数对\\forall t\\in[0,1],若f(x)满足:f(tx_1+(1-t)x_2)\\le tf(x_1)+(1-t)f(x_2),则称f(x)为凸函数 对∀t∈[0,1],若f(x)满足:f(tx1​+(1−t)x2​)≤tf(x1​)+(1−t)f(x2​),则称f(x)为凸函数 ①l(w)l(w)l(w)是凸函数：这使得我们找到的局部最小值也是全局最小值 ②l(w)l(w)l(w)至少三次连续可微：我们将广泛得使用泰勒近似，这个假设大大简化了讨论 ③在www上没有设置任何约束，添加约束会增加问题的复杂程度，我们在这里将不讨论 1.2 局部最小值 1.2.1 定义 定义：我们将w∗w^*w∗称为l(w)l(w)l(w)的一个局部最小值，如果有： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)≤l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)\\le l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)≤l(w) LPLPLP范数：∣∣x∣∣p=(∑i=1n∣xi∣p)1p||x||_p=(\\sum_{i=1}^n |x_i|^p)^{\\frac1p}∣∣x∣∣p​=(∑i=1n​∣xi​∣p)p1​ 因为我们之前有l(w)l(w)l(w)是凸函数的假设，所以找到的局部最小值w∗w^*w∗也是全局最小值，有： ∀w∈Rd,都有l(w∗)≤l(w)\\forall w\\in R^d,都有l(w^*)\\le l(w) ∀w∈Rd,都有l(w∗)≤l(w) 当满足如下条件时，我们称w∗w^*w∗为l(w)l(w)l(w)的一个严格的局部最小值： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)&lt;l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)&lt; l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)&lt;l(w) 注意不是所有的凸函数都有严格的局部最小值，比如l(w)=1l(w)=1l(w)=1 1.2.2 必要条件 一个点w∗w^*w∗成为局部最小值的必要条件为： ∇l(w∗)=0→\\nabla l(w^*)=\\overrightarrow0 ∇l(w∗)=0 ∇为梯度算子，∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]T\\nabla为梯度算子，\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T ∇为梯度算子，∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 如果点w∗w^*w∗满足上述必要条件，则该点成为严格的局部最小值的充分条件是HessianHessianHessian矩阵为正定的： 即：∇2l(w∗)=H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]是正定的。即：\\nabla^2l(w^*)=H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right]是正定的。 即：∇2l(w∗)=H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​是正定的。 正定矩阵的定义为： 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的设A是n阶方阵，即A\\in R^{n\\times n},如果对于任何非零向量X\\in R^n都有：X^TA~X&gt;0,那么A是正定的 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的 二、泰勒展开 为了理解更新过程中的损失函数变化，我们对损失函数进行泰勒展开： 2.1 一阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]Tl(w+\\Delta w)=l(w)+g(w)^T\\Delta w\\\\ g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 一阶泰勒展开对应于线性近似： 在Δw\\Delta wΔw很小时，这些近似是合理可靠的，线性近似的误差为： O(∣∣Δw∣∣22)=∑i=1dΔwi2O(||\\Delta w||_2^2)=\\sum_{i=1}^d\\Delta w_i^2 O(∣∣Δw∣∣22​)=i=1∑d​Δwi2​ 2.2 二阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔw+12ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j=∂2l(w)∂wi∂wjl(w+\\Delta w)=l(w)+g(w)^T\\Delta w+\\frac12\\Delta w^TH(w)\\Delta w\\\\ H(w)为Hessian矩阵，[H(w)]_{i,j}=\\frac{\\partial^2 l(w)}{\\partial w_i\\partial w_j} l(w+Δw)=l(w)+g(w)TΔw+21​ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j​=∂wi​∂wj​∂2l(w)​ 二阶泰勒展开对应于二次近似： 二次近似的误差为： O(∣∣Δw∣∣23)=(∑i=1nΔwi2)32O(||\\Delta w||_2^3)=(\\sum_{i=1}^n\\Delta w_i^2)^\\frac32 O(∣∣Δw∣∣23​)=(i=1∑n​Δwi2​)23​ 三、搜索方向方法 我们求解问题的目标是：寻找w∗w^*w∗使得l(w)l(w)l(w)取得最小值，我们需要研究寻找w∗w^*w∗的方向。 3.1 核心思想 给定一个初始点w0w^0w0，寻找一个迭代序列：w0→w1→...→wkw^0\\rightarrow w^1\\rightarrow...\\rightarrow w^kw0→w1→...→wk，wkw^kwk表示第kkk次迭代找到的www，我们希望：k→∞,wk→w∗k\\rightarrow\\infty,w^k\\rightarrow w^*k→∞,wk→w∗ 我们的目的就是寻找一个步长sss用于对www进行更新，更新过程： wk+1=wk+sw^{k+1}=w^k+s wk+1=wk+s 由此我们得到了一个梯度下降过程的伪代码： 根据上述算法，我们便需要解决以下两个问题： ①如何寻找一个合理的步长sss ②如何确定我们找到了w∗w^*w∗以跳出循环 3.2 梯度下降 我们需要寻找到一个使得函数梯度下降最快的方向，并朝该方向迈出一步。 考虑一阶泰勒展开： l(wk+s)=l(wk)+g(wk)Tsl(w^k+s)=l(w^k)+g(w^k)^Ts l(wk+s)=l(wk)+g(wk)Ts 那么下降最快的方向就是−g(wk)-g(w^k)−g(wk)，因此我们在梯度下降过程中所作的就是： s=−αg(wk),α&gt;0s=-\\alpha g(w^k),\\alpha&gt;0 s=−αg(wk),α&gt;0 正确性验证：我们需要验证的是总有一些足够小的ααα使得： l(wk−αg(wk))&lt;l(wk)l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) l(wk−αg(wk))&lt;l(wk) 根据泰勒展开： l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)\\begin{aligned} &amp;l\\big(w^k-\\alpha g(w^k)\\big)=l(w^k)-\\alpha g(w^k)^Tg(w^k)+O(\\alpha^2)\\\\ &amp;\\because g(w^k)^Tg(w^k)&gt;0,\\alpha\\rightarrow0 过程中\\alpha^2\\rightarrow0的速度大于\\alpha\\\\ &amp;\\therefore 存在足够小的\\alpha，使得：l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) \\end{aligned} ​l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)​ ααα的设定需要合理，太小的话会导致收敛过慢，太大的话会导致发散。 3.3 Adagrad算法 对于ααα的一个选择是：设置ααα使得步长适合于所有的特征，Adagrad算法通过保持每个优化变量的平方梯度的运行平均值来实现这一点 Adagrad算法为具有大梯度的变量设置小学习率，并为具有小梯度的特征设置大学习率。 Adagrad算法的伪代码如下： 该算法的特点是：每一个维度都有不同的学习率，第jjj维的学习率维： αzj+ϵ\\frac{\\alpha}{\\sqrt{z_j+\\epsilon}} zj​+ϵ​α​ 3.4 牛顿方法 牛顿方法处理梯度下降利用二阶泰勒展开： l(wk+s)=l(wk)+g(wk)Ts+12sTH(wk)sl(w^k+s)=l(w^k)+g(w^k)^Ts+\\frac12s^TH(w^k)s l(wk+s)=l(wk)+g(wk)Ts+21​sTH(wk)s H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​ 因为我们假设l(w)l(w)l(w)是凸函数，则H(w)H(w)H(w)对所有www都是半正定的，则有： sTH(wk)s≥0s^TH(w^k)s\\ge0 sTH(wk)s≥0 事实上，牛顿方法在严格的局部极小值附近有很好的性质，一旦足够接近一个解，它就会迅速收敛，为了便于分析，我们假设H(w)H(w)H(w)是正定的，即： sTH(wk)s&gt;0s^TH(w^k)s&gt;0 sTH(wk)s&gt;0 二次近似的梯度为： ∂l(wk+s)∂s=g(wk)+H(wk)s\\frac{\\partial l(w^k+s)}{\\partial s}=g(w^k)+H(w^k)s ∂s∂l(wk+s)​=g(wk)+H(wk)s 这意味着我们要找到的w∗w^*w∗满足： g(w∗)+H(w∗)s=0g(w^*)+H(w^*)s=0 g(w∗)+H(w∗)s=0 所以我们可以得到步长sss： s=H(wk)−1g(wk)s=H(w^k)^{-1}g(w^k) s=H(wk)−1g(wk) 3.4.1 Example 这有一个简单的例子清楚地说明了利用二阶泰勒展开的牛顿方法的优势： 假设损失函数实际上是严格的凸二次函数，即： l(w)=12wTAw+bTw+cl(w)=\\frac12w^TAw+b^Tw+c l(w)=21​wTAw+bTw+c 其中AAA为正定矩阵，bbb为任意向量，ccc为任意常数，在这种情况下，牛顿方法仅需一步即可收敛： l(w)在Aw+b=0时收敛l(w)在Aw+b=0时收敛 l(w)在Aw+b=0时收敛 我们不妨以二维向量为例： A=[a11 a12a21 a22],w=[w1,w2]T,b=[b1,b2]TA=\\left[ \\begin{aligned} &amp;a_{11}~~~~a_{12}\\\\ &amp;a_{21}~~~~a_{22}\\\\ \\end{aligned} \\right],w=[w_1,w_2]^T,b=[b_1,b_2]^T A=[​a11​ a12​a21​ a22​​],w=[w1​,w2​]T,b=[b1​,b2​]T 则损失函数则变为： l(w1,w2)=12(a11w12+(a12+a21)w1w2+a22w22)+(b1w1+b2w2)+cl(w_1,w_2)=\\frac12\\big(a_{11}w_1^2+(a_{12}+a_{21})w_1w_2+a_{22}w_2^2\\big)+(b_1w_1+b_2w_2)+c l(w1​,w2​)=21​(a11​w12​+(a12​+a21​)w1​w2​+a22​w22​)+(b1​w1​+b2​w2​)+c g(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2]=[a11w1+(a12+a21)w2+b1,a22w2+(a12+a21)w1+b2]\\begin{aligned} &amp;g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2}]=[a_{11}w_1+(a_{12}+a_{21})w_2+b_1,a_{22}w_2+(a_{12}+a_{21})w_1+b_2] \\end{aligned} ​g(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​]=[a11​w1​+(a12​+a21​)w2​+b1​,a22​w2​+(a12​+a21​)w1​+b2​]​ H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2∂2l(w)∂w2∂w1 ∂2l(w)∂w22]=[ a11 a12+a21a12+a21 a22]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_2\\partial w_1}~~~~\\frac{\\partial^2l(w)}{\\partial w_2^2} \\end{aligned} \\right]=\\left[ \\begin{aligned} &amp;~~~~a_{11}~~~~a_{12}+a_{21}\\\\ &amp;a_{12}+a_{21}~~~~a_{22} \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​∂w2​∂w1​∂2l(w)​ ∂w22​∂2l(w)​​⎦⎥⎥⎥⎤​=[​ a11​ a12​+a21​a12​+a21​ a22​​] H(w)−1=[a22a11a22−(a12+a21)2 −(a12+a21)a11a22−(a12+a21)2−(a12+a21)a11a22−(a12+a21)2 a11a11a22−(a12+a21)2]=β[ a22 −(a12+a21)−(a12+a21) a11],β=1a11a22−(a12+a21)2H(w)^{-1}=\\left[ \\begin{aligned} &amp;\\frac{a_{22}}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{a_{11}}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right]=\\beta\\left[ \\begin{aligned} &amp;~~~~a_{22}~~~~-(a_{12}+a_{21})\\\\ &amp;-(a_{12}+a_{21})~~~~a_{11} \\end{aligned} \\right],\\beta=\\frac1{a_{11}a_{22}-(a_{12}+a_{21})^2} H(w)−1=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2a22​​ a11​a22​−(a12​+a21​)2−(a12​+a21​)​a11​a22​−(a12​+a21​)2−(a12​+a21​)​ a11​a22​−(a12​+a21​)2a11​​​⎦⎥⎥⎥⎤​=β[​ a22​ −(a12​+a21​)−(a12​+a21​) a11​​],β=a11​a22​−(a12​+a21​)21​ s=−H(w)−1g(w)=−β[(a11a22−(a12+a21)2)w1+a22b1−(a12+a21)b2(a11a22−(a12+a21)2)w2+a11b2−(a12+a21)b1]=[−w1+(a12+a21)b2−a22b1a11a22−(a12+a21)2−w2+(a12+a21)b1−a11b2a11a22−(a12+a21)2]s=-H(w)^{-1}g(w)=-\\beta\\left[ \\begin{aligned} &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_1+a_{22}b_1-(a_{12}+a_{21})b_2\\\\ &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_2+a_{11}b_2-(a_{12}+a_{21})b_1 \\end{aligned} \\right]\\\\ =\\left[ \\begin{aligned} &amp;-w_1+\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;-w_2+\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] s=−H(w)−1g(w)=−β[​(a11​a22​−(a12​+a21​)2)w1​+a22​b1​−(a12​+a21​)b2​(a11​a22​−(a12​+a21​)2)w2​+a11​b2​−(a12​+a21​)b1​​]=⎣⎢⎢⎢⎡​​−w1​+a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​−w2​+a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 则更新www的过程为： w=w+s=[(a12+a21)b2−a22b1a11a22−(a12+a21)2(a12+a21)b1−a11b2a11a22−(a12+a21)2]w=w+s=\\left[ \\begin{aligned} &amp;\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] w=w+s=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 为了便于讨论，我们不妨使得A=IA=IA=I，则有w=[−b1,−b2]T=−bw=[-b_1,-b_2]^T=-bw=[−b1​,−b2​]T=−b，则有Aw+b=0Aw+b=0Aw+b=0，仅通过一次更新即实现了收敛。 注意： ①H(w)H(w)H(w)是一个d×dd\\times dd×d的矩阵，它的构造代价很大，一个很好的近似方法是只计算其对角线条目 ②本质上这是梯度下降法和牛顿方法的结合，为了避免牛顿法的发散，一个好的方法是从梯度下降（甚至随机梯度下降）开始，然后完成优化牛顿法。通常，牛顿法使用的二阶近似值更可能接近最佳值 ","link":"https://2006wzt.github.io/post/机器学习实战（八）：梯度下降/"},{"title":"机器学习实战（七）：逻辑回归","content":"逻辑回归 一、基本思想 逻辑回归是一种用于分类任务的经典机器学习算法。 我们之前介绍过机器学习算法大致可以分为两类： ①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)\\begin{aligned} &amp;①生成学习：估计P(x,y)=P(x|y)P(y)\\\\ &amp;②判别学习：估计P(y|x) \\end{aligned} ​①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)​ 上节学习的朴素贝叶斯属于生成学习算法，而逻辑回归则属于判别学习算法，它与高斯朴素贝叶斯相对应，即判别式的高斯朴素贝叶斯。 逻辑回归的核心函数为：sigmoidsigmoidsigmoid函数，也成为激活函数： sigmoid(x)=σ(x)=11+e−xsigmoid(x)=\\sigma(x)=\\frac{1}{1+e^{-x}} sigmoid(x)=σ(x)=1+e−x1​ 我们对逻辑回归模型的建模如下： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 与感知机原理相同，我们利用升维将偏差项bbb处理到www中，则模型简化为： P(y∣x)=11+e−y(wTx)P(y|x)=\\frac{1}{1+e^{-y(w^Tx)}} P(y∣x)=1+e−y(wTx)1​ 二、逻辑回归的参数 在逻辑回归模型中，重要的参数也为www，因此我们需要用到之前学过的几个概率估计方法对其进行估计。 2.1 极大似然估计（MLE） 利用极大似然估计逻辑回归的参数：在MLEMLEMLE中，要极大化的条件数据为P(Y∣X,w)P(Y|X,w)P(Y∣X,w)，X,YX,YX,Y为训练数据，我们要找到一个合适的www使得特征向量集为XXX时，观察到标签YYY的概率最大 X：d×n维矩阵，即X=[x1→,x2→,...,xn→]∈Rd×nY：n维向量，即Y=[y1,y2,...,yn]\\begin{aligned} &amp;X：d\\times n维矩阵，即X=[\\overrightarrow {x_1},\\overrightarrow{x_2},...,\\overrightarrow{x_n}]\\in R^{d\\times n}\\\\ &amp;Y：n维向量，即Y=[y_1,y_2,...,y_n] \\end{aligned} ​X：d×n维矩阵，即X=[x1​​,x2​​,...,xn​​]∈Rd×nY：n维向量，即Y=[y1​,y2​,...,yn​]​ MLEMLEMLE的假设为： P(Y∣X,w)=∏i=1nP(yi∣xi,w)P(Y|X,w)=\\prod_{i=1}^nP(y_i|x_i,w) P(Y∣X,w)=i=1∏n​P(yi​∣xi​,w) 由此，我们对参数www进行的极大似然估计过程为： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡11+e−yi(wTxi) =argmaxw −log⁡(1+e−yi(wTxi))=argminw log⁡(1+e−yi(wTxi))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log \\frac{1}{1+e^{-y_i(w^Tx_i)}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\log(1+e^{-y_i(w^Tx_i)})=\\underset{w}{argmin}~\\log(1+e^{-y_i(w^Tx_i)}) \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log1+e−yi​(wTxi​)1​ =wargmax​ −log(1+e−yi​(wTxi​))=wargmin​ log(1+e−yi​(wTxi​))​ 为了求解www，我们引入函数l(w)l(w)l(w)：在负数域上求解−l(w)-l(w)−l(w)的最大值： l(w)=∑i=1nlog⁡(1+e−yi(wTxi))l(w)=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i)}) l(w)=i=1∑n​log(1+e−yi​(wTxi​)) 我们要寻找max⁡{∣−l(w)∣}\\max\\{|-l(w)|\\}max{∣−l(w)∣}所对应的www。 2.1.1 1-D Example 我们考虑一个1维问题，如下图所示：+++表示正类，ooo表示负类： 如上图所示，x0x_0x0​恒为1，数据点特征的不同仅为x1x_1x1​的不同，本质上是一个1维的问题： x=[x0,x1],w=[w0,w1],l(w)=l([w0,w1])\\begin{aligned} &amp;x=[x_0,x_1],w=[w_0,w_1],l(w)=l([w_0,w_1]) \\end{aligned} ​x=[x0​,x1​],w=[w0​,w1​],l(w)=l([w0​,w1​])​ 右图为l(w)l(w)l(w)所对应的曲面，我们可以确定：w0=1,w1=0.7w_0=1,w_1=0.7w0​=1,w1​=0.7时，∣−l(w)∣|-l(w)|∣−l(w)∣取得最大值，因此我们计算出w=[1,0.7]w=[1,0.7]w=[1,0.7] 我们也可以用热力图更直观得得到www的最佳取值： 以下是部分样例对上述图像的贡献图： 2.1.2 2-D Example 我们考虑一个2维问题，如下图所示：+++表示正类，ooo表示负类： 根据热力图，我们得到∣−l(w)∣|-l(w)|∣−l(w)∣在w=[−0.81.0.81]w=[-0.81.0.81]w=[−0.81.0.81]时取得最大值。 MLEMLEMLE计算结果如下所示，其中红色表示正类别的概率很高。黑线表示MLEMLEMLE学习的决策边界。 决策边界：P(y=−1∣x)=P(y=+1∣x)即：11+ewTx=11+e−wTx→wTx=−wTx→wTx=0w=[w1,w2]=[−0.81,0.81],x=[x1,x2]wTx=w1⋅x1+w2⋅x2=−0.81x1+0.81x2=0→x1=x2\\begin{aligned} &amp;决策边界：P(y=-1|x)=P(y=+1|x)\\\\ &amp;即：\\frac{1}{1+e^{w^Tx}}=\\frac{1}{1+e^{-w^Tx}}\\rightarrow w^Tx=-w^Tx\\rightarrow w^Tx=0\\\\ &amp;w=[w_1,w_2]=[-0.81,0.81],x=[x_1,x_2]\\\\ &amp;w^Tx=w_1\\cdot x_1+w_2\\cdot x_2=-0.81x_1+0.81x_2=0\\rightarrow x_1=x_2 \\end{aligned} ​决策边界：P(y=−1∣x)=P(y=+1∣x)即：1+ewTx1​=1+e−wTx1​→wTx=−wTx→wTx=0w=[w1​,w2​]=[−0.81,0.81],x=[x1​,x2​]wTx=w1​⋅x1​+w2​⋅x2​=−0.81x1​+0.81x2​=0→x1​=x2​​ 2.2 极大后验估计（MAP） 在极大后验估计中，我们将www视为一个随机变量，并且可以预先指定它的先验分布。我们先预先指定www的先验分布： w∼N(0,σ2)P(w)=12πσ2e−wTw2σ2w\\sim \\mathcal{N}(0,\\sigma^2)\\\\ P(w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{w^Tw}{2\\sigma^2}} w∼N(0,σ2)P(w)=2πσ2​1​e−2σ2wTw​ 这是逻辑回归的高斯近似。 以下计算过程中我们要用到链式法则： P(A,B∣C)=P(A∣B,C)P(B∣C)P(A,B|C)=P(A|B,C)P(B|C) P(A,B∣C)=P(A∣B,C)P(B∣C) 同时需要注意XXX与www是无关的。 我们在MAPMAPMAP中的目标是找到给定数据的最可能的模型参数，即使后验值最大化的参数： w^MAP=argmaxw P(w∣D)=argmaxw P(D∣w)P(w)P(D) =argmaxw P(D∣w)P(w)=argmaxw P(Y,X∣w)P(w) =argmaxw P(Y∣X,w)P(X∣w)P(w)=argmaxw P(Y∣X,w)P(w) =argmaxw log⁡P(Y∣X,w)+log⁡P(w) =argmaxw (−∑i=1nlog⁡(1+e−yi(wTxi))+log⁡12πσ2−12σ2wTw) =argminw (∑i=1nlog⁡(1+e−yi(wTxi))+λwTw)\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|D)=\\underset{w}{argmax}~\\frac{P(D|w)P(w)}{P(D)}\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(D|w)P(w)=\\underset{w}{argmax}~P(Y,X|w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(Y|X,w)P(X|w)P(w)=\\underset{w}{argmax}~P(Y|X,w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\log P(Y|X,w)+\\log P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\big(-\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}-\\frac1{2\\sigma^2}w^Tw\\big)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmin}~\\big(\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\lambda w^Tw\\big) \\end{aligned} ​w^MAP​=wargmax​ P(w∣D)=wargmax​ P(D)P(D∣w)P(w)​ =wargmax​ P(D∣w)P(w)=wargmax​ P(Y,X∣w)P(w) =wargmax​ P(Y∣X,w)P(X∣w)P(w)=wargmax​ P(Y∣X,w)P(w) =wargmax​ logP(Y∣X,w)+logP(w) =wargmax​ (−i=1∑n​log(1+e−yi​(wTxi​))+log2πσ2​1​−2σ21​wTw) =wargmin​ (i=1∑n​log(1+e−yi​(wTxi​))+λwTw)​ 其中：λ=12σ2\\lambda=\\frac1{2\\sigma^2}λ=2σ21​，与MLEMLEMLE同理，我们引入函数l(w)l(w)l(w)，在负数域上求解∣−l(w)∣|-l(w)|∣−l(w)∣最大值以得到www： l(w)=∑i=1nlog⁡(1+e−y1(wTxi))+λwTwl(w)=\\sum_{i=1}^n\\log(1+e^{-y_1(w^Tx_i)})+\\lambda w^Tw l(w)=i=1∑n​log(1+e−y1​(wTxi​))+λwTw 三、更新参数 除了上述利用函数求极值的方法求解参数www的方法，我们也可以用梯度下降的方式更新www 3.1 损失函数 根据模型预测的概率为： P(y=1∣x)=h(x)=σ(wTx)=11+e−(wTx) (1)P(y=0∣x)=1−h(x) (2)\\begin{aligned} &amp;P(y=1|x)=h(x)=\\sigma(w^Tx)=\\frac{1}{1+e^{-(w^Tx)}}~~~~~~~~(1)\\\\ &amp;P(y=0|x)=1-h(x)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(2) \\end{aligned} ​P(y=1∣x)=h(x)=σ(wTx)=1+e−(wTx)1​ (1)P(y=0∣x)=1−h(x) (2)​ 合并(1)(1)(1)式(2)(2)(2)式得： P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}\\begin{aligned} &amp;P(y|x,w)=h(x)^y(1-h(x))^{1-y},y\\in\\{0,1\\} \\end{aligned} ​P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}​ 我们进行极大似然估计可得： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∏i=1nh(xi)yi(1−h(xi))1−yi=argmaxw ∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi)))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nh(x_i)^{y_i}(1-h(x_i))^{1-y_i}=\\underset{w}{argmax}~\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i))\\big)\\\\ \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∏n​h(xi​)yi​(1−h(xi​))1−yi​=wargmax​ i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)))​ 由此我们可以定义损失函数： L(w)=−∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi))L(w)=-\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i)\\big) L(w)=−i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)) 我们希望损失函数最小化以得到最优的参数www，与感知机的更新过程相同，我们求导以得到更新项。 3.2 梯度下降 梯度下降的过程为遍历数据集，利用每个样本对参数www进行更新：令z=wTxz=w^Txz=wTx 每一个样本所对应的损失函数为： l(w)=−yln⁡h(x)−(1−y)ln⁡(1−h(x))l(w)=-y\\ln h(x)-(1-y)\\ln(1-h(x)) l(w)=−ylnh(x)−(1−y)ln(1−h(x)) 对www进行求导，求出更新项： ∂l(w)∂w=∂l(w)∂h(x)⋅∂h(x)∂z⋅∂z∂w,h(x)=11+e−(wTx)①∂l(w)∂h(x)=−yh(x)+1−y1−h(x)=h(x)−yh(x)(1−h(x))②∂h(x)∂z=e−x(1+e−x)=h(x)(1−h(x)) ③∂z∂w=x因此：∂l(w)∂w=h(x)−yh(x)(1−h(x))⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=\\frac{\\partial l(w)}{\\partial h(x)}\\cdot \\frac{\\partial h(x)}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w},h(x)=\\frac{1}{1+e^{-(w^Tx)}}\\\\ &amp;①\\frac{\\partial l(w)}{\\partial h(x)}=-\\frac{y}{h(x)}+\\frac{1-y}{1-h(x)}=\\frac{h(x)-y}{h(x)(1-h(x))}\\\\ &amp;②\\frac{\\partial h(x)}{\\partial z}=\\frac{e^{-x}}{(1+e^{-x})}=h(x)(1-h(x))~~③\\frac{\\partial z}{\\partial w}=x\\\\ &amp;因此：\\frac{\\partial l(w)}{\\partial w}=\\frac{h(x)-y}{h(x)(1-h(x))}\\cdot h(x)(1-h(x))\\cdot x=(h(x)-y)\\cdot x \\end{aligned} ​∂w∂l(w)​=∂h(x)∂l(w)​⋅∂z∂h(x)​⋅∂w∂z​,h(x)=1+e−(wTx)1​①∂h(x)∂l(w)​=−h(x)y​+1−h(x)1−y​=h(x)(1−h(x))h(x)−y​②∂z∂h(x)​=(1+e−x)e−x​=h(x)(1−h(x)) ③∂w∂z​=x因此：∂w∂l(w)​=h(x)(1−h(x))h(x)−y​⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x​ 则梯度下降的更新过程为： w→w−η∑i=1n(h(xi)−yi)⋅x=w−η∑i=1n(σ(wTxi)−yi)⋅xiw\\rightarrow w-\\eta\\sum_{i=1}^n(h(x_i)-y_i)\\cdot x=w-\\eta\\sum_{i=1}^n(\\sigma(w^Tx_i)-y_i)\\cdot x_i w→w−ηi=1∑n​(h(xi​)−yi​)⋅x=w−ηi=1∑n​(σ(wTxi​)−yi​)⋅xi​ 四、模型实现 我们将通过手动实现与调库的方式去构造逻辑回归模型。 4.1 数据集 本次我们仍使用乳腺癌数据集，进行逻辑回归二分类器的实现，数据集详细内容可通过如下代码查看，不再过多赘述。 4.2 手动实现模型 我们利用梯度下降的方式对模型进行更新： 4.3 调库实现模型 调库用到的函数为sklearn所提供的LogisticRegression函数，其函数原型如下： 可以发现该模型并不是用梯度下降的方式去更新模型的，而是通过参数的估计，要用到优化求解器，其相关参数如下表所示： 模型参数 Parameter含义 备注 penalty 正则化项 广义线性模型的正则项，可选值包括L1正则项'l1'、L2正则项'l2'、复合正则'elasticnet'和无正则项None，默认值为'l2'。值得注意的是，正则项的选择应与优化求解器相匹配(详见solver参数)。 l1_ratio 正则权重系数 L1正则和L2正则的权重系数，取值空间为0-1。若为0，则相当于为L2正则；若为1，则为L1正则，否则为Elasticnet正则。 dual 对偶问题 默认值False，可设为True将问题转换为对偶问题（详见本博客SVM问题中原问题-对偶问题的推导），仅适用于采用L2正则化且求解器为'liblinear’的情况。当样本数少于特征数时，推荐为True。 tol 迭代阈值 求解器迭代求解时，停止迭代的目标函数改变阈值。 C 正则化系数倒数 注意，其值与正则化强度相反，即C值越小，正则化程度越大。其值必须为正，且默认值为1 fit_intercept 是否预设偏置 控制广义线性模型中是否预设偏置值 intercept_scaling 预设偏置值 广义线性模型中预设的偏置值，仅当求解器为'liblinear'同时fit_intercept时生效。注意：该偏置值会作为新的特征计算其系数，因此也会计入L1和L2正则。 class_weight 样本权重 用于处理样本不均衡问题。默认值为None，即各类别样本权重一样，可通过设置字典定义权重系数，或设为'balanced'，即根据样本数自动计算权重。若在fit函数中设置sample_weight参数，两者作用会叠加。 random_state 随机状态 LR模型中的随机性主要体现在求解器迭代时对样本的随机选取，适用于当求解器为'liblinear'或'sag'时。 solver 求解器 sklearn中共提供了5种优化求解器，分别为'liblinear'、'sag'、'saga'、'newton-cg'和'lbfgs'。各求解器的适用条件不同，具体见后文。默认值为'liblinear'。 max_iter 最大迭代步数 ‘newton-cg’、'sag'和'lbfgs' 求解器所需要的最大迭代步数 multi_class 多分类策略 取值可为'ovr'、'multinomial'和'auto'。'ovr'即采用'one vs rest'策略对二分类模型进行集成；'multinomial'即采用'multinomial loss'直接求解多分类问题。默认为'auto'，其会在两分类问题或求解器为'liblinear'时选择'ovr'，而其它情况下选择'multinomial'。默认值为'auto' 在此我们不对参数作过多解释，具体调参过程就问题而论。 ","link":"https://2006wzt.github.io/post/机器学习实战（七）：逻辑回归/"},{"title":"机器学习实战（六）：朴素贝叶斯","content":"朴素贝叶斯 一、基本思想 在机器学习中，朴素贝叶斯分类器是在强独立假设下基于贝叶斯定理的一系列简单概率分类器，强独立假设为： 对于训练数据：D={(x1,y1),(x2,y2),...,(xn,yn)}\\begin{aligned} &amp;对于训练数据：D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\} \\end{aligned} ​对于训练数据：D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)}​ 我们假设它是从未知分布中抽取的独立同分布，则有： P(D)=P((x1,y1),(x2,y2),...,(xn,yn))=∏α=1nP(xα,yα)P(D)=P((x_1,y_1),(x_2,y_2),...,(x_n,y_n))=\\prod_{\\alpha=1}^nP(x_\\alpha,y_\\alpha) P(D)=P((x1​,y1​),(x2​,y2​),...,(xn​,yn​))=α=1∏n​P(xα​,yα​) 如果我们有足够的数据，我们可以估算P(X,Y)P(X,Y)P(X,Y)，类似于上一节的硬币例子，我们想象一个巨大的骰子，每个可能的(X,Y)(X,Y)(X,Y)值都有一面。我们可以通过计数来估计某一特定方面出现的概率。 P^(x,y)=∑i=1nI(xi=x∩yi=y)nxi=x且yi=y时，I(xi=x∩yi=y)=1，否则为0\\hat{P}(x,y)=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{n}\\\\ x_i=x且y_i=y时，I(x_i=x\\cap y_i=y)=1，否则为0 P^(x,y)=n∑i=1n​I(xi​=x∩yi​=y)​xi​=x且yi​=y时，I(xi​=x∩yi​=y)=1，否则为0 当然，如果我们主要感兴趣的是从特征xxx预测标签yyy，我们可以直接估计P(y∣x)P(y | x)P(y∣x)，而不是P(x，y)P(x，y)P(x，y)。我们可以使用贝叶斯最优分类器对P(y∣x)P(y | x)P(y∣x)进行预测： P^(y∣x)=P^(x∣y)P^(y)P(x)=P^(x,y)P(x)=∑i=1nI(xi=x∩yi=y)/n∑i=1nI(xi=x)/n=∑i=1nI(xi=x∩yi=y)∑i=1nI(xi=x)\\hat{P}(y|x)=\\frac{\\hat{P}(x|y)\\hat{P}(y)}{P(x)}=\\frac{\\hat{P}(x,y)}{P(x)}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)/n}{\\sum_{i=1}^nI(x_i=x)/n}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{\\sum_{i=1}^nI(x_i=x)} P^(y∣x)=P(x)P^(x∣y)P^(y)​=P(x)P^(x,y)​=∑i=1n​I(xi​=x)/n∑i=1n​I(xi​=x∩yi​=y)/n​=∑i=1n​I(xi​=x)∑i=1n​I(xi​=x∩yi​=y)​ 如上图韦恩图所示，我们最终的预测结果可以表示为： P^(y∣x)=∣C∣∣B∣\\hat{P}(y|x)=\\frac{|C|}{|B|} P^(y∣x)=∣B∣∣C∣​ 二、朴素贝叶斯算法 2.1 贝叶斯规则 朴素贝叶斯算法的核心是贝叶斯公式： P(y∣x)=P(x∣y)P(y)P(x)P(y|x)=\\frac{P(x|y)P(y)}{P(x)} P(y∣x)=P(x)P(x∣y)P(y)​ 根据上述贝叶斯公式我们可以知道：如果我们可以预测P(x∣y)P(x|y)P(x∣y)和P(y)P(y)P(y)，那么我们即可预测P(y∣x)P(y|x)P(y∣x) ①预测P(y)P(y)P(y)很容易，假如yyy取离散的值，我们只需要记录观察到结果为yyy的次数即可： P^(y=c)=∑i=1nI(yi=c)n=π^c\\hat{P}(y=c)=\\frac{\\sum_{i=1}^nI(y_i=c)}{n}=\\hat{\\pi}_c P^(y=c)=n∑i=1n​I(yi​=c)​=π^c​ ②但是预测P(x∣y)P(x|y)P(x∣y)并不容易，因此我们需要引入朴素贝叶斯假设。 2.2 朴素贝叶斯假设 朴素贝叶斯假设为： P(x∣y)=∏α=1dP(xα∣y)x=[x1,x2,...,xd],xα是d维特征向量x在第α维度上的值P(x|y)=\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\\\ x=[x_1,x_2,...,x_d],x_\\alpha是d维特征向量x在第\\alpha维度上的值 P(x∣y)=α=1∏d​P(xα​∣y)x=[x1​,x2​,...,xd​],xα​是d维特征向量x在第α维度上的值 即：对于给定的标签，其特征值是独立的，这是一个非常大胆的假设 经常使用朴素贝叶斯分类器的一个例子是垃圾邮件的过滤，此处数据为电子邮件，标签为是垃圾邮件还是非垃圾邮件； 朴素贝叶斯假设意味着电子邮件中的单词在条件上是独立的，因为我们知道电子邮件是否是垃圾邮件，显然这不是真的。无论是垃圾邮件还是非垃圾邮件都不是独立随机抽取的。然而，即使违反了这一假设，由此产生的分类器在实践中也可以很好地工作。 由此我们可以对P(x∣y)P(x|y)P(x∣y)进行估计：假设朴素贝叶斯假设成立，则贝叶斯分类器定义如下： h(x)=argmaxy P(y∣x)=argmaxy P(x∣y)P(y)P(x)=argmaxy P(x∣y)P(y) =argmaxy (∏α=1dP(xα∣y))P(y)=argmaxy (∑α=1dlog⁡P(xα∣y))+log⁡P(y)\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~P(y|x)=\\underset{y}{argmax}~\\frac{P(x|y)P(y)}{P(x)}=\\underset{y}{argmax}~P(x|y)P(y)\\\\ &amp;~~~~~~~~~=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\big)P(y)=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log P(x_\\alpha|y)\\big)+\\log P(y) \\end{aligned} ​h(x)=yargmax​ P(y∣x)=yargmax​ P(x)P(x∣y)P(y)​=yargmax​ P(x∣y)P(y) =yargmax​ (α=1∏d​P(xα​∣y))P(y)=yargmax​ (α=1∑d​logP(xα​∣y))+logP(y)​ 估计P(xα∣y)P(x_\\alpha|y)P(xα​∣y)和P(y)P(y)P(y)都很容易，因此我们则可实现贝叶斯分类器。 三、估算P(xα∣y)P(x_\\alpha|y)P(xα​∣y) 3.1 Case#1：分类特征 分类特征：对于ddd维特征向量xxx，它的第α\\alphaα维特征xαx_\\alphaxα​有KαK_\\alphaKα​个取值，即分类问题的特征 例如：年龄、性别、省份等特征，特们都有固定个数的KαK_\\alphaKα​个取值，xα∈{f1,f2,...,fKα}x_\\alpha\\in\\{f_1,f_2,...,f_{K_\\alpha} \\}xα​∈{f1​,f2​,...,fKα​​} P(xα=fj∣y=c)=[θjc]α,∑j=1Kαθjc=1P(x_\\alpha=f_j|y=c)=[\\theta_{jc}]_\\alpha,\\sum_{j=1}^{K_\\alpha}\\theta_{jc}=1 P(xα​=fj​∣y=c)=[θjc​]α​,j=1∑Kα​​θjc​=1 [θjc]α[θ_{jc}]_α[θjc​]α​是特征ααα在假设标签是ccc时，具有值fjf_jfj​的概率。约束表明xαx_αxα​必须具有一个类别{1，…，Kα}\\{1，…，K_α\\}{1，…，Kα​} 下面我们对[θjc]α[\\theta_{jc}]_\\alpha[θjc​]α​进行估计： [θjc]^α=∑i=1nI(xi=fj∩yi=c)+l∑i=1nI(yi=c)+lKα\\hat{[\\theta_{jc}]}_\\alpha=\\frac{\\sum_{i=1}^nI(x_i=f_j\\cap y_i=c)+l}{\\sum_{i=1}^nI(y_i=c)+lK_\\alpha} [θjc​]^​α​=∑i=1n​I(yi​=c)+lKα​∑i=1n​I(xi​=fj​∩yi​=c)+l​ l ~l~ l 是一个平滑参数： ①l=0l=0l=0时，我们得到MLEMLEMLE估计量，l&gt;0l&gt;0l&gt;0时，我们得到MAPMAPMAP估计量 ②l=1l=1l=1时，我们得到拉普拉斯平滑 最终我们得到的模型为： h(x)=argmaxy (∏α=1d[θjc]^α)π^c=argmaxy (∑α=1dlog⁡[θjc]α^)+log⁡π^c\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d\\hat{[\\theta_{jc}]}_\\alpha\\big)\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log\\hat{[\\theta_{jc}]_\\alpha}\\big)+\\log\\hat\\pi_c \\end{aligned} ​h(x)=yargmax​ (α=1∏d​[θjc​]^​α​)π^c​=yargmax​ (α=1∑d​log[θjc​]α​^​)+logπ^c​​ 3.2 Case#2：多项式特征 多项式特征：特征值不是诸如男女之类的分类特征，而是计数值，即回归问题的特征，但是计数值是有限的 例如：垃圾邮件过滤的例子中，各个特征是不同的单词，维度ddd即为单词表的大小，特征值是一个单词出现的次数，比如某个单词ααα出现十次，即xαx_αxα​=10，可能意味着该邮件为垃圾邮件。 xα∈{0,1,2,...,m},m=∑α=1dxαx_\\alpha\\in\\{0,1,2,...,m\\},m=\\sum_{\\alpha=1}^dx_\\alpha xα​∈{0,1,2,...,m},m=α=1∑d​xα​ 新的估计方式如下：P(x∣m,y=c)P(x|m,y=c)P(x∣m,y=c)表示标签y=cy=cy=c时，一个文本长度为mmm的邮件中特征向量为xxx的概率 P^(x∣m,y=c)=m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θαc)xα\\hat{P}(x|m,y=c)=\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha} P^(x∣m,y=c)=x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θαc​)xα​ θαc\\theta_{\\alpha c}θαc​是在标签y=cy=cy=c时，选中特征向量中的xαx_\\alphaxα​的概率，有∑α=1dθαc=1\\sum_{\\alpha=1}^d\\theta_{\\alpha c}=1∑α=1d​θαc​=1，以垃圾邮件为例，θαc\\theta_{\\alpha c}θαc​即为单词xαx_\\alphaxα​在文本中所占的比例。 下面我们对θαc\\theta_{\\alpha c}θαc​进行估计： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ xiαx_{i\\alpha}xiα​是第iii个电子邮件的特征向量的第α\\alphaα维度的值，mi=∑α=1dxiαm_i=\\sum_{\\alpha=1}^dx_{i\\alpha}mi​=∑α=1d​xiα​即第iii个电子邮件中的文本总数 最终我们得到的模型为： h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 3.3 Case#3：连续特征 连续特征：计数值是连续的值，例如身高体重，连续特征所对应的朴素贝叶斯也称为高斯朴素贝叶斯 xα∈Rx_\\alpha\\in R xα​∈R 新的估计方式如下： P^(xα∣y=c)=N(μαc,σαc2)=12πσαce−12(xα−μαcσαc)2\\hat{P}(x_\\alpha|y=c)=\\mathcal{N}(\\mu_{\\alpha c},\\sigma_{\\alpha c}^2)=\\frac1{\\sqrt{2\\pi}\\sigma_{\\alpha c}}e^{-\\frac12(\\frac{x_\\alpha-\\mu_{\\alpha c}}{\\sigma_{\\alpha c}})^2} P^(xα​∣y=c)=N(μαc​,σαc2​)=2π​σαc​1​e−21​(σαc​xα​−μαc​​)2 注意，上面指定的模型基于我们对数据的假设，即每个特征α来自一类条件高斯分布。 对参数进行估计： μ^αc=1nc∑i=1nI(yi=c)xiασ^αc=1nc∑i=1nI(yi=c)(xiα−μαc)2nc=∑i=1nI(yi=c)\\hat\\mu_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}\\\\\\hat\\sigma_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)(x_{i\\alpha}-\\mu_{\\alpha c})^2\\\\ n_c=\\sum_{i=1}^nI(y_i=c) μ^​αc​=nc​1​i=1∑n​I(yi​=c)xiα​σ^αc​=nc​1​i=1∑n​I(yi​=c)(xiα​−μαc​)2nc​=i=1∑n​I(yi​=c) 高斯朴素贝叶斯分类器本质上为逻辑回归模型： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 四、朴素贝叶斯分类器 朴素贝叶斯分类器为一个线性分类器： 对于一个多项式特征的数据集，其进行分类的过程如下：假设yi∈{−1,+1}y_i\\in\\{-1,+1\\}yi​∈{−1,+1}且特征都是多项式特征，有： h(x)=argmaxc P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x∣y=+1)P(y=+1)P(x)&gt;P(x∣y=−1)P(y=−1)P(x)即：P(y=+1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(+1))xα&gt;P(y=−1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(−1))xα即：log⁡P(y=+1)+∑α=1dxαlog⁡θα(+1)&gt;log⁡P(y=−1)+∑α=1dxαlog⁡θα(−1)即：[log⁡P(y=+1)−log⁡P(y=−1)]+∑α=1dxα(log⁡θα(+1)−log⁡θα(−1))&gt;0\\begin{aligned} &amp;h(x)=\\underset{c}{argmax}~P(y=c|x)\\\\ &amp;设h(x)=+1,则P(y=+1|x)&gt;P(y=-1|x)\\\\ &amp;根据贝叶斯公式，则有：\\frac{P(x|y=+1)P(y=+1)}{P(x)}&gt;\\frac{P(x|y=-1)P(y=-1)}{P(x)}\\\\ &amp;即：P(y=+1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (+1)})^{x_\\alpha}&gt;P(y=-1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (-1)})^{x_\\alpha}\\\\ &amp;即：\\log P(y=+1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(+1)}&gt;\\log P(y=-1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(-1)}\\\\ &amp;即：\\big[\\log P(y=+1)-\\log P(y=-1)\\big]+\\sum_{\\alpha=1}^dx_\\alpha(\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)})&gt;0 \\end{aligned} ​h(x)=cargmax​ P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x)P(x∣y=+1)P(y=+1)​&gt;P(x)P(x∣y=−1)P(y=−1)​即：P(y=+1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(+1)​)xα​&gt;P(y=−1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(−1)​)xα​即：logP(y=+1)+α=1∑d​xα​logθα(+1)​&gt;logP(y=−1)+α=1∑d​xα​logθα(−1)​即：[logP(y=+1)−logP(y=−1)]+α=1∑d​xα​(logθα(+1)​−logθα(−1)​)&gt;0​ 我们可以发现上述正确分类的形式与感知机很像，我们可以进行如下变换： 令：b=log⁡P(y=+1)−log⁡P(y=−1),wα=log⁡θα(+1)−log⁡θα(−1)则：h(x)=+1↔wTx+b&gt;0\\begin{aligned} &amp;令：b=\\log P(y=+1)-\\log P(y=-1),w_\\alpha=\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)}\\\\ &amp;则：h(x)=+1\\leftrightarrow w^Tx+b&gt;0 \\end{aligned} ​令：b=logP(y=+1)−logP(y=−1),wα​=logθα(+1)​−logθα(−1)​则：h(x)=+1↔wTx+b&gt;0​ 与感知机的形式完全相同，最终我们将朴素贝叶斯分类器等效为了感知机。 五、算法实现 我们将通过手动实现与调库的方式去构造朴素贝叶斯模型。 5.1 数据集 本次我们使用的数据集为垃圾邮件数据集，其下载链接为：UCI Machine Learning Repository: SMS Spam Collection Data Set 下载后的SMSSpamCollection中的内容如下图所示，显然我们需要对文件进行处理整合，数据处理过程如下： 上述数据集预处理过程中有几点需要注意： ①nltk报错问题，不妨尝试运行以下两部分代码完善nltk库的安装 ②TF-IDF特征矩阵：TF-IDF是Term Frequency - Inverse Document Frequency的缩写，即“词频——逆文本频率”。它由两部分组成，TF和IDF，也就是这两部分的乘积。TF指的就是常用的词频，即某个单词在当前文本中出现的频率。IDF，即“逆文本频率”，反映了一个单词在当前文本中的重要性 TF(t,D)=单词t在文本D中出现的次数文本D的总单词数IDF(t)=log⁡语料库文本总数+1包含词语t的文本总数+1TF−IDF(t,D)=TF(t,D)⋅IDF(t)\\begin{aligned} &amp;TF(t,D)=\\frac{单词t在文本D中出现的次数}{文本D的总单词数}\\\\ &amp;IDF(t)=\\log\\frac{语料库文本总数+1}{包含词语t的文本总数+1}\\\\ &amp;TF-IDF(t,D)=TF(t,D)\\cdot IDF(t) \\end{aligned} ​TF(t,D)=文本D的总单词数单词t在文本D中出现的次数​IDF(t)=log包含词语t的文本总数+1语料库文本总数+1​TF−IDF(t,D)=TF(t,D)⋅IDF(t)​ 我们处理后得到的 vectorizer.fit_transform(X) 输出的特征矩阵形式为(A,B) C(A,B)~C(A,B) C，AAA为文件索引，BBB为特定词的向量索引，CCC为文件AAA中单词BBB的TF−IDFTF-IDFTF−IDF分数 5.2 手动实现模型 我们选择多项式特征的数据集，即在上述数据处理的基础上将文本转为统计单词数量的矩阵，数据预处理与存储过程如下： 参考多项式特征的模型，我们可以构造我们的训练代码： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 5.3 调库实现模型 调库实现模型时，我们可以使用另一种方法，即上面提到过的TF-IDF特征矩阵，即连续特征的数据集，数据预处理与存储过程如下： 我们直接利用sklearn所提供的朴素贝叶斯分类器，观察分类情况，可以发现调库的准确率达到了0.9596412556053812，也较为准确。 ","link":"https://2006wzt.github.io/post/机器学习实战（六）：朴素贝叶斯/"},{"title":"机器学习实战（五）：概率估计","content":"概率估计 一、分布预测分类 我们在KNNKNNKNN算法中已经学习过了贝叶斯最优分类器： 如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx=arg⁡max⁡yP(y∣x)\\begin{aligned} &amp;如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签~label_x=\\arg\\max_{y}P(y|x) \\end{aligned} ​如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx​=argymax​P(y∣x)​ 如果我们可以根据训练集得到一个大致的分布P(X,Y)，则可以利用贝叶斯最优分类器进行分类，对分布进行预测的学习分为两类： ①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)\\begin{aligned} &amp;①生成学习：预测P(X,Y)=P(X|Y)P(Y)\\\\ &amp;②判别学习：直接预测P(Y|X) \\end{aligned} ​①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)​ 二、极大似然估计 Maximum Likelihood Estimation (MLE) 2.1 简单场景：掷硬币 我们投十次硬币，假设投掷结果为：D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T}，则我们一般会进行以下预测： nH=4,nT=6,P(H)=θ≈nHnH+nT=0.4n_H=4,n_T=6,P(H)=\\theta\\approx\\frac{n_H}{n_H+n_T}=0.4 nH​=4,nT​=6,P(H)=θ≈nH​+nT​nH​​=0.4 2.2 形式化定义 上述掷硬币的例子就是极大似然估计的过程，对于MLEMLEMLE，一般分为两步： ①对分布类型进行明确的建模假设 ②设置分布中所涉及的参数 对于掷硬币问题的分布，我们易知这是一个经典的二项分布，他有两个参数：抛硬币次数nnn，某个事件（例如：硬币正面朝上）发生的概率θ\\thetaθ，我们不妨假设 P(H)=θ ~P(H)=\\theta~ P(H)=θ ，则有： P(D∣θ)=CnH+nTnH⋅θnH⋅(1−θ)nT\\begin{aligned} &amp;P(D|\\theta)=C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}\\\\ \\end{aligned} ​P(D∣θ)=CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​​ P(D∣θ) P(D|\\theta)~P(D∣θ) 表示θ\\thetaθ为某个值时，抛硬币结果为DDD的概率，比如D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T} MLEMLEMLE的规则：对于固定的事件DDD，找到一个θ\\thetaθ使得P(D∣θ)P(D|\\theta)P(D∣θ)最大： θ^MLE=argmaxθ P(D∣θ)=argmaxθ CnH+nTnH⋅θnH⋅(1−θ)nT=argmaxθ (nHln⁡θ+nTln⁡(1−θ))\\begin{aligned} &amp;\\hat{\\theta}_{MLE}=\\underset{\\theta}{argmax}~P(D|\\theta)=\\underset{\\theta}{argmax}~C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}=\\underset{\\theta}{argmax}~(n_H\\ln\\theta+n_T\\ln(1-\\theta)) \\end{aligned} ​θ^MLE​=θargmax​ P(D∣θ)=θargmax​ CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​=θargmax​ (nH​lnθ+nT​ln(1−θ))​ 我们对函数进行求导即可得到θ\\thetaθ的极大似然估计值： 令f(θ)=nHln⁡θ+nTln⁡(1−θ)df(θ)dθ=nHθ−nT1−θ=0→θ=nHnH+nT,即θ^MLE=nHnH+nT\\begin{aligned} &amp;令f(\\theta)=n_H\\ln\\theta+n_T\\ln(1-\\theta)\\\\ &amp;\\frac{df(\\theta)}{d\\theta}=\\frac{n_H}{\\theta}-\\frac{n_T}{1-\\theta}=0\\rightarrow \\theta=\\frac{n_H}{n_H+n_T},即\\hat{\\theta}_{MLE}=\\frac{n_H}{n_H+n_T} \\end{aligned} ​令f(θ)=nH​lnθ+nT​ln(1−θ)dθdf(θ)​=θnH​​−1−θnT​​=0→θ=nH​+nT​nH​​,即θ^MLE​=nH​+nT​nH​​​ 可以发现极大似然估计的预测结果与我们的直观预测相同，这就是MLEMLEMLE 三、先验估计 3.1 简单场景：掷硬币 我们仍可以用掷硬币的场景去理解先验估计：假设我们预感θ\\thetaθ接近0.50.50.5。但我们的样本量很小，所以我们对此估计并不确信，可以作如下处理： θ^=nH+mnH+nT+2m\\hat{\\theta}=\\frac{n_H+m}{n_H+n_T+2m} θ^=nH​+nT​+2mnH​+m​ 当nnn很大时，该处理对θ\\thetaθ的影响微不足道；但是当nnn较小时，该处理可以使得θ\\thetaθ更接近我们的猜测。 3.2 形式化定义 假设θθθ是根据分布P(θ)P(θ)P(θ)得到的一个随机值，DDD为一个事件，则有如下贝叶斯公式： P(θ∣D)=P(D∣θ)P(θ)P(D)=P(D,θ)P(D)P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D,\\theta)}{P(D)} P(θ∣D)=P(D)P(D∣θ)P(θ)​=P(D)P(D,θ)​ ①P(θ)P(θ)P(θ)是我们在看到数据之前θθθ的先验分布 ②P(D∣θ)P(D|\\theta)P(D∣θ)是对于给定的参数θ\\thetaθ，事件DDD发生的可能性 ③P(θ∣D)P(\\theta|D)P(θ∣D)是我们观察数据后得到的θ\\thetaθ的后验分布 我们常常利用BetaBetaBeta分布得到θθθ的先验分布： P(θ)=θα−1(1−θ)β−1B(α,β)(其中B(α,β)=Γ(α)Γ(β)Γ(α+β),其目的是对P(θ)进行归一化)P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\ (其中B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)},其目的是对P(\\theta)进行归一化) P(θ)=B(α,β)θα−1(1−θ)β−1​(其中B(α,β)=Γ(α+β)Γ(α)Γ(β)​,其目的是对P(θ)进行归一化) P(θ∣D)∝P(D∣θ)P(θ)∝θnH+α−1(1−θ)nT+β−1P(\\theta|D)\\propto P(D|\\theta)P(\\theta)\\propto\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1} P(θ∣D)∝P(D∣θ)P(θ)∝θnH​+α−1(1−θ)nT​+β−1 四、极大后验估计 在我们知道关于θ\\thetaθ的分布，可以利用极大后验估计得到θ\\thetaθ的估计值 MAPMAPMAP规则：对于一个固定的事件DDD，找到一个θ\\thetaθ，使得P(θ∣D)P(\\theta|D)P(θ∣D)最大： θ^MAP=argmaxθ P(θ∣D)=argmaxθ P(D∣θ)P(θ)P(D)=argmaxθ P(D∣θ)P(θ) =argmaxθ (θnH+α−1(1−θ)nT+β−1)=argmaxθ (nH+α−1)ln⁡θ+(nT+β−1)ln⁡(1−θ) =nH+α−1nH+nT+α+β−2\\begin{aligned} &amp;\\hat\\theta_{MAP}=\\underset{\\theta}{argmax}~P(\\theta|D)=\\underset{\\theta}{argmax}~\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\underset{\\theta}{argmax}~P(D|\\theta)P(\\theta)\\\\ &amp;~~~~~~~~~~~=\\underset{\\theta}{argmax}~(\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1})=\\underset{\\theta}{argmax}~(n_H+\\alpha-1)\\ln\\theta+(n_T+\\beta-1)\\ln(1-\\theta)\\\\ &amp;~~~~~~~~~~~~=\\frac{n_H+\\alpha-1}{n_H+n_T+\\alpha+\\beta-2} \\end{aligned} ​θ^MAP​=θargmax​ P(θ∣D)=θargmax​ P(D)P(D∣θ)P(θ)​=θargmax​ P(D∣θ)P(θ) =θargmax​ (θnH​+α−1(1−θ)nT​+β−1)=θargmax​ (nH​+α−1)lnθ+(nT​+β−1)ln(1−θ) =nH​+nT​+α+β−2nH​+α−1​​ 当n→∞n\\rightarrow\\inftyn→∞时，α−1\\alpha-1α−1和β−2\\beta-2β−2与nHn_HnH​和nTn_TnT​相比可以忽略，即θ^MAP→θ^MLE\\hat\\theta_{MAP}\\rightarrow\\hat\\theta_{MLE}θ^MAP​→θ^MLE​ 五、总结 在监督学习中有一个数据集DDD，我们运用它来训练模型，它有一个参数 θθθ，利用这个模型我们希望对测试点xtx_txt​进行预测，则有如下方法： MLE:P(y∣xt;θ) learning:θ^MLE=arg⁡max⁡θP(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt,θ) learning:θ^MAP=arg⁡max⁡θP(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt,θ)=∫θP(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的\\begin{aligned} &amp;MLE:P(y|x_t;\\theta)~learning:\\hat\\theta_{MLE}=\\arg\\max_\\theta P(D|\\theta),此处的\\theta为一个模型参数\\\\ &amp;MAP:P(y|x_t,\\theta)~learning:\\hat\\theta_{MAP}=\\arg\\max_\\theta P(\\theta|D)\\propto P(D|\\theta)P(\\theta)，此处的\\theta为随机变量\\\\ &amp;True~Bayesian:P(y|x_t,\\theta)=\\int_\\theta P(y|\\theta)P(\\theta|D)d\\theta,此处的\\theta是考虑所有可能模型积分出来的 \\end{aligned} ​MLE:P(y∣xt​;θ) learning:θ^MLE​=argθmax​P(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt​,θ) learning:θ^MAP​=argθmax​P(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt​,θ)=∫θ​P(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的​ ","link":"https://2006wzt.github.io/post/机器学习实战（五）：概率估计/"},{"title":"机器学习实战（四）：感知机算法","content":"感知机算法 一、形式化定义 在机器学习中，感知机是一种用于处理监督学习的二分类问题的分类器，它是一种线性分类器，即一种基于线性预测函数（将一组权重与特征向量相结合）进行预测的分类算法，该模型有着如下的假设： ①处理的问题是二分类问题：e.g. yi∈{−1,+1}②数据是线性可分的\\begin{aligned} &amp;①处理的问题是二分类问题：e.g.~~y_i\\in\\{-1,+1\\}\\\\ &amp;②数据是线性可分的 \\end{aligned} ​①处理的问题是二分类问题：e.g. yi​∈{−1,+1}②数据是线性可分的​ 感知机模型最终所要求解的是权重向量www，其形式化定义如下： 权重向量：w=[w1,w2,...,wd]T特征向量：x=[x1,x2,...,xd]T解空间：H={h(x)=wTx+b=0}\\begin{aligned} &amp;权重向量：w=[w_1,w_2,...,w_d]^T\\\\ &amp;特征向量：x=[x_1,x_2,...,x_d]^T\\\\ &amp;解空间：\\mathcal{H}=\\{h(x)=w^Tx+b=0\\} \\end{aligned} ​权重向量：w=[w1​,w2​,...,wd​]T特征向量：x=[x1​,x2​,...,xd​]T解空间：H={h(x)=wTx+b=0}​ 二、感知机实现 2.1 权重向量 根据形式化定义，我们知道我们的求解目标是一个权重向量www，最终得到模型函数：h(x)=wTx+bh(x)=w^Tx+bh(x)=wTx+b 即我们求解得到www对应的是多维空间中的一个超平面wTx+b=0w^Tx+b=0wTx+b=0，该平面在多维空间中将数据点分为两部分 而我们最后的分类只需要根据h(x)h(x)h(x)的正负判断数据点在哪一侧即可。 权重向量www即为所要求解的超平面的法向量 2.2 偏差项 我们注意到模型函数中还有一项常数bbb，我们称之为偏差项，如果没有偏差项，www所定义的超平面一定经过原点。 为了便于后续处理，我们通过升维操作将偏差项bbb吸收到www中去： 目标超平面：wTx+b=0令x′=[x 1],w=[w′ b]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}\\begin{aligned} &amp;目标超平面：w^Tx+b=0\\\\ &amp;令x&#x27;=\\left[ \\begin{aligned} &amp;x\\\\ &amp;~1 \\end{aligned} \\right],w=\\left[ \\begin{aligned} &amp;w&#x27;\\\\ &amp;~b \\end{aligned} \\right]，则有w&#x27;^Tx&#x27;=w^Tx+b=0\\\\ &amp;由此解空间变为：\\mathcal{H}=\\{h(x)=w^Tx=0\\} \\end{aligned} ​目标超平面：wTx+b=0令x′=[​x 1​],w=[​w′ b​]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}​ 通过升维后超平面一定是经过原点的，则有： 设yi∈{−1,+1},h(xi)&gt;0↔ yi=+1,h(xi)&lt;0↔ yi=−1可得：yi⋅h(xi)=yi⋅(wTxi)&gt;0↔xi分类正确\\begin{aligned} &amp;设y_i\\in\\{-1,+1\\},h(x_i)&gt;0\\leftrightarrow~y_i=+1,h(x_i)&lt;0\\leftrightarrow~y_i=-1\\\\ &amp;可得：y_i\\cdot h(x_i)=y_i\\cdot(w^Tx_i)&gt;0\\leftrightarrow x_i分类正确 \\end{aligned} ​设yi​∈{−1,+1},h(xi​)&gt;0↔ yi​=+1,h(xi​)&lt;0↔ yi​=−1可得：yi​⋅h(xi​)=yi​⋅(wTxi​)&gt;0↔xi​分类正确​ 注意我们尽量将二分类问题的标签设置为{−1,+1}\\{-1,+1\\}{−1,+1}，如果设置为{0,+1}\\{0,+1\\}{0,+1}则没有上述结论。 2.3 算法实现 首先我们先看感知机算法的伪码： mmm用于记录在训练集上分类错误的次数，如果出错则m=m+1m=m+1m=m+1，同时对权重向量www进行调整，调整方式为w=w+yxw=w+yxw=w+yx，该调整方式的几何解释如下图所示： 如上图所示，图一为初始的权重向量以及其所对应的超平面，我们以一个标签为−1-1−1的数据点为例，该超平面错误得将该数据点分到了+1+1+1的一侧（即yi⋅(wTxi)≤0y_i\\cdot(w^Tx_i)\\le0yi​⋅(wTxi​)≤0），则要进行调整：w=w+yx=w−xw=w+yx=w-xw=w+yx=w−x，则得到了如图三所示的调整后的超平面。 数学解释如下，w和bw和bw和b的更新是一个梯度下降的过程，而我们用到的损失函数为： L(w,b)=−∑xi∈Dyi(wTxi+b)L(w,b)=-\\sum_{x_i\\in D}y_i(w^Tx_i+b) L(w,b)=−xi​∈D∑​yi​(wTxi​+b) 当分类错误时会有 yi⋅(wTxi+b)≤0 ~y_i\\cdot(w^Tx_i+b)\\le0~ yi​⋅(wTxi​+b)≤0 ，自然就会使得该损失函数变大，该函数对www和bbb求偏导得： ∂L(w,b)∂w=−∑xi∈Dyixi∂L(w,b)∂b=−∑xi∈Dyi\\frac{\\partial L(w,b)}{\\partial w}=-\\sum_{x_i\\in D}y_ix_i\\\\ \\frac{\\partial L(w,b)}{\\partial b}=-\\sum_{x_i\\in D}y_i ∂w∂L(w,b)​=−xi​∈D∑​yi​xi​∂b∂L(w,b)​=−xi​∈D∑​yi​ 由此我们得到更新过程： w→w+yixib→b+yiw\\rightarrow w+y_ix_i\\\\ b\\rightarrow b+y_i w→w+yi​xi​b→b+yi​ 三、感知机的收敛 感知机可以说是第一个具有强大形式保证的算法。如果有数据集合是线性可分的，感知机将在有限更新次数中找到一个可以分离两类数据点的超平面，如果数据不是线性可分的，它将永远循环。 假设∃w∗有:对于∀(xi,yi)∈D，yi⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi∈D,∣∣w∗∣∣=∑i=1n(wi∗)2=1,∣∣xi∣∣≤1对于w∗对应的超平面，令γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣，即离超平面最近的点对应的距离\\begin{aligned} &amp;假设∃w^∗有:对于∀(x_i, y_i) ∈ D，y_i\\cdot(x^Tw^∗) &gt; 0\\\\ &amp;对每个数据进行等比例收敛使得：对于\\forall x_i\\in D,||w^*||=\\sqrt{\\sum_{i=1}^n(w^*_i)^2}=1,||x_i||\\le1\\\\ &amp;对于w^*对应的超平面，令\\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||，即离超平面最近的点对应的距离 \\end{aligned} ​假设∃w∗有:对于∀(xi​,yi​)∈D，yi​⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi​∈D,∣∣w∗∣∣=i=1∑n​(wi∗​)2​=1,∣∣xi​∣∣≤1对于w∗对应的超平面，令γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣，即离超平面最近的点对应的距离​ 由此，我们得到一个关于更新次数和γ\\gammaγ之间关系的定理： 定理 3-1 感知机算法最多对超平面进行1γ2次调整感知机算法最多对超平面进行\\frac1{\\gamma^2}次调整 感知机算法最多对超平面进行γ21​次调整 此处的调整次数（更新次数）即为上述伪码中for循环中执行 w=w+yx ~w=w+yx~ w=w+yx 的次数，对上述定理的证明如下： γ\\gammaγ是基于上述w∗w^*w∗所对应的超平面得来的，证明过程主要讨论wTw∗w^Tw^*wTw∗和wTww^TwwTw的大小关系： 1)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw∗=(w+yixi)Tw∗=(wT+xiTyiT)w∗=wTw∗+yi⋅(xiTw∗)∵γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣ 且 yi⋅(xiTw∗)&gt;0 ∴yi⋅(xiTw∗)≥γ∴wTw∗=wTw∗+yi⋅(xiTw∗)≥wTw∗+γ2)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw=(w+yixi)T(w+yixi)=wTw+2yi⋅(wTxi)+yi2xiTxi∵yi∈{−1,+1} 且 yi⋅(wTxi)≤0,数据收敛后∣∣xi∣∣2≤1 ∴ yi2=1,xiTxi≤1∴ wTw=wTw+2yi⋅(wTxi)+yi2xiTxi≤wTw+1\\begin{aligned} 1)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw^*=(w+y_ix_i)^Tw^*=(w^T+x_i^Ty_i^T)w^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\\\ &amp;\\because \\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||~且~ y_i\\cdot(x_i^Tw^*)&gt;0~\\therefore y_i\\cdot(x_i^Tw^*)\\ge \\gamma\\\\ &amp;\\therefore w^Tw^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\ge w^Tw^*+\\gamma\\\\ 2)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw=(w+y_ix_i)^T(w+y_ix_i)=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\\\ &amp;\\because y_i\\in\\{-1,+1\\}~且~y_i\\cdot(w^Tx_i)\\le0,数据收敛后||x_i||_2\\le1~\\therefore~y_i^2=1,x_i^Tx_i\\le 1\\\\ &amp;\\therefore~w^Tw=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\le w^Tw+1 \\end{aligned} 1)2)​如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw∗=(w+yi​xi​)Tw∗=(wT+xiT​yiT​)w∗=wTw∗+yi​⋅(xiT​w∗)∵γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣ 且 yi​⋅(xiT​w∗)&gt;0 ∴yi​⋅(xiT​w∗)≥γ∴wTw∗=wTw∗+yi​⋅(xiT​w∗)≥wTw∗+γ如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw=(w+yi​xi​)T(w+yi​xi​)=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​∵yi​∈{−1,+1} 且 yi​⋅(wTxi​)≤0,数据收敛后∣∣xi​∣∣2​≤1 ∴ yi2​=1,xiT​xi​≤1∴ wTw=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​≤wTw+1​ 调整的过程就是www向w∗w^*w∗趋近的过程，在算法的forforfor循环中，如果进行了调整，则wTw∗w^Tw^*wTw∗至少增加了γ\\gammaγ，wTww^TwwTw至多增加了1 设总共调整了M次，则wTw≤M≤wTw∗γ∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw≤M∴M≤1γ2\\begin{aligned} &amp;设总共调整了M次，则w^Tw\\le M\\le\\frac{w^Tw^*}{\\gamma}\\\\ &amp;\\because ||w^*||=1~\\therefore~\\gamma M\\le w^Tw^*\\le||w^T||\\cdot||w^*||=||w||=\\sqrt{w^Tw}\\le\\sqrt{M}\\\\ &amp;\\therefore M\\le\\frac1{\\gamma^2} \\end{aligned} ​设总共调整了M次，则wTw≤M≤γwTw∗​∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw​≤M​∴M≤γ21​​ 四、算法实现 根据上述伪码，我们将通过手动实现与调库的方式，分别实现状态机。 4.1 数据集 在本次算法实战中，我们将使用到的数据集为sklearn所提供的乳腺癌数据集，我们首先先来了解一下数据集的内容： 最终我们得到的数据集如下图所示，总共有569个数据，每条数据有30个特征，标签0，1分别代表是否患癌症。 4.2 手动实现模型 参考伪码，我们可以实现感知机的fitfitfit函数，在此基础上我们引入了两个新的量lr和max\\text{_}iter，学习率lrlrlr控制每次对超平面的更新程度，最大训练轮数max\\text{_}iter避免数据线性不可分时出现死循环。 上述的参数得到的准确率为0.9035087719298246，我们可以通过调参过程使得准确率趋于最优。 4.3 调库实现模型 模型的训练主要用到sklearn所提供的库函数Perceptron，它的函数原型如下： ①penalty：正则化项，l2l2l2、l1l1l1或弹性网络。 ②alpha：正则化项系数，如果使用正则化项，则在正则化项前乘上该系数 ③fit_intercept：是否需要计算截距 ④max_iter：最大训练轮数 ⑤tol：训练的停止标准，训练将在loss&gt;previous\\text{_}loss-tol时停止 ⑥shuffle：是否在每轮训练后对训练数据进行随机排列 ⑦eta0：学习率 4.4 可视化 我们不妨仅选取两个特征对模型进行训练，观察感知机模型最终得到的超平面划分是什么样的，我们利用make_classification创造具有两个特征的用于二分类的数据集，调用库函数进行预测： ","link":"https://2006wzt.github.io/post/机器学习实战（四）：感知机算法/"},{"title":"机器学习实战（三）：KNN算法","content":"K-NN算法 一、形式化定义 K-NN算法（k-nearest neighbor）是一种基本的分类与回归算法，在本节我们主要以分类问题为例介绍K-NN算法。 K-NN模型是一个非参数化模型，参数数量随着训练数据的规模增长，更加的灵活，但是却又较高的计算成本。 ①模型假设：相似的输入有着相似的输出 ②分类规则：对于测试输入x，在其k个最相似的训练输入之间分配最常见的标签 ③回归规则：输出是对象的属性值，该值是k个最近邻值的平均值。 设训练集为DDD，输入的测试点的特征向量为xxx，xxx的kkk个近邻点表示为集合SxS_xSx​，SxS_xSx​有如下性质： ①Sx∈D②∣Sx∣=k③∀(x′,y′)∈D−Sx,dist(x,x′)≥max⁡(x′′,y′′)∈Sxdist(x,x′′)\\begin{aligned} &amp;①S_x\\in D\\\\ &amp;②|S_x|=k\\\\ &amp;③\\forall (x&#x27;,y&#x27;)\\in D-S_x,\\text{dist}(x,x&#x27;)\\ge \\max_{(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x}\\text{dist}(x,x&#x27;&#x27;) \\end{aligned} ​①Sx​∈D②∣Sx​∣=k③∀(x′,y′)∈D−Sx​,dist(x,x′)≥(x′′,y′′)∈Sx​max​dist(x,x′′)​ 根据分类规则，我们可以得到模型函数： h(x)=mode({y′′:(x′′,y′′)∈Sx})mode返回集合中出现频率最高的标签y′′h(x)=\\text{mode}(\\{y&#x27;&#x27;:(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x\\})\\\\ \\text{mode}返回集合中出现频率最高的标签y&#x27;&#x27; h(x)=mode({y′′:(x′′,y′′)∈Sx​})mode返回集合中出现频率最高的标签y′′ 根据上述定义，我们可以通过下图更直观得认识KNN算法，当K=1时，新的数据点被分类为正方形，当K=3时，新的数据点被分为三角形 二、参数选择 2.1 距离函数 K-NN分类器基本上依赖于距离函数的选择，距离度量越能反映标签的相似性，分类器的性能就越好。 最为常用的距离函数为闵可夫斯基距离（Minkowski distance），设数据点的特征向量为ddd维向量，则(x,y)(x,y)(x,y)和(x′,y′)(x&#x27;,y&#x27;)(x′,y′)之间的距离为： dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ 其中x=(x1,x2,...,xd),x′=(x1′,x2′,...,xd′)x=(x_1,x_2,...,x_d),x&#x27;=(x&#x27;_1,x&#x27;_2,...,x&#x27;_d)x=(x1​,x2​,...,xd​),x′=(x1′​,x2′​,...,xd′​)，当ppp取值不同时，得到不同的距离函数。 ①p=2p=2p=2时，为欧氏距离： dist(x,x′)=∑r=1d∣xr−xr′∣2\\text{dist}(x,x&#x27;)=\\sqrt{\\sum_{r=1}^d|x_r-x&#x27;_r|^2} dist(x,x′)=r=1∑d​∣xr​−xr′​∣2​ 欧几里得度量（Euclidean Metric）是一个通常采用的距离定义，指在d维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离），在二维和三维空间中的欧氏距离就是两点之间的实际距离。 ②p=1p=1p=1时，为曼哈顿距离： dist(x,x′)=∑r=1d∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\sum_{r=1}^d|x_r-x&#x27;_r| dist(x,x′)=r=1∑d​∣xr​−xr′​∣ 想象你在城市道路里，要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。 实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源，曼哈顿距离也称为城市街区距离（City Block distance）。 ③p=∞p=\\inftyp=∞时，为切比雪夫距离： dist(x,x′)=max⁡r∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\max_r|x_r-x&#x27;_r| dist(x,x′)=rmax​∣xr​−xr′​∣ 二个点之间的切比雪夫距离定义是其各坐标数值差绝对值的最大值。 国际象棋棋盘上二个位置间的切比雪夫距离是指王要从一个位子移至另一个位子需要走的步数。由于王可以往斜前或斜后方向移动一格,因此可以较有效率的到达目的的格子。上图是棋盘上所有位置距f6f6f6位置的切比雪夫距离。 2.2 K参数 我们应该如何选择一个合适的K值使得分类器的效果达到最优？ 1）一个较小的K值： ①减少在学习过程中的近似误差，即减小在训练集上的误差 ②扩大在学习过程中的估计误差，即增大在测试集上的误差 ③会使得模型更加复杂，容易导致过拟合 2）一个较大的K值： ①减少在学习过程中的估计误差，即减小在测试集上的误差 ②扩大在学习过程中的近似误差，即增大在训练集上的误差 ③会使得模型更加简单 因此我们需要对K值进行一个权衡，这取决于所提供的数据集。通常，较大的k值会减少噪声对分类的影响，但会使类之间的边界不那么明显，在二分类问题中，选择k为奇数有助于避免并列。 三、特殊的K-NN分类器 3.1 1-NN分类器 当K=1K=1K=1时，我们得到的分类器便是1-NN分类器，这是一个最为直观的K-NN分类器，选取离得最近的那个数据点的标签作为预测标签。 1-NN边界划分的方法：选定每一个点最近的那个邻居，作它们连线间的中垂线，中垂线相连形成边界，得到的分类边界如下图所示 3.2 贝叶斯最优分类器 假如我们知道了任何xxx对应的yyy，即知道分布P(y∣x)P(y|x)P(y∣x)（这在现实中不可能），这样对于一个xxx你就可以简单地预测最有可能的标签，Bayes optimal classifier预测为： y^=hopt(x)=argmaxy P(y∣x)\\hat y=h_{opt}(x)=\\underset{y}{argmax}~P(y|x) y^​=hopt​(x)=yargmax​ P(y∣x) 即：对于一个xxx，取概率最高的标签作为预测的标签，Bayes optimal classifier 仍然可能出错，它的出错率即为： ϵBayesOpt=1−P(hopt(x)∣x)=1−P(y^∣x)\\epsilon_{BayesOpt}=1-P(h_{opt}(x)|x)=1-P(\\hat y|x) ϵBayesOpt​=1−P(hopt​(x)∣x)=1−P(y^​∣x) 例：假设一个二分类问题，标签仅有+1和-1两种，并且对于某个xxx，有： P(+1∣x)=0.8,P(−1∣x)=0.2P(+1|x)=0.8,P(-1|x)=0.2 P(+1∣x)=0.8,P(−1∣x)=0.2 因此根据Bayes optimal classifier可得： y^=+1,ϵBayesOpt=0.2\\hat y=+1,\\epsilon_{BayesOpt}=0.2 y^​=+1,ϵBayesOpt​=0.2 由此我们可以知道贝叶斯最优分类器的错误率是在给定数据分布时的最小可实现错误率。同时我们也能得到如下定理： 定理 3-1 证明如下： 如上图所示，在数据集趋近于无穷大时，对于任何一个数据点，它与K个近邻的距离趋近于0，即： 设测试点为xt，它的近邻为xNN,当数据集大小n→∞时，dist(xt,xNN)→0xt→ xNN,因此xt预测的标签将为xNN的标签\\begin{aligned} &amp;设测试点为x_t，它的近邻为x_{NN},当数据集大小n\\rightarrow\\infty时，\\text{dist}(x_t,x_{NN})\\rightarrow0\\\\ &amp;x_t\\rightarrow~x_{NN},因此x_t预测的标签将为x_{NN}的标签 \\end{aligned} ​设测试点为xt​，它的近邻为xNN​,当数据集大小n→∞时，dist(xt​,xNN​)→0xt​→ xNN​,因此xt​预测的标签将为xNN​的标签​ 如上图所示，我们以垃圾邮件分类为例，SpamSpamSpam为垃圾邮件，HamHamHam为正常邮件，假设xNNx_{NN}xNN​的标签为SpamSpamSpam，则有： ϵBayesOpt=1−P(s∣x)ϵ1−NN=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt\\begin{aligned} &amp;\\epsilon_{BayesOpt}=1-P(s|x)\\\\ &amp;\\epsilon_{1-NN}=P(s|x)(1-P(s|x))+(1-P(s|x))P(s|x)=2(1-P(s|x))P(s|x)&lt;2(1-P(s|x))=2\\epsilon_{BayesOpt} \\end{aligned} ​ϵBayesOpt​=1−P(s∣x)ϵ1−NN​=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt​​ 四、维度灾难 当向量的维度很高时，KNN模型将变得非常不稳定。 4.1 数据点之间的距离 首先让我们想象一个ddd维的单位超立方体[0,1]d[0,1]^d[0,1]d，所有的数据都在此超立方体内均匀采样，即对于任意的xix_ixi​，都有xi∈[0,1]dx_i\\in[0,1]^dxi​∈[0,1]d。接着我们不妨考虑一下K=10K=10K=10时，对于一个测试点它的近邻所在的超立方体，如下图所示： 设 l ~l~ l 是包含KKK个近邻的最小超立方体的边长， n ~n~ n 是取样的数据点数，因为数据集是在该单位超立方体中均匀取样，则有： ld=kn,l=(kn)1dl^d=\\frac{k}{n},l=(\\frac{k}{n})^{\\frac1d} ld=nk​,l=(nk​)d1​ 对于K=10,n=1000K=10,n=1000K=10,n=1000时的情况，则有： d l 2 0.1 10 0.63 100 0.955 1000 0.9954 因此，当维度很高（d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0）时，几乎需要整个空间才能找到某个测试点的10个近邻。 从而导致最近的KKK个邻居并不比训练集中其他的点离测试点更近。 这是因为，在ddd维空间中随机分布的点之间的距离逐渐趋近于一个很小的范围内，如下图所示： 4.2 到超平面的距离 对于二分类问题，我们往往是在ddd维空间中寻找一个超平面，将空间一分为二，超平面两侧的数据即为不同的标签。 如上图所示，维度灾难对于两点之间的距离和点与超平面之间的距离有不同的影响。 同一维度中的移动不能增加或减少到超平面的距离，点只是四处移动并与超平面保持相同的距离。但随着维度的升高，成对点之间的距离变得非常大，到超平面的距离变得相对较小。 维数灾难的一个后果是，大多数数据点往往非常接近这些超平面，并且通常可能轻微的扰动就会更改分类。 4.3 数据降维 为了解决维度灾难，最好的方法便是对数据进行降维操作，我们以图像识别为例：虽然一张脸的图像可能需要1800万像素，但是我们可能只需要用少于50个属性（例如男性/女性、金发/深色头发等）就可以描述一个人，这些属性会随着脸的变化而变化。 如上图所示，这是从底层二维流形绘制的三维数据集示例。蓝色点位于粉色表面之上，而粉色表面则嵌入在三维空间之中，粉色表面映射为二维平面，进而将数据集从三维映射到二维，实现数据降维。 五、算法实战 本次我们使用Scikit-learn所提供的鸢尾花数据集，通过调库的方式实现K-NN算法。 5.1 数据集 我们首先了解一下该数据集的特点，读取数据并将其存储为csv文件的方式如下： 我们最终得到的数据集如下图所示，总共有150个数据，每条数据有4个特征，分别为花萼长度、花萼宽度、花瓣长度、花瓣宽度，相应的不同特征所对应的标签共有3种，分别为setosa、versicolor、virginica，即鸢尾花的三个种类。 5.2 模型训练 模型的训练主要用到sklearn所提供的库函数KNeighborsClassifier，它的函数原型如下： ①n_neighbors：即K值的大小，默认为5 ②weights：用于预测的权重函数。可选参数如下: ③algorithm：计算最近邻居用的算法。可选参数如下： 球树与kd树将会在后续文章中进行介绍，这是一种优化KNN算法的方式。 ④leaf_size：传入BallTree或者KDTree算法的叶子数量，此参数会影响构建、查询BallTree或者KDTree的速度，以及存储BallTree或者KDTree所需要的内存大小。 ⑤p：用于Minkowski metric的超参数，即当我们用闵可夫斯基距离作为度量时，p值的大小。 dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ ⑥metric：计算距离的方式，默认为闵可夫斯基距离。 我们不妨先用简单的欧几里得距离构建模型，观察模型的预测效果： 在这里准确率的计算方法比较简单，即Accuracy=预测准确的数据点总测试点数Accuracy=\\frac{预测准确的数据点}{总测试点数}Accuracy=总测试点数预测准确的数据点​ 上述模型中，我们最终在测试集上得到的准确率为0.9666666666666667，可以看出模型的效果已经非常好了。 5.3 可视化 接下来让我们进行一些调参过程，观察能否让准确率再高一些，首先调整KKK值，对于每个KKK值我们进行十折交叉验证： 如上图所示，是在KKK在[1,30][1,30][1,30]上取不同值时十折交叉验证的准确率，因为是随机对数据集进行划分，所以每次所得的曲线图是不同的，不过其大致趋势是相同的，我们最终可以得到KKK取到[12,18][12,18][12,18]能得到最高准确率的概率最高。 而其他参数的调参过程完全可以效仿上述K值调参可视化的过程，只需要修改需要遍历的取值范围以及参数名即可，即这部分代码： 5.4 特征重要性 因为在鸢尾花数据集中仅有4个特征，所以我们在此讨论特征重要性其实是没必要的，但是对于具有很多特征的数据集我们则需要依据特征的重要性对特征进行筛选，一方面可以提高预测的准确性，另一方面可以提高模型的效率，我们不妨先观察鸢尾花数据集的4个特征对分类准确性的影响。 我们可以通过各个特征对所对应的标签分布直观得判断特征的重要性，可视化代码如下： 我们任取两个特征，观察特征对所对应的数据分布即可判断特征的重要性，如上图所示，是我们选择petal_length,petal_width两个特征可视化的结果，可以发现三种标签的分布并没有过多的交织，说明这两个特征对于分类是较为重要的。 4个特征所对应的6个特征对的标签分布如下图所示，由此我们可以大致判断特征的重要性： 六、总结 1）如果距离可以可靠得反映相异性，则KNN是一个简单高效的模型。 2）在数据集大小n→∞n\\rightarrow\\inftyn→∞时，KNNKNNKNN模型将变得非常精确，但是也会变得非常缓慢。 3）当数据维度d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0时，会发生维度灾难 4）优点：①没有关于数据的假设，例如：对非线性数据非常有用 ②算法简单，易于理解 ③具有较高的精度，但是与更好的监督学习模型相比没有竞争力 ④用途广泛，适用于分类以及回归问题 5）缺点：①具有较高的计算成本，因为算法要用到所有的训练数据 ②具有较高的内存要求，需要存储几乎所有的训练数据 ③对无关特征和数据规模较为敏感，存在维度灾难 ","link":"https://2006wzt.github.io/post/机器学习实战（三）：KNN算法/"},{"title":"机器学习实战（二）：监督学习","content":"监督学习 一、形式化定义 在监督学习的过程中，我们输入的训练数据是成对输入的(x,y)(x,y)(x,y)，x∈Rdx\\in R_dx∈Rd​是输入实例，yyy是其标签，整个训练数据表示为： D={(x1,y1),...,(xn,yn)}⊆Rd×CD = \\{(x_1, y_1), . . . ,(x_n, y_n)\\} ⊆ R_d × C D={(x1​,y1​),...,(xn​,yn​)}⊆Rd​×C 其中RdR_dRd​是ddd维特征空间，CCC是标签空间，xix_ixi​是第iii个样本的特征向量，yiy_iyi​是第i个样本的标签 数据点(xi,yi)(x_i,y_i)(xi​,yi​)来自一些未知的分布P(X,Y)P(X,Y)P(X,Y) 最终我们希望学习出一个模型函数hhh，对于一个新的数据点(x,y)(x,y)(x,y)，我们有较高概率的 h(x)=y 或 h(x)≈y~h(x)=y~或~h(x)\\approx y h(x)=y 或 h(x)≈y 监督学习的标签空间决定了问题的类型，典型的标签空间如下： Type Lable Space E.g. 二分类 C={0,1} or C={−1,+1}C = \\{0, 1\\}~or~C = \\{−1, +1\\}C={0,1} or C={−1,+1} E.g. 垃圾邮件过滤问题. 一个邮件要么是垃圾邮件(-1)要么不是(+1) 多分类 C={1,2,⋅⋅⋅,K}(K≥2)C = \\{1, 2, · · · , K\\} (K ≥ 2)C={1,2,⋅⋅⋅,K}(K≥2) E.g. 人脸识别问题.一个人的身份可以是K个身份中的一个 回归 C=RC = RC=R E.g.预测某一天的温度或者某个人的身高 二、损失函数 2.1 定义 什么是损失函数？顾名思义，它是一个用于评估模型对于数据集拟合的损失程度的函数，我们希望预测的效果越差，损失函数的值越大，这也为我们在搭建模型算法的过程中提供了方向：我们要尽可能得使得模型的损失函数输出最小化，以达到较好的拟合效果。 同时在优化过程中，损失函数输出的值变化也可以说明我们的在模型优化上的进展。 事实上，我们可以设计一个非常基本的损失函数来进一步解释它是如何工作的。对于我们所做的每个预测，我们的损失函数将简单地测量预测值和实际值之间的绝对差，即： L(h)=1n∑i=1n∣h(xi)−yi∣L(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| L(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 其中LLL是损失函数，hhh是我们训练出的模型函数，nnn是验证集的样本数量，h(xi)h(x_i)h(xi​)是特征向量xix_ixi​的预测标签，yiy_iyi​则是实际的标签，显然当我们预测完全准确时L(h)=0L(h)=0L(h)=0，预测值与实际值差别越大，损失函数的值越大，这满足损失函数的特性。 2.2 经典的损失函数 2.2.1 Zero-one loss 0-1损失函数的形式化定义如下： L1/0(h)=1n∑i=1nδh(xi)=yi对于分类问题：δh(xi)=yi={1,if h(xi)=yi0,o.w.对于回归问题：δh(xi)=yi={1,if ∣h(xi)−yi∣&gt;t0,o.w.L_{1/0}(h)=\\frac1n\\sum_{i=1}^n\\delta_{h(x_i)\\not=y_i}\\\\ 对于分类问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~h(x_i)\\not=y_i\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right.\\\\ 对于回归问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~|h(x_i)-y_i|&gt;t\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right. L1/0​(h)=n1​i=1∑n​δh(xi​)​=yi​​对于分类问题：δh(xi​)​=yi​​={​1,if h(xi​)​=yi​0,o.w.​对于回归问题：δh(xi​)​=yi​​={​1,if ∣h(xi​)−yi​∣&gt;t0,o.w.​ 根据δ\\deltaδ函数的定义，当我们的预测值准确时，会给损失函数加上0，对于不准确的预测值则会给损失函数加上1 0-1损失函数也有明显的缺点，对于分类问题影响并不大，但是对于回归问题，ttt是某个自定义的阈值，δ\\deltaδ函数对于所有的误差大于阈值的惩罚相同，即错误的预测所带来的惩罚都为1，对于一些很离谱的预测（比如1预测成了10000）并没有额外的惩罚。 2.2.2 Squared loss 平方损失函数的形式化定义如下： Lsq(h)=1n∑i=1n(h(xi)−yi)2L_{sq}(h)=\\frac1n\\sum_{i=1}^n(h(x_i)-y_i)^2 Lsq​(h)=n1​i=1∑n​(h(xi​)−yi​)2 平方损失函数常用于分类问题，它有两个主要特点： ①损失函数的值永远是非负的 ②损失函数的值与预测的绝对误差呈二次关系 显然平方损失函数规避了0-1损失函数的缺点，对于预测误差较大的样本，所带来的惩罚也越大，但同时对于预测较准确的点，所带来的惩罚也会变得更小，对于噪声数据的处理会使得模型的效果变差。 2.2.3 Absolute loss 绝对损失函数的形式化定义如下： Labs(h)=1n∑i=1n∣h(xi)−yi∣L_{abs}(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| Labs​(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 绝对损失函数的值随着预测失误而线性增长，因此更适合于噪声数据 三、泛化能力 学习方法的泛化能力，是指由该学习方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。 对于给定的一个损失函数LLL，我们可以得到一个使得损失函数最小化的模型hhh，即： h=argminh∈H L(h)h=\\underset{h\\in H}{argmin}~L(h) h=h∈Hargmin​ L(h) 机器学习的很大一部分集中在这个问题上，即如何有效地进行最小化。如果我们得到一个模型函数h(⋅)h(·)h(⋅)，它在我们的训练数据DDD上的损失很小，我们该如何确定它在DDD之外的数据上的损失也很小呢？这就是泛化的问题，一个泛化能力较差的模型如下： Bad example： h(x)={yi,(xi,yi)∈D∩x=xi0 ,o.w.h(x)=\\left\\{ \\begin{aligned} &amp;y_i,(x_i,y_i)\\in D \\cap x=x_i\\\\ &amp;0~,o.w. \\end{aligned} \\right. h(x)={​yi​,(xi​,yi​)∈D∩x=xi​0 ,o.w.​ 对于这个模型函数，我们在训练的数据集DDD上的误差为0，但是对于数据集DDD外的数据点，显然将会有很大的误差，这就是过拟合问题。 四、过拟合 什么是过拟合？ 在模型的监督学习过程中，我们往往是在训练集上设计模型，在测试集上评估模型的准确性，如上图所示，图1是欠拟合的情况，即模型在训练集上也没有很好的准确性，因此在测试集上的准确性也不会高；图2是较为合适的拟合，它规避了噪声的影响，较为准确得对两类点进行了划分；图3则是过拟合的情况，它最大化了模型在训练集上的准确度而忽略了噪声点的影响，使得模型在训练集上表现得很好，但是在测试集上却表现不佳。 在西瓜书中也有一个比较形象的例子： 五、训练与测试 5.1 定义 训练是使得模型可以学习的过程，而测试则是模型进行预测的过程，训练集的标签是已知的，即可观察的，而测试集的正确标签则是未知的，我们的模型要从已知的特征与标签的对应关系中进行学习，然后对未知的测试集进行预测。 **No free lunch rule：**一个模型hah_aha​即使在某些问题上比另一个模型hbh_bhb​好，也必然存在另一些问题使得hbh_bhb​效果比hah_aha​好 5.2 数据集的划分 对于所提供的数据集，为了训练模型，我们往往需要对数据集DDD进行划分，一般划分成训练集DTRD_{TR}DTR​、验证集DVAD_{VA}DVA​、测试集DTED_{TE}DTE​三部分。 一般的划分比例为80%（DTRD_{TR}DTR​），10%（DVAD_{VA}DVA​），10%（DTED_{TE}DTE​），对于不同的数据集可以进行调整。 为什么我们需要验证集DVAD_{VA}DVA​？ DVAD_{VA}DVA​用于检查从DTRD_{TR}DTR​中获得的模型函数h(⋅)h(\\cdot)h(⋅)是否存在过拟合的问题，也就是我们所追求的目标不只是hhh在训练集DTRD_{TR}DTR​上的损失最小化，也要兼顾在验证集DVAD_{VA}DVA​上的损失最小化，如果h(⋅)h(\\cdot)h(⋅)在DVAD_{VA}DVA​上的损失很大，h(⋅)h(·)h(⋅)将根据DTRD_{TR}DTR​进行修订，并在DVAD_{VA}DVA​上再次验证。该过程将不断来回，直到在DVAD_{VA}DVA​上产生低损失，在验证集上的误差是接近泛化误差的。 在DTRD_{TR}DTR​和DVAD_{VA}DVA​的大小之间有一个权衡：对于较大的DTRD_{TR}DTR​，训练结果会更好，但如果DVAD_{VA}DVA​较大，验证会更可靠（噪音更少）。 对于监督学习实际的应用问题，我们一般只需要在提供的已知标签的数据集上进行训练集与验证集的划分，根据所提供的数据集的特点，有如下的划分规则： ①含有时间成分的数据集：一定要遵循过去预测未来的规则，不能利用未来的数据去预测过去的数据。 ②不含有时间成分的数据集：可以均匀随机得进行划分 利用验证集进行模型评估的常用方法为k折交叉验证，其原理如下图所示： 六、总结 1）学习的过程： Learning:h(⋅)=argminh∈H 1∣DTR∣∑(x,y)∈DTRL(x,y∣h(⋅))Learning:h(\\cdot)=\\underset{h\\in \\mathcal{H}}{argmin}~\\frac1{|D_{TR}|}\\sum_{(x,y)\\in D_{TR}}L(x,y|h(\\cdot)) Learning:h(⋅)=h∈Hargmin​ ∣DTR​∣1​(x,y)∈DTR​∑​L(x,y∣h(⋅)) 2）评估的过程： Evaluation:ϵTE=1∣DTE∣∑(x,y)∈DTEL(x,y∣h(⋅))Evaluation:\\epsilon_{TE}=\\frac{1}{|D_{TE}|}\\sum_{(x,y)\\in D_{TE}}L(x,y|h(\\cdot)) Evaluation:ϵTE​=∣DTE​∣1​(x,y)∈DTE​∑​L(x,y∣h(⋅)) 根据监督学习的上述基本原理，我们将在后续文章中开始相关算法的实战！ ","link":"https://2006wzt.github.io/post/机器学习实战（二）：监督学习/"},{"title":"机器学习实战（一）：机器学习导论","content":"机器学习导论 一、什么是机器学习 机器学习是人工智能的一个分支，涉及算法的设计和开发，允许计算机根据经验数据进化行为。由于智能需要知识，计算机必须获取知识。Tom Mtichell于1997年对机器学习的定义：一个计算机程序A，从经验E中学习关于某个任务T的性能度量P，E有助于提高计算机在任务T上的性能表现，这是基于统计和优化的，而不是基于逻辑的。 二、ML VS CS 机器学习的核心是程序，但是最终目标是结果。以垃圾邮件分类为例，我们所要研究的是进行分类的算法，但是我们的最终目标是邮件的类型。而传统的计算机科学则是根据一定的逻辑规则由输入得到正确的输出。 三、基本分类 3.1 监督学习 在监督学习（supervised learning）的过程中，算法从我们所提供的数据集中进行学习。 监督学习与非监督学习的最大区别在于我们知道用于训练的数据集的正确结果或期望输出，算法对验证集进行预测，由监督者对预测结果进行评估与纠正，进而优化算法，当算法达到可接受的性能水平时，则停止学习。 监督学习有两种主要类型：分类与回归 3.1.1 分类问题 顾名思义，就是根据特征对事物进行分类，一些经典的分类问题例子： ①垃圾邮件过滤问题：根据邮件内容对邮件是否为垃圾邮件进行分类，这是一个二分类问题。 ②人脸识别问题：显然这就是一个多分类问题，根据照片中所包含的数据判断出人脸所对应的身份 3.1.2 回归问题 回归用于预测输入和输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值也随之发生变化，经典的回归问题如下： ①房价预测：根据所提供的相关特征对某个楼盘的房价进行预测 ②天气预测：根据相关指标对未来的天气，例如PM2.5值进行预测 3.2 非监督学习 无监督学习（unsupervised learning）是一类用于在数据中寻找模式的机器学习技术。无监督学习算法使用的输入数据都是没有标注过的，这意味着数据只给出了输入变量（自变量 X）而没有给出相应的输出变量（因变量y）。 在无监督学习中，算法本身将发掘数据中有趣的结构。人工智能研究的领军人物 Yann LeCun，解释道：无监督学习能够自己进行学习，而不需要被显式地告知他们所做的一切是否正确。 非监督学习最为经典的例子便是聚类问题。 3.2.1 聚类问题 顾名思义，聚类就是根据所提供的相关数据，对具有相似特征的事物进行分类，比如照片的类型、网页主题的聚类。 在后期的学习中我们主要关注监督学习的相关算法。 3.3 强化学习 强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优化行为策略的机器学习问题，强化学习的本质是学习最优的序贯决策。 智能系统与环境的互动如下图所示，在每一步t，智能系统从环境中观测到一个状态（state）sts_tst​与一个奖励rtr_trt​，采取一个动作（action）ata_tat​。环境根据只能系统选择的动作，决定下一步t+1的状态st+1s_{t+1}st+1​与奖励rt+1r_{t+1}rt+1​。要学习的策略表示为给定的状态下采取的动作。 智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化，强化学习的过程中系统不断试错，以达到学习最优策略的目的。 3.4 半监督学习 半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。通常有少量标注数据、大量未标注数据，因为标注数据的构成往往需要人工，成本较高，未标注的数据的收集则不需要太多成本，半监督学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。 四、经典的机器学习过程 如下图所示，这是一个属于监督学习的二分类问题的机器学习过程，通过所提供的训练数据使用学习算法训练出模型，再将我们所要预测的不带标签（label）的新数据样本输入到模型中去，进而预测出该样本所属的类别。 在后续的博客中，我将会介绍机器学习中的重要概念与相关算法，还请大家多多支持与关注😄！ ","link":"https://2006wzt.github.io/post/机器学习实战（一）：机器学习导论/"},{"title":"Python入门与实战","content":"Python 一.Python初探 1.Python语言的基本要素 （1）符号和注释 ①符号：需要用英文字符,程序前面不能随便加空格 ②注释：单行注释用#开头，选中部分Ctrl+/可以代码和注释间相互转化 （2）变量 变量有名字，其值可以存储数据 （3）赋值语句 变量=表达式，使变量的值变得跟表达式一样 2.初步认识字符串 （1）字符串初步 字符串可以且必须用单引号，双引号和三引号括起来；当字符串太长时，可以用\\进行分行输出 三双引号字符串中可以包含换行符，制表符等其他字符，双引号中也可以有\\n 有n个字符的字符串，编号从左向右为0-&gt;n-1，从右往左编号为-1-&gt;-n 字符串可以用+号连接 注意字符串不可以修改，在初始化后不可以修改该字符串中任何一个字符 可以用in和not in判断子串 （2）字符串和数的转换 3.简单的输入输出 （1）输入输出初步 输出语句print()，可以输出一项或多项，输出多项时用，隔开，输出效果为用空格隔开，输出后自动换行 有时不想换行可以在print的括号最后加上end=&quot;&quot;,缺省的情况下为end=&quot;\\n&quot; 输入语句x=input(y)，先输出y再等待输入，最后将输入的内容以字符串的形式赋值给x。 要求y一定要是字符串，且赋值输入字符串给x时不带最后的回车 4.初步认识列表 （1）列表初步 列表可以有0到多个元素，并且可以通过下标访问，下标的概念与字符串相同，列表中的元素类型可以不同 可以用in判断列表中是否有某个元素 输入两个整数并求和 二.基本运算、条件分支和输出格式控制 1.算术运算、逻辑运算和分支语句 （1）算术运算 +，-，*，/结果为小数，%取模，//求商结果为整数，**求幂 （2）关系运算符，逻辑运算符，逻辑表达式 ①关系运算符：==,&lt;,&lt;=,&gt;,&gt;=,!=，比较结果为bool值，True或者False，字符串按照字典排序进行比较 ②逻辑运算符：and（与），or（或），not（非），False等于0，True等于1，优先级not&gt;and&gt;or 0，&quot;&quot;(空字符串)，[]（空表）都相当于False；非0数，非空字符串，非空列表都相当于True ③逻辑表达式：逻辑表达式是短路计算的，当表达式的值可以确定时则会停止计算 （3）条件分支语句 字符串切片：若s为一个字符串，则s[x:y]为从下标x到下标y-1的子串 2.输出格式控制 输出格式控制符与C语言的大致相同，且输出格式控制符仅能作用在字符串的输出中 三.循环语句 1.for循环语句 forin: ​ 令variable的值依次是sequence里面的每一个元素，并对每个variable值进行操作statements1 2.break、continue语句 break语句用于跳出循环，continue语句用于直接进入下一层循环，其用法与C语言类似 下为一些for循环语句的例题: 3.while循环语句 4.异常处理 常见的异常有： （1）不合适的转换：int(&quot;abc&quot;),float(&quot;abc&quot;),int(12.34) （2）输入已经结束后还执行input() （3）除法的除数为0 （4）整数和字符串相加 （5）列表下标越界 5.循环综合例题 四.函数和递归 1.函数的概念和用法 函数中的变量：在函数中定义的变量不在函数外起到作用，如果函数内的变量x与全局变量x重名，则如果在函数中未对变量x进行赋值，则认为x为全局变量，反之则为函数内部的变量，在函数内可以用global x声明x为全局变量 python还有很多内置函数，除了前面所讲，还有abs(x)求x绝对值，max(x),min(x)求列表x中元素的的最大最小值，max(x1,x2...),min(x1,x2...)等 2.递归的概念 五.字符串和元组 1.Python变量的指针本质 isinstance函数： python中的变量都是指针，赋值的过程即为指针指向某个内存的过程，列表中的元素也可以进行赋值，因此列表中的每个元素都是指针 is运算符和==运算符的区别： ①a is b 为True代表a，b指向同一个内存 ②a==b代表a，b指向内存中的值相等 2.字符串详解 转义字符：\\与其后面的一个字符一同构成，\\n为换行符，\\t为制表符等等 字符串切片：s[x:y]代表下标x到y-1的子串，字符串切片的方法也适用于元组和列表 字符串的分割：s.split(x)为以x为分隔符，将字符串s分割成一个列表，列表中的每个元素为字符串的子串 字符串的函数： 字符串编码：字符串的编码在内存中的编码是unicode的 字符串的格式化： 3.元组 元组：一个元组由数个逗号分隔的值组成，前后的括号可加可不加 元组与列表的区别在于：元组不可修改，不可增删元素，不可对元素进行赋值，不可修改元素的排序，对元组进行处理的速度比列表快 注意元组的元素的内容可以被修改。 元组的元素本质上是指针，元组的元素不可修改指的是元组元素的指向不可被改变，但指向的内容可修改 元组的下标访问、切片规则以及运算法则与字符串完全相同 元组的运算、迭代和赋值 元组比大小：类似于字符串按照字典顺序和数字大小一一对应进行比较，但是数字于字符串不能比大小，此时会报错 我们可以用列表或元组取代复杂的分支结构 六.列表 1.列表的基本操作 列表是可以增删元素的，并且列表中的元素也可以进行修改 列表的运算 列表的切片 2.列表的排序 选择排序：基础的排序算法，但是效率较低 python自带的排序函数： 对于不同的需求，我们可以自定义比较函数用在sort函数上 3.复杂列表的自定义排序 （1）lambda表达式 lambda x:x[2]，表示一个函数，参数是x，返回值是x[2]，相当于是一个匿名函数 （2）自定义排序 （3）元组的排序 元组的元素不能被修改，因此没有相应的sort函数，但是有sorted函数可以作用于元组，但是返回值是一个列表 4.列表和元组的高级用法 列表的相关函数： 列表映射： 列表过滤： 列表生成式： 二维列表的定义： 列表的拷贝： 列表的深拷贝： 元组和列表的互转： 元组列表和字符串的互转： 5.例题 七.字典和集合 1.字典的基本概念 字典中的每个元素由“键：值”两部分组成，每个元素的键是唯一且不与其他元素的键相同的，可以根据键对元素进行快速查找 键必须是不可变的数据类型，如字符串，整数，小数，元组；而列表，集合，字典等可变数据类型不能作为字典元素的键 字典的增删元素： 字典元素不能重复，如果重复则保留后一个出现的元素： 字典的构造： 2.字典的相关函数 遍历字典： 字典的浅拷贝与深拷贝： 3.字典例题 4.集合 集合的定义同数学上的集合，集合中元素类型可以不同，不会有重复的元素，可以增删元素，集合中的元素类型应为可变数据类型如：字符串，整数，小数，复数，元组，而不能为列表、字典和集合 集合的作用是可以快速地判断某个元素是否在一堆元素里面 集合的构造： 集合常用的函数： 集合的逻辑运算： 集合的比较： 八.文件读写和文件夹操作和数据库 1.文本文件的读写 创建文本文件并写入内容： 读取现有文件： 在文件中添加内容： 2.文本文件的编码 常见文件的编码有gbk和utf-8两种，ANSI对应gbk，创建和读写文件时都可以指定编码，如果不指定则默认为缺省的编码 .py文件必须存成utf-8格式才能运行，如果存成ansi格式（即为gbk格式），应在文件开头写#coding =gbk 3.文件的路径 相对路径：文件路径没有包含盘符 当前文件夹：一般来说.py文件所在的文件夹就是当前文件夹 绝对路径：文件路径指明了盘符 4.文件夹操作 python的文件夹和文件操作函数： 有时文件夹非空但也想要删除文件夹，则不能调用os.rmdir(x)，可以自己定义删除文件夹的函数： 获取文件夹总大小也可以同理自己定义函数： 5.命令行参数 Win+R并输入cmd即可打开控制台，输入python ×××.py即可运行×××.py，注意需要先将控制台转到.py当前文件夹下 当.py文件需要输入参数时，我们希望可以直接在命令行直接输入，可进行如下操作： 6.文件处理实例 7.数据库和SQL语言简介 （1）数据库： 数据库可以存放大量数据，并且提供了方便快捷的检索手段，一个数据库可以是一个文件 一个数据库中可以有多张表，每张表又又不同的字段，字段又有其相应的类型 （2）SQL语言： SQL命令是用于进行数据库操作的标准语句 用sqlite3.exe打开.db文件时，输入select * from 表名;即可显示表中信息，注意一定要加分号 （3）数据库的查询和修改 数据库的查询： 数据库的修改： （4）数据库二进制字段处理 九.正则表达式 1.正则表达式的概念 正则表达式是某些字符有特殊含义的字符串，可以用相关函数用来判断其他字符串是否能与正则表达式进行匹配 正则表达式中的功能字符：\\d等不是转义字符，都是两个字符 正则表达式中的特殊字符：. + ? * ( ) [ ] { } ^ \\ $，如果要在正则表达式中表示这几个字符本身，应该在前面加\\ 2.字符范围和量词 字符的复合： 正则表达式示例： 3.正则表达式的函数 使用正则表达式要import re (1)re.match(pattern,string,flags=0)：从string的起始位匹配一个模式pattern，flags标志位用于控制模式串的匹配方式，如果匹配的上则返回一个匹配对象，否则返回None (2)re.search(pattern,string,flags=0)：查找字符串中第一个可以匹配成功的子串，查找成功返回匹配对象，否则返回None (3)re.findall(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个列表，如果匹配失败则返回空表 (4)re.finditer(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个序列，序列中的每个元素都是一个匹配对象 4.边界符号 边界符号本身不与任何字符匹配 5.分组 括号中的一个表达式就是分组，多个分组按照左括号，从左到右从1开始编号 在分组的右边可以通过分组的编号引用该分组所匹配的子串 分组作为一个整体，后面可以跟量词，且不一定需要匹配相同的字符串 当正则表达式中没有分组时，re.findall返回所有匹配子串构成的列表 当正则表达式有且仅有一个分组时，re.findall返回的是一个子串的列表，每个元素是一个匹配子串中分组对应的内容 当正则表达式中有超过一个分组时，re.findall返回的是一个元组的列表，元组中的每个元素依次是各个分组的内容 6.|的用法 |表示或，如果没有放在分组中，|的起作用范围是直到整个正则表达式开头或结尾的另一个”|“ |从左到右短路匹配，当匹配到左边的后不会再匹配右边 |也可以用在分组中，起作用的范围就仅限于分组 7.贪婪模式和懒惰模式 （1）贪婪模式 量词+,*,?,{m,n}都默认匹配尽可能长的子串，存在很明显的弊端 （2）懒惰模式 即为非贪婪模式，在量词+,*,?,{m,n}后面加上？则匹配尽可能短的子串 8.匹配对象的函数 十.玩转Python生态 1.用datatime库处理日期、时间 处理日期： 处理时刻： 2.用random库处理随机事务 设置随机数种子：如果没有设置则是按照系统时间作为随机数种子，随机数种子设定后每次随机结果都是唯一的 random库应用实例： 3.用jieba库进行分词和中文词频统计 不仅可以将一个词加入jieba的字典，也可以将一个文件中的词加入jieba的字典 4.用openpyxl库处理excel文档 （1）用openpyxl库读取excel文档 有时候单元格中的内容是公式，希望打开文档的时候自动计算公式的值，则可进行如下操作： （2）用openpyxl库创建excel文档 （3）用openpyxl库设定excel文档单元格样式 4.用Pillow处理图像 （1）图像基本常识： 图像由像素构成：屏幕上每个像素点由三个距离非常近的点构成，分别显示红绿蓝三种颜色，每个像素可以由一个元组表示(r,g,b)， r，g，b通常是不大于255的整数，分别表示红绿蓝三原色的深浅程度 图像模式： ①RGB：一个像素有红绿蓝三个分量 ②RGBA：一个像素有红绿蓝三个分量以及透明分量 ③CYMK：一个像素有青色（Cyan），洋红色（Magenta），黄色（Yellow），黑色（K代表黑）四个分量构成，每个像素用元组(c,y,m,k)表示，对应于彩色打印机或者印刷机的四种颜色墨水 L：黑白图像，像素就是一个整数，代表灰度 （2）图像的基本操作： ①图像的缩放： ②图像的旋转、翻转和滤镜效果： ③图像的裁剪： ④图像素描化： ⑤为图像添加水印： 原理：在将一个图像粘贴到另一个图像上时，会用到paste函数，paste函数还有一个参数mask称为“掩膜”指定img的每个像素粘贴过去的透明度，如果透明度为0则完全透明，如果透明度为255则完全遮盖原图像的像素 mask本质上是一个模式为“L”的图片即一个Image对象 十一.数据分析和展示 1.numpy库的使用 numpy是一个多维数组库，创建多维数组很方便，可以代替多维列表，速度比多维列表快，支持向量和矩阵的各种数学运算，所有元素类型必须相同 （1）用numpy库创建数组 （2）numpy常用的属性与函数 （3）numpy数组元素增删 numpy数组一旦生成，则不能增删元素，此处的增删指的是增删之后生成一个新数组，原数组保持不变 numpy增添数组元素： numpy删除数组元素： （4）在numpy数组中查找元素 （5）numpy数组的数学运算 （6）numpy数组的切片 不同于列表的切片，numpy数组的切片是“视图”，即为原数组的一部分，而非原数组一部分的拷贝，切片改变原数组也改变 2.数据分析库pandas pandas的核心功能是在二维表格上做各种操作，需要numpy库的支持，在openpyxl库的支持下还可以读取excel文档 pandas中最为关键的类是DataFrame，用于表示二维表格 （1）pandas的重要类Series Series为一维表格，每个元素带有标签与下标，类似于列表和字典的结合 （2）DataFrame的构造和访问 DataFrame是带行列标签的二维表格，它的每一行都是一个Series （3）DataFrame的切片、增删和统计 ①DataFrame的切片： ②DataFrame的分析统计： ③DataFrame的修改和增删： 修改和增加： 删除： （4）用pandas库读取excel和csv文档 需要依赖于openpyxl库 ①用pandas读excel文档：读取的每张工作表都是一个DataFrame ②用pandas写excel文档 3.用matplotlib进行数据展示 在此感慨：清华镜像源yyds！！！ （1）绘制直方图 （2）绘制堆叠直方图 （3）绘制对比直方图 （4）绘制折线图和散点图 （5）绘制饼图 （6）绘制热力图 热力图用于展示二维数据： （7）绘制雷达图 （8）一个窗口绘制多幅子图 十二.网络爬虫设计 1.爬虫的用途和原理 1）爬虫的用途： ①在网络上搜集数据（搜索引擎） ②模拟浏览器快速操作（如抢票，选课） ③模拟浏览器操作，替代填表等重复操作 2）最基本的爬虫写法：数据获取型爬虫的本质就是自动获取网页并抽取其中的内容 ①手工找出合适的url（网址） ②用浏览器手工查找url对应的网页，并查看网页源码，找出包含想要内容的字符串的模式 ③在程序中获取url对应的网页 ④在程序中用正则表达式或BeautifulSoup库抽取网页中想要的内容并保存 上述代码原来if语句写的是： if not(x.lower().endswith(&quot;.jpg&quot;) or x.lower().endswith(&quot;.jpeg&quot;) or x.lower().endswith(&quot;.png&quot;)): continue#只取.png和.jpg图片 但是发现爬取失败，经过查看各个图片网址可以发现，各个图片网址格式为：u=623374478,4175757280&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG (500×658) (baidu.com) 因此并不能正确得爬取图片，因此对if语句进行了修改，查找字符串中得jpeg，jpg和png用于判断是否进行爬取 同时文件名的处理也做了相应的修改，最终爬取成功！！！ 局限是requests库获取网络资源很容易被反爬虫，且不能获取JavaScript编写的动态网页源代码，可以尝试pyppeteer库，速度快且暂未被许多网站反爬 2.用pyppeteer库获取网页 pyppeteer的工作原理： ①启动一个浏览器Chromium，用浏览器装入网页 ②从浏览器可以获取网页源代码，如果网页有JavaScript程序，获取到的是JavaScript被浏览器执行后的网页源代码 ③可以向浏览器发出命令，模拟用户在浏览器上的键盘输入、鼠标点击等操作，让浏览器转到其他网页 十三.面向对象程序设计 1.类和对象的概念 类：类是用来代表事物的，对于一种事物，可以用一个类来概括其属性 对象：类的实例称为对象，类主要用于将数据和操作数据的函数捆绑在一起，便于当作一个整体使用 2.对象的比较 python中所有的类都有eq,ne等方法用于比较（注意双下划线） 自定义对象的比较： 3.继承与派生 定义一个新类B时，如果发现B类具有A类的全部特点，则可以在定义B类时以A为基类，定义B类为A类的派生类 object类：python中所有类都是object类的派生类，因而具有object类的各种属性和方法 4.静态属性和静态方法 静态属性：静态属性被所有对象所共享，一共只有一份 静态方法：静态方法不是作用在具体的某个对象上，因此不能访问非静态属性 静态属性和静态方法这种机制存在的目的就是少写全局变量和全局函数 5.对象作为集合元素或字典的键 （1）可哈希：可哈希的东西才可以作为字典的键和集合的元素，hash(x)有定义即hash(x)有返回值，说明x是可哈希的 （2）哈希值和字典、集合的关系 字典和集合都是一种称为”哈希表“的数据结构，根据元素的哈希值为元素寻找存放的槽，哈希值可以看作是槽编号，一个槽中可以放多个哈希值相同的元素。 两个对象的哈希值不同可以作为同一集合的不同元素和同一字典的不同键，当hash(a)==hash(b)但是a!=b时，a，b也可以作为同一集合的不同元素和同一字典的不同键，自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址），而非对象的值，可以重写自定义类的hash方法 若 dt 是个字典，dt[x] 计算过程如下： 根据hash(x)去找x应该在的槽的编号 如果该槽没有元素，则认为dt中没有键为x的元素 如果该槽中有元素，则试图在槽中找一个元素y，使得 y的键 == x。 如果找到，则dt[x] 即为 y的值，如果找不到，则dt[x]没定义，即 认为dt中不存在键为x的元素 （3）自定义类的对象是否可哈希 自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址） 如果为自定义的类重写了eq(self,other)成员函数，则其 hash成员函数会被自动设置为None。这种情况下，该类就变成不可哈希的 一个自定义类，只有在重写了eq方法却没有重写hash方法的情况下，才是不可哈希的。 十四.tkinter图形界面程序设计 1.控件概述 控件(widgets)： 按钮、列表框、单选框、多选框、编辑框.... 布局 ：如何将控件摆放在窗口上合适的位置 事件响应： 对鼠标点击、键盘敲击、控件被点击等操作进行响应 对话框 ：弹出一个和用户交互的窗口接受一些输入 tkinter的扩展控件： from tkinter import ttk，tk中的控件ttk都有且更加美观，并且ttk中还有一些tk不具有的控件 2.布局基础 用grid进行布局，grid布局在窗口上布置网格，控件放在网格单元里面居中摆放 默认情况下的grid规则： 1）一个单元格只能放一个控件，控件在单元格中居中摆放。 2）不同控件高宽可以不同，因此网格不同行可以不一样高，不同列也 可以不一样宽。但同一行的单元格是一样高的，同一列的单元格也 是一样宽的。 3）一行的高度，以该行中包含最高控件的那个单元格为准。单元格的 高度，等于该单元格中摆放的控件的高度（控件如果有上下留白， 还要加上留白的高度）。列宽度也是类似的处理方式。 4）若不指定窗口的大小和显示位置，则窗口大小和网格的大小一样， 即恰好能包裹所有控件；显示位置则由Python自行决定。 3.使用Frame控件进行布局 1）控件多了，要算每个控件行、列、rowspan,columnspan很麻烦 2）Frame控件上面还可以摆放控件，可以当作底板使用 3）可以在Frame控件上面设置网格进行Grid布局，摆放多个控件 4.控件属性和事件响应 （1）控件属性 1）有的控件有函数可以用来设置和获取其属性，或以字典下标的形式获取和设置其属性 2）有的控件必须和一个变量相关联，取变量值或设置变量值，就是取或设置该控件的属性 （2）事件响应 1）创建有些控件时，可以用command参数指定控件的事件响应函数 2）可以用控件的bind函数指定事件响应函数 示例： 5.Python实例：火锅店点菜系统 十五.Python游戏设计 外星人入侵主要依靠pygame库进行设计，在此仅附上代码 1.aline_invasion.py 2.game_stats.py 3.scoreboard.py 4.button.py 5.aline.py 6.ship.py 7.settings.py 8.bullet.py 十六.用opencv进行人脸识别 1.读取图片、灰度转换、修改尺寸 2.绘制图形 3.人脸检测 详解detectMultiScale： 此方法的任务是检测不同大小的对象，并返回矩形的列表。 第1个参数img需要是灰度图片 第2个参数scaleFactor，很重要 第3个参数minNeighbors，很重要 第4个参数flag=0即可 第5，6个参数minSize和maxSize 设置检测对象的最大最小值，低于minSize和高于maxSize的话就不会检测出来。参数类型为二元组，指定矩形的最小范围和最大范围 scaleFactor默认为1.1，Haar cascade的工作原理是一种“滑动窗口”的方法，通过在图像中不断的“滑动检测窗口”来匹配人脸。 因为图像的像素有大有小，图像中的人脸因为远近不同也会有大有小，所以需要通过scaleFactor参数设置一个缩小的比例，对图像进行逐步缩小来检测，这个参数设置的越大，计算速度越快，但可能会错过了某个大小的人脸。可以根据图像的像素值来设置此参数，像素大缩小的速度就可以快一点，通常在1~1.5之间。 minNeighbors默认为3，指定每个候选矩形有多少个“邻居”，指定每个候选矩形有多少个“邻居”，也可以理解为检测次数，只有检测minNeighbors次，某处都识别为人脸才会显示出来矩形 4.视频人脸检测 5.人脸录入 6.数据训练 7.人脸识别 十七.scikit-learn实现机器学习 1.机器学习介绍及其原理 ①人工智能：就其本质而言，是及其对人思维信息过程的模拟，让它能像人一样去思考 人工智能可以根据输入信息进行模型结构，权重更新，实现最终优化 特点：信息处理，自我学习，优化升级 ②人工智能的核心方法：机器学习，深度学习 机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术 机器学习：使用算法来解析数据，从中学习，然后对真实世界中的事件进行决策和预测 深度学习：模仿人类的神经网络，建立模型，进行数据分析 ③机器学习分类： 监督式学习：基于数据及结果进行预测 非监督式学习：从数据中挖掘关联性 强化学习：强调如何基于环境而行动以获取最大利益 ④监督式学习核心步骤： 1.使用标签数据训练机器学习模型 ●&quot;标签数据&quot; 是指由输入数据对应的正确的输出结果 ●&quot;机器学习模型&quot;将学习输入数据与之对应的输出结果间的函数关系 2.调用训练好的机器学习模型,根据新的输入数据预测对应的结果 相比于监督式学习,非监督式学习不需要标签数据,而是通过引入预先设定的优化准则进行模型训练,比如自动将数据分为三类 2.机器学习开发环境部署 1）Scikit-learn：是python语言中专门针对机器学习应用而发展起来的开源框架，可以实现数据预处理，分类，回归，降维，模型选择等常用的机器学习算法 2）Jupyter notebook 3）Anaconda 3.机器学习实现之数据预处理 1）Iris数据集： 2）使用scikit-learn进行数据处理的四个关键点 ①区分开属性数据和结果数据 ②属性数据和结果数据都是量化的 ③运算过程中，属性数据与结果数据的类型都是numpy数组 ④属性数据与结果数据的维度是对应的，即行数应该对应 4.机器学习实现之模型训练 1）分类：根据数据集目标的特征或属性，划分到已有的类别中 2）常用的算法：K近邻（KNN），逻辑回归，决策树，朴素贝叶斯 3）K近邻分类模型（KNN）：给定一个训练数据集，对新的输入实例，在训练的数据集中找到与该实例最邻近的K个实例，这个K实例的多数属于某一类，就把该输入实例归到这个类中 4）使用scikit-learn进行建模的四个步骤： ①调用需要使用的模型类 ②模型初始化：创建一个模型实例 ③模型训练 ④模型预测 5.机器学习实现之模型评估 1）为了对模型的准确率进行评估，我们不能用全部的数据进行训练，因为这样无法判断模型的准确性，不妨将数据集分为训练集与测试集两部分，一部分用于训练，一部分用于测试，这样便于对模型进行评估，下面是不进行数据分离的结果，显然K=1时，正确率100% 2）下面进行数据分离然后进行模型评估 3）确定最合适的K值，遍历所有可取的K值 K值越小，模型越复杂，并且因为每次数据分离是随机的，训练出的结果也不相同 6.逻辑回归模型 1）逻辑回归模型：用于解决分类问题的一种模型。根据数据特征或属性,计算其归属于某一类别的概率P(x) ,根据概率数值判断其所属类别。主要应用场景: 二分类问题。 2）皮马印第安人糖尿病数据集 注意数据集的特点是有缺失数据 3）使用准确率进行模型评估的局限性：没有体现数据的实际分布情况，没有体现模型错误预测的类型 空准确率：当模型总是预测比较高的类别，其预测准确率的数值，即比例较高的类别在模型中所占的比例 4）混淆矩阵 比F1更具一般形式的Fβ′:Fβ′=(1+β2)×P×R(β2×P)+R①β=1:标准的F1②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2\\begin{aligned} &amp;比F_1更具一般形式的F_{\\beta&#x27;}:\\\\ &amp;F_{\\beta&#x27;}=\\frac{(1+\\beta^2)\\times P\\times R}{(\\beta^2\\times P)+R}\\\\ &amp;①\\beta=1:标准的F_1\\\\ &amp;②\\beta&lt;1:偏精确率P\\\\ &amp;③\\beta&gt;1:偏召回率R\\\\ &amp;\\beta最常用的值为0.5，1，2 \\end{aligned} ​比F1​更具一般形式的Fβ′​:Fβ′​=(β2×P)+R(1+β2)×P×R​①β=1:标准的F1​②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2​ 计算混淆矩阵进行模型评估 ","link":"https://2006wzt.github.io/post/Python入门与实战/"}]}