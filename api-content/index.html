{"posts":[{"title":"机器学习实战（十四）：树模型","content":"树模型 树模型在机器学习中至关重要，它不仅本身具有较好的性能，也可以用于优化其他的算法。 我们在本节将要介绍优化 KNN ~KNN~ KNN 算法的树模型以及决策树。 一、 KNN ~KNN~ KNN 的数据结构 在KNN算法中我们要找到测试点的最近的K个邻居，但是这需要我们求解所有点与测试点之间的距离（我们称这个过程为线性扫描），在数据集很大时这显然是不合理的，为此我们需要在此讨论以下KNN算法的数据结构。 1.1 时间复杂度 我们首先回顾一下 KNN ~KNN~ KNN 算法的时间复杂度，设数据集大小为 n ~n~ n ，特征向量维度为 d ~d~ d ，则对一个点进行分类的时间复杂度为： O(nd) ~O(nd)~ O(nd) 显然，随着数据集的增大，计算量将变得巨大，导致算法运行速度很慢，这并不是我们想看到的 我们希望找到一个较好的数据结构，使得对测试点进行分类时不再需要遍历每一个点。 1.2 KD ~KD~ KD 树 KD ~KD~ KD （K-Dimensional）树是一种对 k ~k~ k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构，它是一种二叉树，表示对 k ~k~ k 维空间的一个划分。 构造 KD ~KD~ KD 树相当于不断地用超平面将 k ~k~ k 维空间划分，构成一系列的 k ~k~ k 维超矩形区域， KD ~KD~ KD 树的每一个结点对应一个超矩形。 1.2.1 构造KD树 构造 KD ~KD~ KD 树的方法如下： ① 构造根节点，根节点对应特征空间中包含所有实例点的超矩形区域。 ② 在超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，由此确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前的超矩形区域分为左右两个子区域，这时该超矩形内的实例被分到了两个子区域，生成两个子节点。 ③ 对每个结点重复执行②操作直到子区域内不再存在实例，由此得到的结点为叶子结点。 构造 KD ~KD~ KD 树的算法的形式化定义如下： 输入： k~k k维空间数据集为： D={(x1,y1),(x2,y2),...,(xn,yn)}D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\} D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)} 其中我们将特征向量对应的数据集记为： T={x1,x2,...,xn}T=\\{x_1,x_2,...,x_n\\} T={x1​,x2​,...,xn​} 其中的特征向量为 k ~k~ k 维向量： xi=[ xi(1),xi(2),...,xi(k) ]Tx_i=[~x_i^{(1)},x_i^{(2)},...,x_i^{(k)}~]^T xi​=[ xi(1)​,xi(2)​,...,xi(k)​ ]T （1）开始：构造根节点：根节点对应包含 T ~T~ T 的 k ~k~ k 维空间的超矩形区域。 选择 x(1) ~x^{(1)}~ x(1) 为坐标轴，以 T ~T~ T 中所有实例的 x(1) ~x^{(1)}~ x(1) 坐标的中位数为切分点，将根节点对应的超矩形区域分为两个子区域。 切分由通过切分点并与坐标轴 x(1) ~x^{(1)}~ x(1) 垂直的超平面实现，由根节点生成深度为1的左、右子节点： 切分点：xp=median(x(1))左子节点中的实例：x(1)&lt;xp右子节点中的实例：x(1)&gt;xp\\begin{aligned} &amp;切分点：x_p=\\text{median}(x^{(1)})\\\\ &amp;左子节点中的实例：x^{(1)}&lt;x_p\\\\ &amp;右子节点中的实例：x^{(1)}&gt;x_p \\end{aligned} ​切分点：xp​=median(x(1))左子节点中的实例：x(1)&lt;xp​右子节点中的实例：x(1)&gt;xp​​ 将落在切分超平面上的实例点保存为根节点。 （2）重复：对深度为 j ~j~ j 的结点，选择 x(l) ~x^{(l)}~ x(l) 为切分的坐标轴， l=(jmod k)+1 ~l=(j\\mod k)+1~ l=(jmodk)+1 ，以该节点区域中所有实例的 x(l) ~x^{(l)}~ x(l) 的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴 x(l) ~x^{(l)}~ x(l) 垂直的超平面实现。 由该节点生成深度为 j+1 ~j+1~ j+1 的左、右子节点： 切分点：xp=median(x(l))左子节点中的实例：x(l)&lt;xp右子节点中的实例：x(l)&gt;xp\\begin{aligned} &amp;切分点：x_p=\\text{median}(x^{(l)})\\\\ &amp;左子节点中的实例：x^{(l)}&lt;x_p\\\\ &amp;右子节点中的实例：x^{(l)}&gt;x_p \\end{aligned} ​切分点：xp​=median(x(l))左子节点中的实例：x(l)&lt;xp​右子节点中的实例：x(l)&gt;xp​​ 将落在切分超平面上的实例点保存在该节点。 对于维度的选择还有另一个方法，即选择方差最大的维度去进行划分，这样可能会划分得更好。 （3）直到两个子区域没有实例存在时停止，从而形成 KD ~KD~ KD 树的区域划分。 我们不妨在 2 ~2~ 2 维空间模拟一下 KD ~KD~ KD 树构造的过程： T={(2,3)T,(5,4)T,(9,6)T,(4,7)T,(8,1)T,(7,2)T}T=\\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\\} T={(2,3)T,(5,4)T,(9,6)T,(4,7)T,(8,1)T,(7,2)T} 其构造过程如下： ①选择根节点： x(1) ~x^{(1)}~ x(1) 所对应的切分点： (5,4) ~(5,4)~ (5,4) 或 (7,2) ~(7,2)~ (7,2) ，我们不妨选择 (7,2) ~(7,2)~ (7,2) 左子区域包含： (2,3),(4,7),(5,4) ~(2,3),(4,7),(5,4)~ (2,3),(4,7),(5,4) ，右子区域包含： (8,1),(9,6) ~(8,1),(9,6)~ (8,1),(9,6) ②对深度为 1 ~1~ 1 的结点继续划分： l=(1mod 2)+1=2 ~l=(1\\mod 2)+1=2~ l=(1mod2)+1=2 ，以 x(2) ~x^{(2)}~ x(2) 为基准进行划分 左子区域切分点： (5,4) ~(5,4)~ (5,4) ，右子区域切分点： (9,6) ~(9,6)~ (9,6) ③对深度为 2 ~2~ 2 的结点继续划分： l=(2mod 2)+1=1 ~l=(2\\mod 2)+1=1~ l=(2mod2)+1=1 ，以 x(1) ~x^{(1)}~ x(1) 为基准进行划分 由此得到三个新的切分点： (2,3),(4,7),(8,1) ~(2,3),(4,7),(8,1)~ (2,3),(4,7),(8,1) 由此得到的划分如下图所示： 得到的 KD ~KD~ KD 树如下： 注意：我们在实际的算法中往往并不会使用全部的实例点去构造 KD ~KD~ KD 树，因为这样的时间复杂度很高，往往选取部分点对区域进行划分 1.2.2 搜索KD树 我们构造KD树的目的还是用于进行分类，因此我们需要思考如何搜索KD树来进行分类，k-近邻的搜索方式如下： ①对于给定的测试点 xt ~x_t~ xt​ ，我们首先在 KD ~KD~ KD 树中找到包含该测试点的叶子结点 ②从该结点出发，依次退回到父节点，不断查找与目标点最邻近的结点 ③当确定不可能存在更近的结点时中止，这样搜索区域便被限制在空间的局部区域上了 为了更加直观得理解该算法，我们进行详细的分析：以 1−NN ~1-NN~ 1−NN 为例 输入：已构造的 KD ~KD~ KD 树，目标点 xt ~x_t~ xt​ 输出： xt ~x_t~ xt​ 的最近邻 （1）在 KD ~KD~ KD 树中找到包含目标点的叶子节点：寻找方法很容易，只需要从根节点开始递归得访问 KD ~KD~ KD 树，如果 xt(l)&lt;xp ~x_t^{(l)}&lt;x_p~ xt(l)​&lt;xp​ 则转到左子结点，反之则转到右子结点，直到子节点为叶子结点为止。 （2）此叶子结点为“当前最近结点”，递归得向上回退，对每个结点进行如下操作： ① 如果该结点保存的实例点比“当前最近结点”离目标点 xt ~x_t~ xt​ 距离更近，则以当前结点为“当前最近结点” ② 当前最近点一定存在于该结点的一个子结点对应的区域，检查该子结点对应的父结点的另一子结点对应的区域中是否存在更近的点。 具体地，检查另一子结点对应地区域是否与以目标点为球心、以目标点与当前最近结点的距离为半径的超球体相交，如果相交则可能存在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着递归得进行搜索。 如果不相交则向上回退。 （3）当回退到根节点时，搜索结束。最后的“当前最近结点”记为 xt ~x_t~ xt​ 的最近邻点。 如果实例点是随机分布的，则 KD ~KD~ KD 树搜索的平均计算时间复杂度为 O(log⁡n) ~O(\\log n)~ O(logn) ， KD ~KD~ KD 树更加适合训练实例数 n ~n~ n 远大于空间维数 k ~k~ k 时的 k ~k~ k 近邻搜索，当训练实例数接近特征空间维度数时它则接近于线性扫描。 我们以下图为例： A ~A~ A 为根节点，子结点为 B、C ~B、C~ B、C ，目标点为 S ~S~ S 得到的 KD ~KD~ KD 树如下： （1） 首先，我们找到了 S ~S~ S 位于区域②，因此得到“当前最近结点”为 D ~D~ D （2） 然后，检查叶子节点②的父节点 B ~B~ B 的另一子结点①，发现①没有与超球体相交，则不需要检查 （3） 继续，返回父节点 A ~A~ A ，发现结点 C ~C~ C 对应的区域中④与超球体相交，对④进行搜索找到了更近的点 E ~E~ E （4） 最终，我们得到了 S ~S~ S 的最近邻为 E ~E~ E 1.2.3 代码实现 手动实现的代码更加灵活，但鉴于笔者才疏学浅，我的选择往往是调库，手动实现是为了加强理解。 首先我们定义树节点类：代码实现参考了文章 k近邻算法之kd树优化 接着我们定义用于构造和搜索Kd树的类： 由此我们可以定义以 Kd ~Kd~ Kd 树为数据结构的优化后的 KNN ~KNN~ KNN 模型： 注意，我们上述实现的代码与我们的举例存在不同，一方面我们选择切分维度的方法是选择方差最大的那个维度，另外我们搜索 KD ~KD~ KD 树时不再是寻找最近邻，而是寻找 k ~k~ k 近邻，区别在于我们添加了一个数组存储当前寻找到的 k ~k~ k 个近邻，超球体半径是第 k ~k~ k 小的数据点与目标点之间的距离，这样实现的代码更具有普适性。 我们可以用鸢尾花数据集对上述模型进行验证： 可以发现正确率为0.967，模型效果良好，上述手动实现过程还是比较复杂的，在算法竞赛过程中为了提高编码效率，还是调库效率较高，不过如果涉及到算法的优化的话，面向手动实现的代码进行分析更有优势。 1.2.4 总结 ① KD树是一种二叉树，其中每个节点都是一个k维点。 ② 可以将每个非叶节点视为隐式生成一个拆分超平面，该超平面将空间拆分为两部分，称为半空间。 ③ 此超平面左侧的点由该节点的左子树表示，而超平面右侧的点由右子树表示。 ④ 超平面方向的选择方式如下：树中的每个节点都与k维度中的一个维度相关联，超平面垂直于该维度的轴。 1.3 球树 球树类似于KD树，但是不用超平面对特征空间进行分割，而是用超球面进行分割 ball结构允许我们沿着点所在的底层流形对数据进行分区，而不是重复剖析整个特征空间（如KD树） 1.3.1 伪码实现 球树的构造伪码如下图所示，因为其构造过程类似于 KD ~KD~ KD 树，所以在此不再手动实现（笔者此刻不愿意coding了 TAT） 1.3.2 球树应用 球树的作用与 KD ~KD~ KD 树相同，都是对 KNN ~KNN~ KNN 的数据结构进行优化， KD ~KD~ KD 树适用于低维空间，球树适用于高维空间。 ① KNN ~KNN~ KNN 在测试过程中很慢，因为它做了很多不必要的工作。 ② KD ~KD~ KD 树对特征空间进行分区，这样我们就可以排除距离最近的 k ~k~ k 个邻居更远的整个分区。 但是，拆分是轴对齐的，无法很好地延伸到更高的维度。 ③ 球树划分了点所在的流形，而不是整个空间。这使得它在更高的维度上表现得更好。 二、决策树 2.1 核心思想 假设我们进行一个二分类问题，如果我们知道一个测试点属于一个100万个点的集群，所有这些点的标签都为正值，那么在我们计算测试点到这100万个距离中的每一个点的距离之前，我们也会知道它的邻居将为正值，由此就有了决策树的思想。 决策树的构建过程，我们不存储训练数据，而是使用训练数据来构建一个树结构，该结构递归地将空间划分为具有类似标签的区域。 决策树特点： ① 首先，决策树的根节点对应整个训练集 ② 然后，通过一个简单的阈值 t ~t~ t ，将该集合沿一个维度 l ~l~ l 大致分成两半。 ③ x(l)≥t ~x^{(l)}≥ t~ x(l)≥t 的数据点落在右子节点中，其他所有节点落在左子节点中。 ④ 选择阈值 t ~t~ t 和维度 l ~l~ l ，以便生成的子节点在类成员方面更纯粹。 ⑤ 理想情况下，所有的正节点都属于一个子节点，所有的负节点都属于另一个子节点。 ⑥ 满足上述条件后，则完成决策的构建，否则要继续对叶子结点进行分割，直到所有叶子结点都属于一个类或不再可分 决策树在KNN之上的优点： ① 决策树构建之后我们便不再需要存储各个训练数据，只需要存储所有叶子结点的标签 ② 决策树在测试期间速度非常快，因为测试输入只需遍历树到一片叶子，预测是叶子的主要标签 ③ 决策树不需要度量，因为分割基于特征阈值而不是距离。 2.2 构造决策树 我们所要构建的决策树的目标是： ① 使得决策树最大紧凑化 ② 使得叶子结点都只包含一种标签的结点 要找到一棵最小化的树是一个NP完全问题，但是我们可以用贪婪策略非常有效地近似它。 我们不断拆分数据，以最小化杂质函数，该函数用于测量子对象中的标签纯度。 我们首先了解一下构造决策树用到的一些重要概念。 2.2.1 基尼系数 首先我们假设数据集 S ~S~ S 为： S={(x1,y1),(x2,y2),...,(xn,yn)}yi∈{1,2,...,c}S=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}\\\\ y_i\\in\\{1,2,...,c\\} S={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)}yi​∈{1,2,...,c} 接着定义数据集的子集 Sk ~S_k~ Sk​ ： Sk={(x,y)∣y=k}S=S1∪S2∪...∪ScS_k=\\{(x,y)|y=k\\}\\\\ S=S_1\\cup S_2\\cup...\\cup S_c Sk​={(x,y)∣y=k}S=S1​∪S2​∪...∪Sc​ 然后我们可以定义输入分数 pk ~p_k~ pk​ ： pk=∣Sk∣∣S∣p_k=\\frac{|S_k|}{|S|} pk​=∣S∣∣Sk​∣​ 基尼不纯度：表示在样本集合中一个随机选中的样本被分错的概率，则整个数据集的基尼不纯度为： Gini(S)=∑k=1cpk(1−pk)\\text{Gini}(S)=\\sum_{k=1}^cp_k(1-p_k) Gini(S)=k=1∑c​pk​(1−pk​) 显然当数据集 S ~S~ S 中的标签只有一个时， G(S)=0 ~G(S)=0~ G(S)=0 ，同时我们也可以定义决策树的基尼不纯度： GiniT(S)=∣SL∣∣S∣GiniT(SL)+∣SR∣∣S∣GiniT(SR)\\text{Gini}^T(S)=\\frac{|S_L|}{|S|}\\text{Gini}^T(S_L)+\\frac{|S_R|}{|S|}\\text{Gini}^T(S_R) GiniT(S)=∣S∣∣SL​∣​GiniT(SL​)+∣S∣∣SR​∣​GiniT(SR​) 2.2.2 信息熵 信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。直到1948年，香农提出了**“信息熵”**的概念，才解决了对信息的量化度量问题。信息熵这个词是香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。信源的不确定性越大，信息熵也越大。 从机器学习的角度来看，信息熵表示的是信息量的期望值，我们的假设与杂质函数的假设相同，则信息量的定义如下： Ik=log⁡pkI_k=\\log p_k Ik​=logpk​ 由于信息熵是信息量的期望值，所以信息熵 H(S) ~H(S)~ H(S) 的定义如下：信息熵反映的是不确定性 H(S)=−∑k=1cpklog⁡pkH(S)=-\\sum_{k=1}^cp_k\\log p_k H(S)=−k=1∑c​pk​logpk​ 同理我们可以定义决策树的熵： HT(S)=∣SL∣∣S∣HT(SL)+∣SR∣∣S∣HT(SR)H^T(S)=\\frac{|S_L|}{|S|}H^T(S_L)+\\frac{|S_R|}{|S|}H^T(S_R) HT(S)=∣S∣∣SL​∣​HT(SL​)+∣S∣∣SR​∣​HT(SR​) 在实际的场景中，我们可能需要研究数据集中某个特征等于某个值时的信息熵等于多少，这个时候就需要用到条件熵。条件熵 H(Y∣X) ~H(Y|X)~ H(Y∣X) 表示特征 X ~X~ X 为某个值的条件下，标签集为 Y ~Y~ Y 的熵。条件熵的计算公式如下： H(D,X)=H(Y∣X)=∑x∈Xp(x)H(Y∣X=x)=−∑x∈Xp(x)∑y∈Yp(y∣x)log⁡p(y∣x)\\begin{aligned} &amp;H(D,X)=H(Y|X)=\\sum_{x\\in X}p(x)H(Y|X=x)=-\\sum_{x\\in X}p(x)\\sum_{y\\in Y}p(y|x)\\log p(y|x) \\end{aligned} ​H(D,X)=H(Y∣X)=x∈X∑​p(x)H(Y∣X=x)=−x∈X∑​p(x)y∈Y∑​p(y∣x)logp(y∣x)​ 现在已经知道了什么是熵，什么是条件熵，接下来就可以看看什么是信息增益了。 所谓的信息增益就是表示我已知条件X后能得到信息Y的不确定性的减少程度。 就好比，我在玩读心术。你心里想一件东西，我来猜。我已开始什么都没问你，我要猜的话，肯定是瞎猜。这个时候我的熵就非常高。然后我接下来我会去试着问你是非题，当我问了是非题之后，我就能减小猜测你心中想到的东西的范围，这样其实就是减小了我的熵。那么我熵的减小程度就是我的信息增益。 所以信息增益如果套上机器学习的话就是，如果把特征A对训练集S的信息增益记为g(D, A)的话，那么g(D, A)的计算公式就是： g(D,A)=H(D)−H(D,A)g(D,A)=H(D)-H(D,A) g(D,A)=H(D)−H(D,A) 我们不妨通过一个例子理解上述与熵相关的概念：我们有如下&quot;客户流失数据集&quot;，0代表未流失，1代表流失 编号 性别 活跃度 是否流失 1 男 高 0 2 女 中 0 3 男 低 1 4 女 高 0 5 男 高 0 6 男 中 0 7 男 中 1 8 女 中 0 9 女 低 1 10 女 中 0 11 女 高 0 12 男 低 1 13 女 低 1 14 男 高 0 15 男 高 0 假如要算性别和活跃度这两个特征的信息增益的话，首先要先算总的信息熵和条件熵。 计算总的信息熵很简单：15条数据中标签为0的有10个，标签为1的有5个 yi∈{0,1}p0=∣S0∣∣S∣=1015=23p1=∣S1∣∣S∣=515=13y_i\\in\\{0,1\\}\\\\ p_0=\\frac{|S_0|}{|S|}=\\frac{10}{15}=\\frac23\\\\ p_1=\\frac{|S_1|}{|S|}=\\frac{5}{15}=\\frac13 yi​∈{0,1}p0​=∣S∣∣S0​∣​=1510​=32​p1​=∣S∣∣S1​∣​=155​=31​ 则可得总信息熵为： H(S)=−∑k=01pklog⁡pk=−23log⁡23−13log⁡13≈0.9182H(S)=-\\sum_{k=0}^1p_k\\log p_k=-\\frac23\\log\\frac23-\\frac13\\log\\frac13\\approx0.9182 H(S)=−k=0∑1​pk​logpk​=−32​log32​−31​log31​≈0.9182 接下来就是条件熵的计算，以性别为男的熵为例。表格中性别为男的数据有8条，这8条数据中有3条数据的标签为1，有5条数据的标签为0。所以根据条件熵的计算公式能够得出该条件熵为： H(Y∣gender=man)=−38log⁡38−58log⁡58≈0.9543H(Y|\\text{gender=man})=-\\frac38\\log\\frac38-\\frac58\\log\\frac58\\approx0.9543 H(Y∣gender=man)=−83​log83​−85​log85​≈0.9543 同理，我们也可以计算出性别为女时的条件熵： H(Y∣gender=woman)=−27log⁡27−57log⁡57≈0.8631H(Y|\\text{gender=woman})=-\\frac27\\log\\frac27-\\frac57\\log\\frac57\\approx0.8631 H(Y∣gender=woman)=−72​log72​−75​log75​≈0.8631 由此可得总的条件熵为： H(Y∣gender)=p(gender=max)H(Y∣gender=man)+p(gender=woman)H(Y∣gender=woman) =815×0.9543+715×0.8631≈0.9117\\begin{aligned} &amp;H(Y|\\text{gender})=p(\\text{gender=max})H(Y|\\text{gender=man})+p(\\text{gender=woman})H(Y|\\text{gender=woman})\\\\ &amp;~~~~~~~~~~~~~~~~~~~~~~~~=\\frac8{15}\\times0.9543+\\frac7{15}\\times0.8631\\approx0.9117 \\end{aligned} ​H(Y∣gender)=p(gender=max)H(Y∣gender=man)+p(gender=woman)H(Y∣gender=woman) =158​×0.9543+157​×0.8631≈0.9117​ 接着我们可以按照相同的方法计算活跃度的条件熵： H(Y∣activation=low)=−44log⁡44−0=0H(Y∣activation=mid)=−45log⁡45−15log⁡15≈0.7219H(Y∣activation=high)=−66log⁡66−0=0H(Y∣activation)=515H(Y∣activation=mid)≈0.2406\\begin{aligned} &amp;H(Y|\\text{activation=low})=-\\frac44\\log\\frac44-0=0\\\\ &amp;H(Y|\\text{activation=mid})=-\\frac45\\log\\frac45-\\frac15\\log\\frac15\\approx0.7219\\\\ &amp;H(Y|\\text{activation=high})=-\\frac66\\log\\frac66-0=0\\\\ &amp;H(Y|\\text{activation})=\\frac5{15} H(Y|\\text{activation=mid})\\approx0.2406 \\end{aligned} ​H(Y∣activation=low)=−44​log44​−0=0H(Y∣activation=mid)=−54​log54​−51​log51​≈0.7219H(Y∣activation=high)=−66​log66​−0=0H(Y∣activation)=155​H(Y∣activation=mid)≈0.2406​ 由此可得性别和活跃度两个特征的信息增益： g(S,gender)=H(S)−H(Y∣gender)=0.9182−0.9117=0.0065g(S,activation)=H(S)−H(Y∣activation)=0.9182−0.2406=0.6776\\begin{aligned} &amp;g(S,\\text{gender})=H(S)-H(Y|\\text{gender})=0.9182-0.9117=0.0065\\\\ &amp;g(S,\\text{activation})=H(S)-H(Y|\\text{activation})=0.9182-0.2406=0.6776 \\end{aligned} ​g(S,gender)=H(S)−H(Y∣gender)=0.9182−0.9117=0.0065g(S,activation)=H(S)−H(Y∣activation)=0.9182−0.2406=0.6776​ 那信息增益算出来之后有什么意义呢？回到读心术的问题，为了我能更加准确的猜出你心中所想，我肯定是问的问题越好就能猜得越准！换句话来说我肯定是要想出一个信息增益最大（减少不确定性程度最高）的问题来问你，显然上述两个特征中活跃度的信息增益最高，而这也是 ID3 ~ID3~ ID3 算法的基本思想。 同时支持 ID3 ~ID3~ ID3 算法的的一个定理为：信息增益一定非负，相关证明可以参考 如何证明信息增益一定大于0？ 2.2.3 ID3 ~ID3~ ID3 算法 （1）中止条件： ID3 ~ID3~ ID3 算法的终止条件为： ①子集中的所有数据点具有相同的标签 y ~y~ y ，停止拆分，并创建一个标签为 y ~y~ y 的叶子节点 ②没有更多的特征用于切分子集，比如两个数据点的特征向量相同但是标签不同，停止拆分，并创建一个标签为最常见标签的叶子节点 （2）算法过程： 比如我们辨别西瓜好坏的决策树如下： 很明显上述的 ID3 ~ID3~ ID3 算法是比较适合具有多项式特征的数据集的，而对于具有连续型的特征数据并不推荐该算法。 算法实现如下：首先我们用到的库为： 决策树模型为： 决策树分类器为： 验证模型时，我们用到的数据集为西瓜好坏数据集：watermelon20.xlsx 最终我们得到的决策树如下图所示： 算法一个明显的弊端是无法处理包含在训练集中未出现过的特征取值的测试点，这种情况经常出现在具有连续型特征的数据集上，这要求我们的训练集要足够大，保证对各种取值的覆盖，我们后面的 CART ~CART~ CART 算法会应对这个问题。 2.2.4 C4.5 ~C4.5~ C4.5 算法 C4.5 ~C4.5~ C4.5 算法是对 ID3 ~ID3~ ID3 算法的扩展，它们的区别在于 ID3 ~ID3~ ID3 每次选择信息增益最大的特征进行划分，而 C4.5 ~C4.5~ C4.5 每次选择信息增益率最大的特征进行划分，实现 C4.5 ~C4.5~ C4.5 算法只需要修改上述代码中的计算部分。 由于在使用信息增益这一指标进行划分时，更喜欢可取值数量较多的特征。为了减少这种偏好可能带来的不利影响，Ross Quinlan使用了信息增益率这一指标来选择最优划分属性，信息增益率的定义如下： 设数据集为 D ~D~ D ，某一特征为 A ~A~ A ， Gain(D,A) ~\\text{Gain}(D,A)~ Gain(D,A) 为信息增益， V ~V~ V 表示特征 A ~A~ A 取值的集合，则信息增益率定义如下： Gain ratio(D,A)=Gain(D,A)−∑v∈V∣Dv∣∣D∣log⁡∣Dv∣∣D∣\\text{Gain ratio}(D,A)=\\frac{\\text{Gain}(D,A)}{-\\sum_{v\\in V}\\frac{|D^v|}{|D|}\\log\\frac{|D^v|}{|D|}} Gain ratio(D,A)=−∑v∈V​∣D∣∣Dv∣​log∣D∣∣Dv∣​Gain(D,A)​ 还记得我们刚刚举的例子吗，我们回到客户流失数据集中，可以很容易得计算信息增益率： Gain(D,gender)=0.0065Gain(D,activation)=0.6776\\begin{aligned} &amp;\\text{Gain}(D,\\text{gender})=0.0065\\\\ &amp;\\text{Gain}(D,\\text{activation})=0.6776\\\\ \\end{aligned} ​Gain(D,gender)=0.0065Gain(D,activation)=0.6776​ 15条数据中8条是男性，7条是女性；4条低活跃度，5条中活跃度，6条高活跃度： Gain ratio(D,gender)=Gain(D,gender)−815log⁡815−715log⁡715≈0.0065Gain ratio(D,activation)=Gain(D,activation)−415log⁡415−515log⁡515−615log⁡615≈0.4238\\begin{aligned} &amp;\\text{Gain ratio}(D,\\text{gender})=\\frac{\\text{Gain}(D,\\text{gender})}{-\\frac8{15}\\log\\frac8{15}-\\frac7{15}\\log\\frac7{15}}\\approx0.0065\\\\ &amp;\\text{Gain ratio}(D,\\text{activation})=\\frac{\\text{Gain}(D,\\text{activation})}{-\\frac4{15}\\log\\frac4{15}-\\frac5{15}\\log\\frac5{15}-\\frac6{15}\\log\\frac6{15}}\\approx 0.4238 \\end{aligned} ​Gain ratio(D,gender)=−158​log158​−157​log157​Gain(D,gender)​≈0.0065Gain ratio(D,activation)=−154​log154​−155​log155​−156​log156​Gain(D,activation)​≈0.4238​ 我们可以发现活跃度的信息增益率要比信息增益小很多，这就是 C4.5 ~C4.5~ C4.5 算法的特点。 实现 C4.5 ~C4.5~ C4.5 算法仅需要修改 ID3 ~ID3~ ID3 算法的 calcInfoGain 函数： 值得一提的是：当信息增益为0时，对应的信息增益率的底数也为0，在编写函数时需要注意避免分母为0的情况，同时上述两个算法都可以优化成可以处理具有连续型特征的数据集的算法，只需要将划分不同取值分支的过程改为选择阈值的过程，这也是 CART ~CART~ CART 算法的思想，所以我们不再过多赘述，该思想将会在 CART ~CART~ CART 算法中实现。 最终我们得到的决策树如下字典所示：可以发现对于我们的西瓜数据集来说两个算法得到的决策树相同 2.2.5 CART ~CART~ CART 算法 CART ~CART~ CART 即 Classification and Regression Trees，它既可以作为分类树也可以作为回归树，并且它只能是二叉树。 在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益率来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？当然有！那就是基尼系数！ CART算法使用基尼系数来代替信息增益率，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益与信息增益率是相反的(它们都是越大越好)。 基尼系数的计算方式如下：数据集为 D ~D~ D ， pk ~p_k~ pk​ 表示第 k ~k~ k 个类别在数据集中所占的比例 Gini(D)=∑k=1cpk(1−pk)=1−∑k=1cpk2\\text{Gini}(D)=\\sum_{k=1}^cp_k(1-p_k)=1-\\sum_{k=1}^cp_k^2 Gini(D)=k=1∑c​pk​(1−pk​)=1−k=1∑c​pk2​ 我们还是以客户流失数据集为例：15条数据中，10条标签为0，5条标签为1，则有： Gini(D)=1−(p02+p12)=1−[(23)2+(13)2]≈0.4444\\text{Gini}(D)=1-(p_0^2+p_1^2)=1-\\big[(\\frac23)^2+(\\frac13)^2\\big]\\approx0.4444 Gini(D)=1−(p02​+p12​)=1−[(32​)2+(31​)2]≈0.4444 同时还有基于数据集 D ~D~ D 和特征 A ~A~ A 的 Gini ~\\text{Gini}~ Gini 系数， V ~V~ V 表示特征 A ~A~ A 取值的集合，则定义如下：计算过程类似于条件熵的计算 Gini(D,A)=∑v∈V∣Dv∣∣D∣Gini(Dv)\\text{Gini}(D,A)=\\sum_{v\\in V}\\frac{|D^v|}{|D|}\\text{Gini}(D^v) Gini(D,A)=v∈V∑​∣D∣∣Dv∣​Gini(Dv) 我们以客户流失数据集为例： ① 性别特征：15条数据中，8条男性，7条女性；男性数据中，5条标签0，3条标签1；女性数据中，5条标签0，2条标签1 ∣D∣=15,∣Dman∣=8,∣Dwoman∣=7Gini(Dman)=1−[(58)2+(38)2]≈0.46875Gini(Dwoman)=1−[(57)2+(27)2]≈0.40816Gini(D,gender)=815×0.46875+715×0.40816≈0.44048\\begin{aligned} &amp;|D|=15,|D^{\\text{man}}|=8,|D^{\\text{woman}}|=7\\\\ &amp;\\text{Gini}(D^{\\text{man}})=1-\\big[(\\frac58)^2+(\\frac38)^2\\big]\\approx0.46875\\\\ &amp;\\text{Gini}(D^{\\text{woman}})=1-\\big[(\\frac57)^2+(\\frac27)^2\\big]\\approx0.40816\\\\ &amp;\\text{Gini}(D,\\text{gender})=\\frac8{15}\\times 0.46875+\\frac7{15}\\times 0.40816\\approx0.44048 \\end{aligned} ​∣D∣=15,∣Dman∣=8,∣Dwoman∣=7Gini(Dman)=1−[(85​)2+(83​)2]≈0.46875Gini(Dwoman)=1−[(75​)2+(72​)2]≈0.40816Gini(D,gender)=158​×0.46875+157​×0.40816≈0.44048​ ② 活跃度特征计算同理： ∣D∣=15,∣Dlow∣=4,∣Dmid∣=5,∣Dhigh∣=6Gini(Dlow)=1−[(44)2+(04)2]=0Gini(Dmid)=1−[(45)2+(15)2]=0.32Gini(Dhigh)=1−[(66)2+(06)2]=0Gini(D,activation)=515×0.32=0.10667\\begin{aligned} &amp;|D|=15,|D^{\\text{low}}|=4,|D^{\\text{mid}}|=5,|D^{\\text{high}}|=6\\\\ &amp;\\text{Gini}(D^{\\text{low}})=1-\\big[(\\frac44)^2+(\\frac04)^2\\big]=0\\\\ &amp;\\text{Gini}(D^{\\text{mid}})=1-\\big[(\\frac45)^2+(\\frac15)^2\\big]=0.32\\\\ &amp;\\text{Gini}(D^{\\text{high}})=1-\\big[(\\frac66)^2+(\\frac06)^2\\big]=0\\\\ &amp;\\text{Gini}(D,\\text{activation})=\\frac5{15}\\times 0.32=0.10667 \\end{aligned} ​∣D∣=15,∣Dlow∣=4,∣Dmid∣=5,∣Dhigh∣=6Gini(Dlow)=1−[(44​)2+(40​)2]=0Gini(Dmid)=1−[(54​)2+(51​)2]=0.32Gini(Dhigh)=1−[(66​)2+(60​)2]=0Gini(D,activation)=155​×0.32=0.10667​ 显然我们要选择活跃度特征，因为它的基尼系数小，不纯度更低。 当我们知道如何选择用于切分的特征后，应该思考如何在该特征上选择一个切分点，即如何寻找一个合适的阈值，在此我们面向连续型特征进行分析，而对于离散型的数据类比即可。 ① 首先，将数据集按照最优特征从大到小排列 ② 对于大小为 n ~n~ n 的样本，共有 n−1 ~n-1~ n−1 种切分方式，即有 n−1 ~n-1~ n−1 个切分点，但是这样切分计算量是很大的并且决策树不佳，我们将注意力放在切分的特征 A ~A~ A 上，设 A ~A~ A 有 m ~m~ m 种不同的取值，则我们只需要关注这 m−1 ~m-1~ m−1 个不同取值分界点处的切分即可，同时这样可以规避掉一个问题，即切分的阈值一定不属于所提供的数据集中的一个取值，不需要再考虑特征的取值等于阈值时，数据点划分到左子集还是右子集的问题，而在预测的时候如果出现特征取值等于阈值的情况可以考虑固定好搜索走向或者随机走到左右子树。 ③ 每个切分点将数据集划分为左右两部分： DL,DR ~D_L,D_R~ DL​,DR​ ，则该切分对应着一个基尼系数： Gini(DL,DR)=∣DL∣∣D∣Gini(DL)+∣DR∣∣D∣Gini(DR)\\begin{aligned} &amp;\\text{Gini}(D_L,D_R)=\\frac{|D_L|}{|D|}\\text{Gini}(D_L)+\\frac{|D_R|}{|D|}\\text{Gini}(D_R) \\end{aligned} ​Gini(DL​,DR​)=∣D∣∣DL​∣​Gini(DL​)+∣D∣∣DR​∣​Gini(DR​)​ 找到基尼系数最小的切分方式，选择切分左右两个数据点特征的均值作为阈值。 我们通过代码实现基于 CART ~CART~ CART 算法的决策树模型：首先我们用到的库有： CART ~CART~ CART 树模型定义如下： 利用该分类树的分类器为： 我们可以利用鸢尾花数据集进行模型效果的验证：可以发现准确率很高，达到了0.96 可以发现 CART ~CART~ CART 算法的大体思想与 ID3 ~ID3~ ID3 和 C4.5 ~C4.5~ C4.5 算法相同，模型的实现也比较类似。 CART ~CART~ CART 是一个构造简单并且测试速度很快的树，但是它本身在准确性上并没竞争力，一些诸如 LightGBM 和 XGBoost 等高性能的第三方库提供的树模型具有更强大的性能，适用于机器学习竞赛中。 ","link":"https://2006wzt.github.io/post/机器学习实战（十四）：树模型/"},{"title":"机器学习实战（十三）：核函数","content":"核函数 一、核心思想 在前面我们所讨论的分类器中，基本都是线性分类器，但是当数据集不存在一个线性的决策边界时，线性分类器便无法很好得进行分类。 事实证明，有一种优雅的方法可以将非线性问题合并到大多数的线性分类器可解决的问题中，即将线性分类器非线性化，这便是核函数。 我们可以看一个经典的例子：如下图所示的数据集，显然它是不存在线性的决策边界的，但是我们可以通过函数 ϕ ~\\phi~ ϕ 对特征向量进行特征变换使得数据线性可分。 我们不妨将特征变换函数定义为： ϕ(x)=ϕ([x1,x2]T)=[x1,x2,∣x1⋅x2∣]T\\phi(x)=\\phi([x_1,x_2]^T)=[x_1,x_2,|x_1\\cdot x_2|]^T ϕ(x)=ϕ([x1​,x2​]T)=[x1​,x2​,∣x1​⋅x2​∣]T 我们所添加的维度捕捉了原始特征之间的非线性交互，使得数据变为了线性可分，升维的方法较为简便，并且使得问题保持凸且表现良好，但是缺点是升维可能会导致维度过高，使得模型变得复杂，比如下面这个例子所示： 通过特征变换，维度从 d ~d~ d 维变为了 2d ~2^d~ 2d 维，这种新的表示法 ϕ(x) ~\\phi(x)~ ϕ(x) 非常有表现力，允许复杂的非线性决策边界，但维数非常高。这使得我们的算法速度慢得令人无法忍受。 二、核技巧 核技巧是一种通过在更高维空间中学习函数来绕过这一困境的方法，而无需计算单个向量 ϕ(x) ~\\phi(x)~ ϕ(x) 或完整向量 w ~w~ w 。 2.1 梯度下降 我们考虑平方损失函数的梯度下降过程： l(w)=∑i=1n(wTxi−yi)2l(w)=\\sum_{i=1}^n(w^Tx_i-y_i)^2 l(w)=i=1∑n​(wTxi​−yi​)2 在梯度下降过程中，我们每次需要选择一个步长进行更新： wt+1=wt−s⋅(∂l(w)∂w)∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xiw_{t+1}=w_{t}-s\\cdot(\\frac{\\partial l(w)}{\\partial w})\\\\ \\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i wt+1​=wt​−s⋅(∂w∂l(w)​)∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​ 此处我们要引入一个重要的假设：我们可以将参数 w ~w~ w 表示为特征向量 xi ~x_i~ xi​ 的线性组合： w=∑i=1nαixiw=\\sum_{i=1}^n\\alpha_ix_i w=i=1∑n​αi​xi​ 根据该假设我们可以得到： wt+1 ~w_{t+1}~ wt+1​ 与 xi ~x_i~ xi​ 线性相关， wt ~w_t~ wt​ 与 xi ~x_i~ xi​ 线性相关，由此：梯度 ∂l(w)∂w ~\\frac{\\partial l(w)}{\\partial w}~ ∂w∂l(w)​ 与 xi ~x_i~ xi​ 线性相关： ∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xi=∑i=1nγixi\\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i=\\sum_{i=1}^n\\gamma_ix_i ∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​=i=1∑n​γi​xi​ 由于损失函数为凸函数，最终解与初始化无关我们可以将 w0 ~w_0~ w0​ 初始化为我们想要的任何值，我们不妨令： w0=[0,0,...,0]T,α0=[0,0,...,0]Tw_0=[0,0,...,0]^T,\\alpha^0=[0,0,...,0]^T w0​=[0,0,...,0]T,α0=[0,0,...,0]T 根据上述分析我们可以得到梯度下降的过程为： w1=w0−s⋅2∑i=1n(w0Txi−yi)xi=∑i=1nαi0xi−s∑i=1nγi0xi=∑i=1nαi1xi α1=α0−sγ0w2=w1−s⋅2∑i=1n(w1Txi−yi)xi=∑i=1nαi1xi−s∑i=1nγi1xi=∑i=1nαi2xi α2=α1−sγ1w3=w2−s⋅2∑i=1n(w2Txi−yi)xi=∑i=1nαi2xi−s∑i=1nγi2xi=∑i=1nαi2xi α3=α2−sγ2...wt=wt−1−s⋅2∑i=1n(wt−1Txi−yi)xi=∑i=1nαit−1xi−s∑i=1nγit−1xi=∑i=1nαitxi αt=αt−1−sγt−1\\begin{aligned} &amp;w_1=w_0-s\\cdot2\\sum_{i=1}^n(w_0^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^0x_i-s\\sum_{i=1}^n\\gamma_i^0x_i=\\sum_{i=1}^n\\alpha_i^1x_i~~~~\\alpha^1=\\alpha^0-s\\gamma^0\\\\ &amp;w_2=w_1-s\\cdot2\\sum_{i=1}^n(w_1^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^1x_i-s\\sum_{i=1}^n\\gamma_i^1x_i=\\sum_{i=1}^n\\alpha_i^2x_i~~~~\\alpha^2=\\alpha^1-s\\gamma^1\\\\ &amp;w_3=w_2-s\\cdot2\\sum_{i=1}^n(w_2^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^2x_i-s\\sum_{i=1}^n\\gamma_i^2x_i=\\sum_{i=1}^n\\alpha_i^2x_i~~~~\\alpha^3=\\alpha^2-s\\gamma^2\\\\ &amp;...\\\\ &amp;w_t=w_{t-1}-s\\cdot2\\sum_{i=1}^n(w_{t-1}^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^{t-1}x_i-s\\sum_{i=1}^n\\gamma_i^{t-1}x_i=\\sum_{i=1}^n\\alpha_i^tx_i~~~~\\alpha^t=\\alpha^{t-1}-s\\gamma^{t-1}\\\\ \\end{aligned} ​w1​=w0​−s⋅2i=1∑n​(w0T​xi​−yi​)xi​=i=1∑n​αi0​xi​−si=1∑n​γi0​xi​=i=1∑n​αi1​xi​ α1=α0−sγ0w2​=w1​−s⋅2i=1∑n​(w1T​xi​−yi​)xi​=i=1∑n​αi1​xi​−si=1∑n​γi1​xi​=i=1∑n​αi2​xi​ α2=α1−sγ1w3​=w2​−s⋅2i=1∑n​(w2T​xi​−yi​)xi​=i=1∑n​αi2​xi​−si=1∑n​γi2​xi​=i=1∑n​αi2​xi​ α3=α2−sγ2...wt​=wt−1​−s⋅2i=1∑n​(wt−1T​xi​−yi​)xi​=i=1∑n​αit−1​xi​−si=1∑n​γit−1​xi​=i=1∑n​αit​xi​ αt=αt−1−sγt−1​ 因为 αi0=0 ~\\alpha^0_i=0~ αi0​=0 ，则有： αi1= 0 −sγi0=−sγi0αi2=αi1−sγ01=−sγi0−sγi1αi3=αi2−sγi2=−sγi0−sγi1−sγi2...αit=αit−1−sγit−1=−s∑r=1t−1γir\\begin{aligned} &amp;\\alpha^1_i=~0~-s\\gamma^0_i=-s\\gamma^0_i\\\\ &amp;\\alpha^2_i=\\alpha^1_i-s\\gamma^1_0=-s\\gamma^0_i-s\\gamma^1_i\\\\ &amp;\\alpha^3_i=\\alpha_i^2-s\\gamma^2_i=-s\\gamma^0_i-s\\gamma^1_i-s\\gamma^2_i\\\\ &amp;...\\\\ &amp;\\alpha^t_i=\\alpha_i^{t-1}-s\\gamma^{t-1}_i=-s\\sum_{r=1}^{t-1}\\gamma_i^r \\end{aligned} ​αi1​= 0 −sγi0​=−sγi0​αi2​=αi1​−sγ01​=−sγi0​−sγi1​αi3​=αi2​−sγi2​=−sγi0​−sγi1​−sγi2​...αit​=αit−1​−sγit−1​=−sr=1∑t−1​γir​​ 我们用 xi ~x_i~ xi​ 的线性组合代替 w ~w~ w ，得到新的模型和损失函数： h(xi)=wtTxi=∑j=1nαjtxjTxil(w)=∑i=1n(wtTxi−yi)2=∑i=1n(∑j=1nαjtxjTxi−yi)2h(x_i)=w_t^Tx_i=\\sum_{j=1}^n\\alpha_j^tx_j^Tx_i\\\\ l(w)=\\sum_{i=1}^n(w^T_tx_i-y_i)^2=\\sum_{i=1}^n(\\sum_{j=1}^n\\alpha_j^tx_j^Tx_i-y_i)^2 h(xi​)=wtT​xi​=j=1∑n​αjt​xjT​xi​l(w)=i=1∑n​(wtT​xi​−yi​)2=i=1∑n​(j=1∑n​αjt​xjT​xi​−yi​)2 由此我们可以发现：为了学习具有平方损失的超平面分类器，我们需要的唯一信息是所有数据的特征向量对之间的内积。 2.2 计算内积 有了上述推导，我们将模型简化为只需要求解向量对之间的内积，我们回归到上述的升维操作中去： 在升维之后，内积的计算公式为： ϕ(x)Tϕ(z)=1+x1z1+x2z2+...x1x2...xdz1z2...zd=∏k=1d(1+xkzk)\\phi(x)^T\\phi(z)=1+x_1z_1+x_2z_2+...x_1x_2...x_dz_1z_2...z_d=\\prod_{k=1}^d(1+x_kz_k) ϕ(x)Tϕ(z)=1+x1​z1​+x2​z2​+...x1​x2​...xd​z1​z2​...zd​=k=1∏d​(1+xk​zk​) 我们可以发现，尽管特征向量是 2d ~2^d~ 2d 维的，但是计算其内积仅需要 d ~d~ d 次乘法运算，这极大提高了算法的速度。 我们由此即可定义核函数： k(xi,xj)=ϕ(xi)Tϕ(xj)k(x_i,x_j)=\\phi(x_i)^T\\phi(x_j) k(xi​,xj​)=ϕ(xi​)Tϕ(xj​) 核函数计算出的结果存储在核矩阵中： Kij=ϕ(xi)Tϕ(xj)K_{ij}=\\phi(x_i)^T\\phi(x_j) Kij​=ϕ(xi​)Tϕ(xj​) 诸如 ϕ ~\\phi~ ϕ 之类的用于升维的映射并不好找，因此我们用核函数 k(xi,xj) ~k(x_i,x_j)~ k(xi​,xj​) 去代替这样的映射，处理线性不可分问题。 则上述模型可以表示为：可以发现模型中唯一的未知参数即为 α ~\\alpha~ α ，我们需要对它进行求解 h(xi)=wTxi=∑j=1nαjxjTxi=∑j=1nαjk(xj,xi)h(x_i)=w^Tx_i=\\sum_{j=1}^n\\alpha_jx_j^Tx_i=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) h(xi​)=wTxi​=j=1∑n​αj​xjT​xi​=j=1∑n​αj​k(xj​,xi​) 同时我们也已经得到了： αit=−s∑r=1t−1γir\\alpha^t_i=-s\\sum_{r=1}^{t-1}\\gamma_i^r αit​=−sr=1∑t−1​γir​ 所以当下我们的求解目标变为了 γ ~\\gamma~ γ ： ∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xi=∑i=1nγixiγi=2(wTxi−yi)\\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i=\\sum_{i=1}^n\\gamma_ix_i\\\\ \\gamma_i=2(w^Tx_i-y_i) ∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​=i=1∑n​γi​xi​γi​=2(wTxi​−yi​) 在经过 ϕ ~\\phi~ ϕ 特征变换的新的高维空间中有： γi=2(wTϕ(xi)−yi)=2(∑j=1nαjk(xj,xi)−yi)\\gamma_i=2(w^T\\phi(x_i)-y_i)=2(\\sum_{j=1}^n\\alpha_jk(x_j,x_i)-y_i) γi​=2(wTϕ(xi​)−yi​)=2(j=1∑n​αj​k(xj​,xi​)−yi​) 则梯度下降的过程为： αit+1=αit−sγit=αit−2s(∑j=1nαjtk(xj,xi)−yi)\\alpha_i^{t+1}=\\alpha_i^t-s\\gamma_i^t=\\alpha_i^t-2s(\\sum_{j=1}^n\\alpha_j^tk(x_j,x_i)-y_i) αit+1​=αit​−sγit​=αit​−2s(j=1∑n​αjt​k(xj​,xi​)−yi​) 梯度下降过程中，每次更新 α ~\\alpha~ α 的计算量为 O(n2) ~O(n^2)~ O(n2) ，远好于 O(2d) ~O(2^d)~ O(2d) 三、一般核函数 3.1 常用核函数 （1）线性核函数： K(x,z)=xTzK(x,z)=x^Tz K(x,z)=xTz （2）多项式核函数： K(x,z)=(1+xTz)dK(x,z)=(1+x^Tz)^d K(x,z)=(1+xTz)d （3）高斯核函数（RBF\\text{RBF}RBF）： K(x,z)=e−∣∣x−z∣∣22σ2K(x,z)=e^{-\\frac{||x-z||_2^2}{\\sigma^2}} K(x,z)=e−σ2∣∣x−z∣∣22​​ （4）指数核函数： K(x,z)=e−∣∣x−z∣∣2σ2K(x,z)=e^{-\\frac{||x-z||}{2\\sigma^2}} K(x,z)=e−2σ2∣∣x−z∣∣​ （5）拉普拉斯核函数： K(x,z)=e−∣x−z∣σK(x,z)=e^{-\\frac{|x-z|}{\\sigma}} K(x,z)=e−σ∣x−z∣​ （6）Sigmoid\\text{Sigmoid}Sigmoid核函数： tanh⁡(x)=ex−e−xex+e−x ~\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}~ tanh(x)=ex+e−xex−e−x​ K(x,z)=tanh⁡(γxTz+r)K(x,z)=\\tanh(\\gamma x^Tz+r) K(x,z)=tanh(γxTz+r) 3.2 良定义核函数 良定义的核函数定义如下：通过递归组合以下一个或多个规则构建的核称为定义良好的核： ① k(x,z)=xTz② k(x,z)=ck1(x,z)③ k(x,z)=k1(x,z)+k2(x,z)④ k(x,z)=g(k(x,z))⑤ k(x,z)=k1(x,z)⋅k2(x,z)⑥ k(x,z)=f(x)k1(x,z)f(z)⑦ k(x,z)=ek1(x,z)⑧ k(x,z)=xTAz\\begin{aligned} &amp;①~k(x,z)=x^Tz\\\\ &amp;②~k(x,z)=ck_1(x,z)\\\\ &amp;③~k(x,z)=k_1(x,z)+k_2(x,z)\\\\ &amp;④~k(x,z)=g\\big(k(x,z)\\big)\\\\ &amp;⑤~k(x,z)=k_1(x,z)\\cdot k_2(x,z)\\\\ &amp;⑥~k(x,z)=f(x)k_1(x,z)f(z)\\\\ &amp;⑦~k(x,z)=e^{k_1(x,z)}\\\\ &amp;⑧~k(x,z)=x^TAz \\end{aligned} ​① k(x,z)=xTz② k(x,z)=ck1​(x,z)③ k(x,z)=k1​(x,z)+k2​(x,z)④ k(x,z)=g(k(x,z))⑤ k(x,z)=k1​(x,z)⋅k2​(x,z)⑥ k(x,z)=f(x)k1​(x,z)f(z)⑦ k(x,z)=ek1​(x,z)⑧ k(x,z)=xTAz​ 上述规则中 k1(x,z) ~k_1(x,z)~ k1​(x,z) 和 k2(x,z) ~k_2(x,z)~ k2​(x,z) 都是良定义的核函数， c≥0 ~c\\ge0~ c≥0 ， g ~g~ g 是一个正系数多项式函数， f ~f~ f 是任何函数， A ~A~ A 是半正定的 某个核函数是良定义的等价为： ①核矩阵 K ~K~ K 的特征值都是非负的 ②存在实矩阵 P ~P~ P 使得： K=PTP ~K=P^TP~ K=PTP ③核矩阵 K ~K~ K 是半正定的，即对于任何向量 x ~x~ x ，都有： xTKx≥0 ~x^TKx\\ge0~ xTKx≥0 定理 3-1 RBF核函数：k(x,z)=e−(x−z)2σ2是良定义的\\text{RBF}核函数：k(x,z)=e^{-\\frac{(x-z)^2}{\\sigma^2}}是良定义的 RBF核函数：k(x,z)=e−σ2(x−z)2​是良定义的 证明如下： k(x,z)=e−(x−z)2σ2=e−1σ2(xTx−2xTz+zTz)=e−xTxσ2⋅e2xTzσ2⋅e−xTxσ2\\begin{aligned} &amp;k(x,z)=e^{-\\frac{(x-z)^2}{\\sigma^2}}=e^{-\\frac1{\\sigma^2}(x^Tx-2x^Tz+z^Tz)}=e^{-\\frac{x^Tx}{\\sigma^2}}\\cdot e^{\\frac{2x^Tz}{\\sigma^2}}\\cdot e^{-\\frac{x^Tx}{\\sigma^2}} \\end{aligned} ​k(x,z)=e−σ2(x−z)2​=e−σ21​(xTx−2xTz+zTz)=e−σ2xTx​⋅eσ22xTz​⋅e−σ2xTx​​ 根据规则⑥：f(x)=e−xTxσ2,k1(x,z)=e2xTzσ2→k(x,z)=f(x)⋅k1(x,z)⋅f(z)根据规则⑦：k2(x,z)=2xTzσ2→k1(x,z)=ek2(x,z)根据规则②：k3(x,z)=xTz,c=2xTzσ2→k2(x,z)=c⋅k3(x,z)根据规则①：k3(x,z) is well defined→k2(x,z) is well defined→k1(x,z) is well defined\\begin{aligned} &amp;根据规则⑥：f(x)=e^{-\\frac{x^Tx}{\\sigma^2}},k_1(x,z)=e^{\\frac{2x^Tz}{\\sigma^2}}\\rightarrow k(x,z)=f(x)\\cdot k_1(x,z)\\cdot f(z)\\\\ &amp;根据规则⑦：k_2(x,z)=\\frac{2x^Tz}{\\sigma^2}\\rightarrow k_1(x,z)=e^{k_2(x,z)}\\\\ &amp;根据规则②：k_3(x,z)=x^Tz,c=\\frac{2x^Tz}{\\sigma^2}\\rightarrow k_2(x,z)=c\\cdot k_3(x,z)\\\\ &amp;根据规则①：k_3(x,z)\\text{ is well defined}\\rightarrow k_2(x,z)\\text{ is well defined}\\rightarrow k_1(x,z)\\text{ is well defined}\\\\ \\end{aligned} ​根据规则⑥：f(x)=e−σ2xTx​,k1​(x,z)=eσ22xTz​→k(x,z)=f(x)⋅k1​(x,z)⋅f(z)根据规则⑦：k2​(x,z)=σ22xTz​→k1​(x,z)=ek2​(x,z)根据规则②：k3​(x,z)=xTz,c=σ22xTz​→k2​(x,z)=c⋅k3​(x,z)根据规则①：k3​(x,z) is well defined→k2​(x,z) is well defined→k1​(x,z) is well defined​ 综上推导： k(x,z) is well definedk(x,z)\\text{ is well defined} k(x,z) is well defined 定理 3-2 S1,S2∈Ω,k(S1,S2)=e∣S1∩S2∣是良定义的S_1,S_2\\in \\Omega,k(S_1,S_2)=e^{|S_1\\cap S_2|}是良定义的 S1​,S2​∈Ω,k(S1​,S2​)=e∣S1​∩S2​∣是良定义的 证明如下： 将 Ω ~\\Omega~ Ω 中所有可能的元素排列成一个列表，S1,S2S_1,S_2S1​,S2​分别用一个大小为 ∣Ω∣ ~|\\Omega|~ ∣Ω∣ 的向量 xS1,xS2 ~x^{S_1},x^{S_2}~ xS1​,xS2​ 表示，如果 Ω ~\\Omega~ Ω 中的第 i ~i~ i 个元素属于 S ~S~ S ，则 xiS=1 ~x^{S}_i=1~ xiS​=1 ，反之则 xiS=0 ~x^{S}_i=0~ xiS​=0 ，则上述核函数可以表示为： k(S1,S2)=exS1TxS2\\begin{aligned} &amp;k(S_1,S_2)=e^{x_{S_1}^Tx_{S_2}} \\end{aligned} ​k(S1​,S2​)=exS1​T​xS2​​​ 根据规则⑦和规则①，我们可以得到： k(S1,S2) ~k(S_1,S_2)~ k(S1​,S2​) 为良定义核函数。 四、模型核化 一个算法可以通过三步实现核化： ①证明解决方案位于训练点的范围内，即对于某些 αi ~\\alpha_i~ αi​ ： w=∑i=1nαixiw=\\sum_{i=1}^n\\alpha_ix_i w=i=1∑n​αi​xi​ ②重构算法与分类器，使得输入的特征向量仅用于内积的计算 ③将内积替换为核函数： xiTxj→ϕ(xi)Tϕ(xj)x_i^Tx_j\\rightarrow\\phi(x_i)^T\\phi(x_j) xiT​xj​→ϕ(xi​)Tϕ(xj​) 4.1 线性回归核化 我们回顾一下普通的最小二乘回归 OLS ~OLS~ OLS ： l(w)=∑i=1n(xiTw−yi)w^MLE=argminw ∑i=1n(xiTw−yi)h(x)=wTxl(w)=\\sum_{i=1}^n(x_i^Tw-y_i)\\\\ \\hat{w}_{MLE}=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)\\\\ h(x)=w^Tx l(w)=i=1∑n​(xiT​w−yi​)w^MLE​=wargmin​ i=1∑n​(xiT​w−yi​)h(x)=wTx 输入的训练集为 X ~X~ X 和 Y ~Y~ Y ，则其闭合形式为： w=(XTX)−1XTYw=(X^TX)^{-1}X^TY w=(XTX)−1XTY 我们对该模型进行核化： X ~X~ X 为 n×d ~n\\times d~ n×d 维矩阵， Y ~Y~ Y 为 n×1 ~n\\times1~ n×1 维矩阵， w,xi ~w,x_i~ w,xi​ 为 d×1 ~d\\times1~ d×1 维矩阵， α ~\\alpha~ α 为 n×1 ~n\\times1~ n×1 维矩阵 ①将 w ~w~ w 表示为 xi ~x_i~ xi​ 的线性组合： α=[ α1,α2,...,αn ]T ~\\alpha=[~\\alpha_1,\\alpha_2,...,\\alpha_n~]^T~ α=[ α1​,α2​,...,αn​ ]T w=∑i=1nαixi=XTα\\begin{aligned} &amp;w=\\sum_{i=1}^n\\alpha_ix_i=X^T\\alpha \\end{aligned} ​w=i=1∑n​αi​xi​=XTα​ ②将模型修正为输入特征向量的内积： h(xi)=∑j=1nαjxjTxi=wTxi=αTXxih(x_i)=\\sum_{j=1}^n\\alpha_jx_j^Tx_i=w^Tx_i=\\alpha^TXx_i h(xi​)=j=1∑n​αj​xjT​xi​=wTxi​=αTXxi​ ③用核函数代替内积： h(xi)=∑j=1nαjk(xj,xi)h(x_i)=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) h(xi​)=j=1∑n​αj​k(xj​,xi​) 求解 α ~\\alpha~ α 的闭合形式： w=XTα=(XTX)−1XTY→XTXXTα=XTY→XXTα=YKij=k(xi,xj)→K=XXT→Kα=Yα=K−1Y\\begin{aligned} &amp;w=X^T\\alpha=(X^TX)^{-1}X^TY\\rightarrow X^TXX^T\\alpha=X^TY\\rightarrow XX^T\\alpha=Y\\\\ &amp;K_{ij}=k(x_i,x_j)\\rightarrow K=XX^T\\rightarrow K\\alpha=Y\\\\ &amp;\\alpha=K^{-1}Y \\end{aligned} ​w=XTα=(XTX)−1XTY→XTXXTα=XTY→XXTα=YKij​=k(xi​,xj​)→K=XXT→Kα=Yα=K−1Y​ 岭回归同理，我们也可以实现同样的核化并求解 α ~\\alpha~ α 的闭合形式： 模型为： l(w)=∑i=1n(xiTw−yi)+λ∣∣w∣∣22w^MAP=argminw ∑i=1n(xiTw−yi)+λ∣∣w∣∣22h(x)=∑j=1nαjk(xj,xi)l(w)=\\sum_{i=1}^n(x_i^Tw-y_i)+\\lambda||w||_2^2\\\\ \\hat{w}_{MAP}=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)+\\lambda||w||_2^2\\\\ h(x)=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) l(w)=i=1∑n​(xiT​w−yi​)+λ∣∣w∣∣22​w^MAP​=wargmin​ i=1∑n​(xiT​w−yi​)+λ∣∣w∣∣22​h(x)=j=1∑n​αj​k(xj​,xi​) α ~\\alpha~ α 的闭合形式为： w=XTα=(XTX+λI)−1XTY→(XTX+λI)XTα=XTY→(XTXXT+λXT)α=XTY→(XXT+λI)α=Y→α=(K+λI)−1Y\\begin{aligned} &amp;w=X^T\\alpha=(X^TX+\\lambda I)^{-1}X^TY\\rightarrow (X^TX+\\lambda I)X^T\\alpha=X^TY\\\\ &amp;\\rightarrow (X^TXX^T+\\lambda X^T)\\alpha=X^TY\\rightarrow (XX^T+\\lambda I)\\alpha=Y\\\\ &amp;\\rightarrow \\alpha=(K+\\lambda I)^{-1}Y \\end{aligned} ​w=XTα=(XTX+λI)−1XTY→(XTX+λI)XTα=XTY→(XTXXT+λXT)α=XTY→(XXT+λI)α=Y→α=(K+λI)−1Y​ 4.2 KNN核化 我们以欧拉距离的 KNN ~KNN~ KNN 模型为例： dist(xi,xj)=(xi−xj)T(xi−xj)=xiTxi−2xiTxj+xjTxj=k(xi,xi)−2k(xi,xj)+k(xj,xj)\\text{dist}(x_i,x_j)=(x_i-x_j)^T(x_i-x_j)=x_i^Tx_i-2x_i^Tx_j+x_j^Tx_j=k(x_i,x_i)-2k(x_i,x_j)+k(x_j,x_j) dist(xi​,xj​)=(xi​−xj​)T(xi​−xj​)=xiT​xi​−2xiT​xj​+xjT​xj​=k(xi​,xi​)−2k(xi​,xj​)+k(xj​,xj​) 但是上述核化的意义并不大，因此我们一般不对 KNN ~KNN~ KNN 算法进行核化。 4.3 支持向量机核化 线性支持向量机结合核函数是一个很强大的模型，我们往往求解其对偶问题，所以我们需要先了解对偶问题的定义。 4.3.1 拉格朗日乘子法与KKT条件 在求解最优化问题中，拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件是两种最常用的方法。在有等式约束时使用拉格朗日乘子法，在有不等约束时使用KKT条件。 我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值，支持向量机模型即为该类最优化为问题。 在求解最优化问题时一般会遇到三种情况： （1）无约束条件： 这是最简单的情况，解决方法通常是函数对变量求导，令求导函数等于0的点可能是极值点。将结果带回原函数进行验证即可。 （2）等式约束条件： 设目标函数为 f(x) ~f(x)~ f(x) ，约束条件为 hk(x) ~h_k(x)~ hk​(x) ，有 l ~l~ l 个约束条件，则等式约束条件的最优化问题可以表示为： 求解目标：min⁡f(x)约束条件：hk(x)=0,k=1,2,...l\\begin{aligned} &amp;求解目标：\\min f(x)\\\\ &amp;约束条件：h_k(x)=0,k=1,2,...l \\end{aligned} ​求解目标：minf(x)约束条件：hk​(x)=0,k=1,2,...l​ 我们要用到拉格朗日乘子法处理该最优化问题：首先定义拉格朗日函数： F(x.λ)=f(x)+∑k=1lλkhk(x)F(x.\\lambda)=f(x)+\\sum_{k=1}^l\\lambda_kh_k(x) F(x.λ)=f(x)+k=1∑l​λk​hk​(x) 然后解变量的偏导方程： ∂F(x,λ)∂x1=0,∂F(x,λ)∂x2=0,...,∂F(x,λ)∂xd=0∂F(x,λ)∂λ1=0,∂F(x,λ)∂λ2=0,...,∂F(x,λ)∂λk=0\\frac{\\partial F(x,\\lambda)}{\\partial x_1}=0,\\frac{\\partial F(x,\\lambda)}{\\partial x_2}=0,...,\\frac{\\partial F(x,\\lambda)}{\\partial x_d}=0\\\\ \\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_1}=0,\\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_2}=0,...,\\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_k}=0 ∂x1​∂F(x,λ)​=0,∂x2​∂F(x,λ)​=0,...,∂xd​∂F(x,λ)​=0∂λ1​∂F(x,λ)​=0,∂λ2​∂F(x,λ)​=0,...,∂λk​∂F(x,λ)​=0 （3）不等式约束条件： 设目标函数为 f(x) ~f(x)~ f(x) ，不等式约束条件为 gk(x) ~g_k(x)~ gk​(x) ，有 q ~q~ q 个不等式约束条件，等式约束条件为 hj(k) ~h_j(k)~ hj​(k) ，有 p ~p~ p 个等式约束条件： 求解目标：min⁡f(x)约束条件：hj(x)=0,j=1,2,...,p gk(x)≤0,k=1,2,...,q\\begin{aligned} &amp;求解目标：\\min f(x)\\\\ &amp;约束条件：h_j(x)=0,j=1,2,...,p\\\\ &amp;~~~~~~~~~~~~~~~~~g_k(x)\\le0,k=1,2,...,q \\end{aligned} ​求解目标：minf(x)约束条件：hj​(x)=0,j=1,2,...,p gk​(x)≤0,k=1,2,...,q​ 则我们可以定义不等式约束条件下的拉格朗日函数为： L(x,λ,μ)=f(x)+∑j=1pλjhj(x)+∑k=1qμkgk(x)L(x,\\lambda,\\mu)=f(x)+\\sum_{j=1}^p\\lambda_jh_j(x)+\\sum_{k=1}^q\\mu_kg_k(x) L(x,λ,μ)=f(x)+j=1∑p​λj​hj​(x)+k=1∑q​μk​gk​(x) 常用的方法是 KKT ~KKT~ KKT 条件：即最优值（局部最小值）必须满足以下条件： ① ∂L(x,λ,μ)∂x∣x=x∗=0② hj(x∗)=0③ μkgk(x∗)=0\\begin{aligned} &amp;①~\\frac{\\partial L(x,\\lambda,\\mu)}{\\partial x}|_{x=x^*}=0\\\\ &amp;②~h_j(x^*)=0\\\\ &amp;③~\\mu_kg_k(x^*)=0 \\end{aligned} ​① ∂x∂L(x,λ,μ)​∣x=x∗​=0② hj​(x∗)=0③ μk​gk​(x∗)=0​ 4.3.2 对偶问题的核化 我们首先回顾支持向量机的模型：为了便于计算我们引入一个常系数 12 ~\\frac12~ 21​ 求解目标：(w,b)=argminw,b 12∣∣w∣∣22约束条件：∀i , yi(wTxi+b)≥1\\begin{aligned} &amp;求解目标：(w,b)=\\underset{w,b}{argmin}~\\frac12||w||^2_2\\\\ &amp;约束条件：\\forall i~,~y_i(w^Tx_i+b)\\ge 1 \\end{aligned} ​求解目标：(w,b)=w,bargmin​ 21​∣∣w∣∣22​约束条件：∀i , yi​(wTxi​+b)≥1​ 则拉格朗日函数为： L(w,b,α)=12∣∣w∣∣22+∑i=1nαi(1−yi(wTxi+b))L(w,b,\\alpha)=\\frac12||w||_2^2+\\sum_{i=1}^n\\alpha_i\\big(1-y_i(w^Tx_i+b)\\big) L(w,b,α)=21​∣∣w∣∣22​+i=1∑n​αi​(1−yi​(wTxi​+b)) 根据 KKT ~KKT~ KKT 条件可得： ∂L(w,b,α)∂w=w−∑i=1nαiyixi=0∂L(w,b,α)∂b=−∑i=1nαiyi=0\\begin{aligned} &amp;\\frac{\\partial L(w,b,\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\\\ &amp;\\frac{\\partial L(w,b,\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_i y_i=0 \\end{aligned} ​∂w∂L(w,b,α)​=w−i=1∑n​αi​yi​xi​=0∂b∂L(w,b,α)​=−i=1∑n​αi​yi​=0​ 我们同时也知道，在 w,b ~w,b~ w,b 取得最优值时有： yi(wTxi+b)=1 ~y_i(w^Tx_i+b)=1~ yi​(wTxi​+b)=1 满足了 KKT ~KKT~ KKT 条件 综上，我们可得： w=∑i=1nαiyixi∑i=1nαiyi=0\\begin{aligned} &amp;w=\\sum_{i=1}^n\\alpha_iy_ix_i\\\\ &amp;\\sum_{i=1}^n\\alpha_iy_i=0 \\end{aligned} ​w=i=1∑n​αi​yi​xi​i=1∑n​αi​yi​=0​ 我们上述条件代入原式可得： (w,b)=argminw,b 12∑i=1n∑j=1nαiαjyiyjxiTxj+∑i=1nαi−∑i=1n∑j=1nαiyiyjxiTxj−b⋅∑i=1nαiyi→(w,b)=argminw,b ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj\\begin{aligned} &amp;(w,b)=\\underset{w,b}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j+\\sum_{i=1}^n\\alpha_i-\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_iy_iy_jx_i^Tx_j-b\\cdot\\sum_{i=1}^n\\alpha_iy_i\\\\ &amp;\\rightarrow(w,b)=\\underset{w,b}{argmin}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\end{aligned} ​(w,b)=w,bargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​+i=1∑n​αi​−i=1∑n​j=1∑n​αi​yi​yj​xiT​xj​−b⋅i=1∑n​αi​yi​→(w,b)=w,bargmin​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​​ 由此我们得到了支持向量机模型的对偶问题： 求解目标：α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj约束条件：∑i=1nαiyi=0\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0 \\end{aligned} ​求解目标：α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​约束条件：i=1∑n​αi​yi​=0​ 显然我们可以对该对偶问题进行核化： α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjk(xi,xj)\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j) α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​) 最终的模型为： h(x)=sign(wTx+b)=sign(∑i=1nαiyik(xi,x)+b)h(x)=\\text{sign}(w^Tx+b)=\\text{sign}\\big(\\sum_{i=1}^n\\alpha_iy_ik(x_i,x)+b\\big) h(x)=sign(wTx+b)=sign(i=1∑n​αi​yi​k(xi​,x)+b) 从支持向量的角度对对偶问题有一个很好的解释：对于原始公式，我们知道只有支持向量满足等式约束： yi(wTϕ(xi)+b)=1y_i\\big(w^T\\phi(x_i)+b\\big)=1 yi​(wTϕ(xi​)+b)=1 在对偶问题中我们可以使得支持向量所对应的 αi&gt;0 ~\\alpha_i&gt;0~ αi​&gt;0 ，而其他的输入向量对应的 αi=0 ~\\alpha_i=0~ αi​=0 ，在测试时我们只需要计算支持向量上 h(x) ~h(x)~ h(x) 的和，并在训练后丢弃所有 αi=0 ~\\alpha_i=0~ αi​=0 的特征向量。 对偶有一个明显的问题，就是 b ~b~ b 不再是优化的一部分了，但是我们需要它来进行分类，在对偶中支持向量是那些 αi&gt;0 ~α_i&gt;0~ αi​&gt;0 的向量，因此我们可以推导出 b ~b~ b ： yi(wTϕ(xi)+b)=1,yi∈{−1,+1}→b=yi−wTϕ(xi)→b=yi−∑j=1nαjyjk(xj,xi)\\begin{aligned} &amp;y_i(w^T\\phi(x_i)+b)=1,y_i\\in\\{-1,+1\\}\\rightarrow b=y_i-w^T\\phi(x_i)\\\\ &amp;\\rightarrow b=y_i-\\sum_{j=1}^n\\alpha_jy_jk(x_j,x_i) \\end{aligned} ​yi​(wTϕ(xi​)+b)=1,yi​∈{−1,+1}→b=yi​−wTϕ(xi​)→b=yi​−j=1∑n​αj​yj​k(xj​,xi​)​ 同时如果使用软间隔模型，则仅需添加一个新的约束： 0≤αi≤C0\\le\\alpha_i\\le C 0≤αi​≤C 五、模型实现 我们本次要手动实现核化的软间隔支持向量机，主要运用其对偶问题求解最优化问题。 5.1 数据集 我们本次要生成线性不可分的数据集，因为这样才能体现出核函数的优势所在，生成线性不可分数据集的代码如下所示：我们主要用到的是sklearn所提供的make_moons生成双半月环数据集，这是一个经典的线性不可分数据集。 生成的数据集如下图所示： 5.2 手动实现模型 5.2.1 模型阐述 求解思路来自：支持向量机（SVM）——对偶问题 我们首先要了解对于对偶形式的支持向量机模型的求解方法，考虑到软约束，我们的模型为： 求解目标：α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 我们最后求解出的模型为： h(xi)=∑j=1nαjyjxjTxi+bh(x_i)=\\sum_{j=1}^n\\alpha_jy_jx_j^Tx_i+b h(xi​)=j=1∑n​αj​yj​xjT​xi​+b 我们首先实现该模型的代码： 基于原始模型以及我们作出的假设， KKT ~KKT~ KKT 条件为： {0≤αi≤Cyi⋅h(xi)−1≥0αi(yi⋅h(xi)−1)=0\\begin{aligned} \\left\\{ \\begin{aligned} &amp;0\\le\\alpha_i\\le C\\\\ &amp;y_i\\cdot h(x_i)-1\\ge 0\\\\ &amp;\\alpha_i(y_i\\cdot h(x_i)-1)=0 \\end{aligned} \\right. \\end{aligned} ⎩⎪⎨⎪⎧​​0≤αi​≤Cyi​⋅h(xi​)−1≥0αi​(yi​⋅h(xi​)−1)=0​​ 因为我们有：支持向量所对应的 αi&gt;0 ~\\alpha_i&gt;0~ αi​&gt;0 ，而其他的输入向量对应的 αi=0 ~\\alpha_i=0~ αi​=0 的设定，支持向量是那些满足yih(xi)=1y_ih(x_i)=1yi​h(xi​)=1的点，所以有： αi(yi⋅h(xi)−1)=0\\alpha_i(y_i\\cdot h(x_i)-1)=0 αi​(yi​⋅h(xi​)−1)=0 接着我们需要求解核心问题： α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​ 5.2.2 求解思路 这是一个二次规划问题，我们用到 SMO ~SMO~ SMO 算法对其进行求解，算法思路来自：SMO算法详解 我们不妨先将问题作一个变形： α=argminα 12∑i=1n∑j=1nαiαjyiyjxiTxj−∑i=1nαi\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i\\\\ α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​−i=1∑n​αi​ 我们要寻找一个满足约束条件的 α=[ α1,α2,...,αn ] ~\\alpha=[~\\alpha_1,\\alpha_2,...,\\alpha_n~]~ α=[ α1​,α2​,...,αn​ ] ，假设我们已经找到了这样的一个 α ~\\alpha~ α ，即最终的超平面已经确定： h(xi)=wTxi+bh(x_i)=w^Tx_i+b h(xi​)=wTxi​+b 那么该超平面是满足 KKT ~KKT~ KKT 条件的，则有： {yi⋅h(xi)≥1→ αi=0 ,xi在边界内，正确分类yi⋅h(xi)=1→0&lt;αi&lt;C,xi在边界上，是支持向量，正确分类yi⋅h(xi)≤1→ αi=C ,xi在两条边界之间\\begin{aligned} &amp;\\left\\{ \\begin{aligned} &amp;y_i\\cdot h(x_i)\\ge1\\rightarrow ~~~~\\alpha_i=0~~~~,x_i在边界内，正确分类\\\\ &amp;y_i\\cdot h(x_i)=1\\rightarrow 0&lt;\\alpha_i&lt; C,x_i在边界上，是支持向量，正确分类\\\\ &amp;y_i\\cdot h(x_i)\\le1\\rightarrow ~~~~\\alpha_i=C~~~~,x_i在两条边界之间 \\end{aligned} \\right. \\end{aligned} ​⎩⎪⎨⎪⎧​​yi​⋅h(xi​)≥1→ αi​=0 ,xi​在边界内，正确分类yi​⋅h(xi​)=1→0&lt;αi​&lt;C,xi​在边界上，是支持向量，正确分类yi​⋅h(xi​)≤1→ αi​=C ,xi​在两条边界之间​​ 反过来想，我们求解出来的 αi ~\\alpha_i~ αi​ 和 xi ~x_i~ xi​ 也要满足上述关系。 综上我们的求解过程便是初始化一个 α ~\\alpha~ α 并不断得对其进行优化，直到找到最优的那个 α ~\\alpha~ α ， SMO ~SMO~ SMO 算法每次选择两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 进行更新。 根据我们的约束，显然这两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 满足： αiyi+αjyj=−∑k=1,k=i,jnαkyk=ξ\\alpha_iy_i+\\alpha_jy_j=-\\sum_{k=1,k\\not=i,j}^n\\alpha_ky_k=\\xi αi​yi​+αj​yj​=−k=1,k​=i,j∑n​αk​yk​=ξ 因此有： αi=ξ−αjyjyi=(ξ−αjyj)yi , yi∈{−1,+1}\\alpha_i=\\frac{\\xi-\\alpha_jy_j}{y_i}=(\\xi-\\alpha_jy_j)y_i~,~y_i\\in\\{-1,+1\\} αi​=yi​ξ−αj​yj​​=(ξ−αj​yj​)yi​ , yi​∈{−1,+1} 因此我们只需要找到一个 αj ~\\alpha_j~ αj​ 进行优化，并且求出优化后的值，那么我们的 αi ~\\alpha_i~ αi​ 也完成了优化，为了便于表示，后面我们将挑选出的两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 记作 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 从效果上考虑我们应该优化那些最不满足目标条件的 αi ~\\alpha_i~ αi​ ，而在我们优化过后它不满足目标条件的程度应该减小，而我们为了衡量一个 αi ~\\alpha_i~ αi​ 满足目标条件的程度，引入一个指标：我们首先定义一个误差： Ei=h(xi)−yiE_i=h(x_i)-y_i Ei​=h(xi​)−yi​ 我们发现 ∣E1−E2∣ ~|E_1-E_2|~ ∣E1​−E2​∣ 越大，优化后的 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 满足目标条件的程度就越大。 计算 Ei ~E_i~ Ei​ 的代码如下所示： 接着我们思考一个问题：我们该怎样确定优化后的 α ~\\alpha~ α 是朝着不满足目标条件程度减小的方向移动的呢？ 我们回归到原始的问题： 求解目标：α=argminα 12∑i=1n∑j=1nαiαjyiyjxiTxj−∑i=1nαi约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​−i=1∑n​αi​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 我们求解目标是求解目标函数的局部最小值，那么我们不妨将 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 看作变量，其他 αi ~\\alpha_i~ αi​ 看作常量，则： α=argminα W(α1,α2)W(α1,α2)=Aα12+Bα22+Cα1α2+Dα1+Eα2+F\\alpha=\\underset{\\alpha}{argmin}~W(\\alpha_1,\\alpha_2)\\\\ W(\\alpha_1,\\alpha_2)=A\\alpha_1^2+B\\alpha_2^2+C\\alpha_1\\alpha_2+D\\alpha_1+E\\alpha_2+F α=αargmin​ W(α1​,α2​)W(α1​,α2​)=Aα12​+Bα22​+Cα1​α2​+Dα1​+Eα2​+F 代入 α1=(ξ−α2y2)y1 ~\\alpha_1=(\\xi-\\alpha_2y_2)y_1~ α1​=(ξ−α2​y2​)y1​ 可得： W(α2)=A[(ξ−α2y2)y1]2+Bα22+C(ξ−α2y2)y1α2+D(ξ−α2y2)y1+Eα2+F =(A+B−Cy1y2)α22+(E−2Ay2−Dy1y2)α2+Aξ2+(C+D)ξ+F\\begin{aligned} &amp;W(\\alpha_2)=A\\big[(\\xi-\\alpha_2y_2)y_1\\big]^2+B\\alpha_2^2+C(\\xi-\\alpha_2y_2)y_1\\alpha_2+D(\\xi-\\alpha_2y_2)y_1+E\\alpha_2+F\\\\ &amp;~~~~~~~~~~~~~=(A+B-Cy_1y_2)\\alpha_2^2+(E-2Ay_2-Dy_1y_2)\\alpha_2+A\\xi^2+(C+D)\\xi+F \\end{aligned} ​W(α2​)=A[(ξ−α2​y2​)y1​]2+Bα22​+C(ξ−α2​y2​)y1​α2​+D(ξ−α2​y2​)y1​+Eα2​+F =(A+B−Cy1​y2​)α22​+(E−2Ay2​−Dy1​y2​)α2​+Aξ2+(C+D)ξ+F​ 那么求 W ~W~ W 的极小值，只需要简单得令 ∂W(α2)∂α2=0 ~\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2}=0~ ∂α2​∂W(α2​)​=0 即可，即： 2(A+B−Cy1y2)α2+(E−2Ay2−Dy1y2)=02(A+B-Cy_1y_2)\\alpha_2+(E-2Ay_2-Dy_1y_2)=0 2(A+B−Cy1​y2​)α2​+(E−2Ay2​−Dy1​y2​)=0 最终我们只需要保证我们优化后得 α ~\\alpha~ α 是使得目标函数变小的，就可以确定优化后的 α ~\\alpha~ α 是朝着不满足目标条件程度减小的方向移动。 5.2.3 SMO算法 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的更新 为了处理线性不可分的数据，我们引入了核函数： 求解目标：α=argminα 12∑i=1n∑j=1nαiαjyiyjk(xi,xj)−∑i=1nαi约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j)-\\sum_{i=1}^n\\alpha_i\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​)−i=1∑n​αi​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 核函数的实现代码如下所示，我们有多样的选择： 优化前后 α ~\\alpha~ α 都必须满足 ∑i=1nαiyi=0 ~\\sum_{i=1}^n\\alpha_iy_i=0~ ∑i=1n​αi​yi​=0 ，即： α1newy1+α2newy2=α1oldy1+α2oldy2=ξ\\alpha_1^{new}y_1+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\xi α1new​y1​+α2new​y2​=α1old​y1​+α2old​y2​=ξ 在更新 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 时，有： α1=(ξ−α2y2)y10≤α1≤C,0≤α2≤C\\alpha_1=(\\xi-\\alpha_2y_2)y_1\\\\ 0\\le\\alpha_1\\le C,0\\le\\alpha_2\\le C α1​=(ξ−α2​y2​)y1​0≤α1​≤C,0≤α2​≤C 将上述两个信息结合我们可以进一步得缩小 α2 ~\\alpha_2~ α2​ 的范围：我们将 α2 ~\\alpha_2~ α2​ 的上界与下界定义为 H,L ~H,L~ H,L L≤α2≤H\\begin{aligned} &amp;L\\le\\alpha_2\\le H \\end{aligned} ​L≤α2​≤H​ 将 α1y1+α2y2=ξ ~\\alpha_1y_1+\\alpha_2y_2=\\xi~ α1​y1​+α2​y2​=ξ 看作一个方程，显然几何上这是一条直线，结合取值范围我们可以绘制出图像： （1） y1=y2 ~y_1\\not=y_2~ y1​​=y2​ 时：有两种情况： α1−α2=ξ ~\\alpha_1-\\alpha_2=\\xi~ α1​−α2​=ξ ， α1−α2=−ξ ~\\alpha_1-\\alpha_2=-\\xi~ α1​−α2​=−ξ 0≤α2≤C0≤α1=α2+ξ≤C→−ξ≤α2≤C−ξ0≤α1=α2−ξ≤C→ ξ≤α2≤C+ξ\\begin{aligned} &amp;0\\le\\alpha_2\\le C\\\\ &amp;0\\le\\alpha_1=\\alpha_2+\\xi\\le C\\rightarrow -\\xi\\le\\alpha_2\\le C-\\xi\\\\ &amp;0\\le \\alpha_1=\\alpha_2-\\xi\\le C\\rightarrow ~~~\\xi\\le\\alpha_2\\le C+\\xi\\\\ \\end{aligned} ​0≤α2​≤C0≤α1​=α2​+ξ≤C→−ξ≤α2​≤C−ξ0≤α1​=α2​−ξ≤C→ ξ≤α2​≤C+ξ​ ① α1−α2=ξ ~\\alpha_1-\\alpha_2=\\xi~ α1​−α2​=ξ 时： L=max⁡(0,−ξ)=max⁡(0,α2−α1) H=min⁡(C,C−ξ)=min⁡(C,C+α2−α1)\\begin{aligned} &amp;~L=\\max(0,-\\xi)=\\max(0,\\alpha_2-\\alpha_1)\\\\ &amp;~H=\\min(C,C-\\xi)=\\min(C,C+\\alpha_2-\\alpha_1) \\end{aligned} ​ L=max(0,−ξ)=max(0,α2​−α1​) H=min(C,C−ξ)=min(C,C+α2​−α1​)​ ② α1−α2=−ξ ~\\alpha_1-\\alpha_2=-\\xi~ α1​−α2​=−ξ 时： L=max⁡(0,ξ)=max⁡(0,α2−α1)H=min⁡(C,C+ξ)=min⁡(C,C+α2−α1)\\begin{aligned} &amp;L=\\max(0,\\xi)=\\max(0,\\alpha_2-\\alpha_1)\\\\ &amp;H=\\min(C,C+\\xi)=\\min(C,C+\\alpha_2-\\alpha_1) \\end{aligned} ​L=max(0,ξ)=max(0,α2​−α1​)H=min(C,C+ξ)=min(C,C+α2​−α1​)​ 可以发现两种情况所得的上下界求解公式相同。 （2） y1=y2 ~y_1=y_2~ y1​=y2​ 时，有两种情况： α1+α2=ξ ~\\alpha_1+\\alpha_2=\\xi~ α1​+α2​=ξ ， α1+α2=−ξ ~\\alpha_1+\\alpha_2=-\\xi~ α1​+α2​=−ξ ，推导过程与 y1=y2 ~y_1\\not=y_2~ y1​​=y2​ 时的完全相同： 0≤α2≤C0≤α1= ξ−α2≤C → ξ−C≤α2≤ξ0≤α1=−ξ−α2≤C→−ξ−C≤α2≤−ξ\\begin{aligned} &amp;0\\le\\alpha_2\\le C\\\\ &amp;0\\le\\alpha_1=~~\\xi-\\alpha_2\\le C~\\rightarrow ~~~\\xi-C\\le\\alpha_2\\le\\xi\\\\ &amp;0\\le\\alpha_1=-\\xi-\\alpha_2\\le C\\rightarrow-\\xi-C\\le\\alpha_2\\le-\\xi \\end{aligned} ​0≤α2​≤C0≤α1​= ξ−α2​≤C → ξ−C≤α2​≤ξ0≤α1​=−ξ−α2​≤C→−ξ−C≤α2​≤−ξ​ ① α1+α2=ξ ~\\alpha_1+\\alpha_2=\\xi~ α1​+α2​=ξ 时： L=max⁡(0,ξ−C)=max⁡(0,α1+α2−C)H=min⁡(C,ξ)=min⁡(C,α1+α2)\\begin{aligned} &amp;L=\\max(0,\\xi-C)=\\max(0,\\alpha_1+\\alpha_2-C)\\\\ &amp;H=\\min(C,\\xi)=\\min(C,\\alpha_1+\\alpha_2) \\end{aligned} ​L=max(0,ξ−C)=max(0,α1​+α2​−C)H=min(C,ξ)=min(C,α1​+α2​)​ ② α1+α2=−ξ ~\\alpha_1+\\alpha_2=-\\xi~ α1​+α2​=−ξ 时： L=max⁡(0,−ξ−C)=max⁡(0,α1+α2−C)H=min⁡(C,−ξ)=min⁡(C,α1+α2)\\begin{aligned} &amp;L=\\max(0,-\\xi-C)=\\max(0,\\alpha_1+\\alpha_2-C)\\\\ &amp;H=\\min(C,-\\xi)=\\min(C,\\alpha_1+\\alpha_2) \\end{aligned} ​L=max(0,−ξ−C)=max(0,α1​+α2​−C)H=min(C,−ξ)=min(C,α1​+α2​)​ 两种情况所得的上下界求解公式也完全相同。 综上，上下界的计算通式如下： {L=max⁡(0,α2−α1) ,H=min⁡(C,C+α2−α1) if y1=y2L=max⁡(0,α1+α2−C),H=min⁡(C,α1+α2) if y1=y2\\left\\{ \\begin{aligned} &amp;L=\\max(0,\\alpha_2-\\alpha_1)~~~~~~~~,H=\\min(C,C+\\alpha_2-\\alpha_1)~~~~\\text{if }y_1\\not=y_2\\\\ &amp;L=\\max(0,\\alpha_1+\\alpha_2-C),H=\\min(C,\\alpha_1+\\alpha_2)~~~~~~~~~~~~\\text{if }y_1=y_2 \\end{aligned} \\right. {​L=max(0,α2​−α1​) ,H=min(C,C+α2​−α1​) if y1​​=y2​L=max(0,α1​+α2​−C),H=min(C,α1​+α2​) if y1​=y2​​ 接着我们要对α1,α2\\alpha_1,\\alpha_2α1​,α2​进行更新： α=argminα 12∑i=1n∑j=1nαiαjyiyjk(xi,xj)−∑i=1nαi=argminα W(α1,α2)\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j)-\\sum_{i=1}^n\\alpha_i=\\underset{\\alpha}{argmin}~W(\\alpha_1,\\alpha_2) α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​)−i=1∑n​αi​=αargmin​ W(α1​,α2​) 将 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 视为变量，其余 αi ~\\alpha_i~ αi​ 视为常量，则有：核矩阵 Kij=k(xi,xj) ~K_{ij}=k(x_i,x_j)~ Kij​=k(xi​,xj​) W(α1,α2)=12K11α12+12K22α22+K12y1y2α1α2+y1α1∑i=3nαiyiKi1+y2α2∑i=3nαiyiKi2−α1−α2−∑i=3nαiW(\\alpha_1,\\alpha_2)=\\frac12K_{11}\\alpha_1^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1\\sum_{i=3}^{n}\\alpha_iy_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^{n}\\alpha_iy_iK_{i2}-\\alpha_1-\\alpha_2-\\sum_{i=3}^n\\alpha_i W(α1​,α2​)=21​K11​α12​+21​K22​α22​+K12​y1​y2​α1​α2​+y1​α1​i=3∑n​αi​yi​Ki1​+y2​α2​i=3∑n​αi​yi​Ki2​−α1​−α2​−i=3∑n​αi​ 我们定义： vj=∑i=3nαiyiKij ~v_j=\\sum_{i=3}^n\\alpha_iy_iK_{ij}~ vj​=∑i=3n​αi​yi​Kij​ ， Constant=∑i=3nαi ~\\text{Constant}=\\sum_{i=3}^n\\alpha_i~ Constant=∑i=3n​αi​ ，则有： W(α1,α2)=12K11α12+12K22α22+K12y1y2α1α2+y1α1v1+y2α2v2−α1−α2−ConstantW(\\alpha_1,\\alpha_2)=\\frac12K_{11}\\alpha_1^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1v_1+y_2\\alpha_2v_2-\\alpha_1-\\alpha_2-\\text{Constant} W(α1​,α2​)=21​K11​α12​+21​K22​α22​+K12​y1​y2​α1​α2​+y1​α1​v1​+y2​α2​v2​−α1​−α2​−Constant 代入 α1=(ξ−y2α2)y1 ~\\alpha_1=(\\xi-y_2\\alpha_2)y_1~ α1​=(ξ−y2​α2​)y1​ 可得： W(α2)=12K11(ξ−y2α2)2+12K22α22+K12y2(ξ−y2α2)α2+v1(ξ−y2α2)+y2v2α2−(ξ−y2α2)y1−α2−Constant =12(K11+K22−2K12)α22+(K12ξy2−K11ξy2−v1y2+v2y2+y2y1−1)α2+12K11ξ2+v1ξ−ξy1−Constant\\begin{aligned} &amp;W(\\alpha_2)=\\frac12K_{11}(\\xi-y_2\\alpha_2)^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_2(\\xi-y_2\\alpha_2)\\alpha_2+v_1(\\xi-y_2\\alpha_2)+y_2v_2\\alpha_2-(\\xi-y_2\\alpha_2)y_1-\\alpha_2-\\text{Constant}\\\\ &amp;~~~~~~~~~~~~=\\frac12(K_{11}+K_{22}-2K_{12})\\alpha_2^2+(K_{12}\\xi y_2-K_{11}\\xi y_2-v_1y_2+v_2y_2+y_2y_1-1)\\alpha_2+\\frac12K_{11}\\xi^2+v_1\\xi-\\xi y_1-\\text{Constant} \\end{aligned} ​W(α2​)=21​K11​(ξ−y2​α2​)2+21​K22​α22​+K12​y2​(ξ−y2​α2​)α2​+v1​(ξ−y2​α2​)+y2​v2​α2​−(ξ−y2​α2​)y1​−α2​−Constant =21​(K11​+K22​−2K12​)α22​+(K12​ξy2​−K11​ξy2​−v1​y2​+v2​y2​+y2​y1​−1)α2​+21​K11​ξ2+v1​ξ−ξy1​−Constant​ 对 W ~W~ W 进行求导可得： ∂W(α2)∂α2=(K11+K22−2K12)α2+(K12ξy2−K11ξy2−v1y2+v2y2+y2y1−1)\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2}=(K_{11}+K_{22}-2K_{12})\\alpha_2+(K_{12}\\xi y_2-K_{11}\\xi y_2-v_1y_2+v_2y_2+y_2y_1-1) ∂α2​∂W(α2​)​=(K11​+K22​−2K12​)α2​+(K12​ξy2​−K11​ξy2​−v1​y2​+v2​y2​+y2​y1​−1) 我们令导数为0即可求得我们所期望更新的 α2 ~\\alpha_2~ α2​ ： (K11+K22−2K12)α2=(y2−y1+K11ξ−K12ξ+v1−v2)y2(K_{11}+K_{22}-2K_{12})\\alpha_2=(y_2-y_1+K_{11}\\xi-K_{12}\\xi+v_1-v_2)y_2 (K11​+K22​−2K12​)α2​=(y2​−y1​+K11​ξ−K12​ξ+v1​−v2​)y2​ 我们可以不计算 ξ ~\\xi~ ξ ，通过 α2old ~\\alpha_2^{old}~ α2old​ 更新 α2new ~\\alpha_2^{new}~ α2new​ ： vj=∑i=3nαiyiKij=h(xj)−∑i=12αiyik(xi,xj)−bv1=h(x1)−α1y1K11−α2y2K21−bv2=h(x2)−α1y1K12−α2y2K22−b\\begin{aligned} &amp;v_j=\\sum_{i=3}^n\\alpha_iy_iK_{ij}=h(x_j)-\\sum_{i=1}^2\\alpha_iy_ik(x_i,x_j)-b\\\\ &amp;v_1=h(x_1)-\\alpha_1y_1K_{11}-\\alpha_2y_2K_{21}-b\\\\ &amp;v_2=h(x_2)-\\alpha_1y_1K_{12}-\\alpha_2y_2K_{22}-b \\end{aligned} ​vj​=i=3∑n​αi​yi​Kij​=h(xj​)−i=1∑2​αi​yi​k(xi​,xj​)−bv1​=h(x1​)−α1​y1​K11​−α2​y2​K21​−bv2​=h(x2​)−α1​y1​K12​−α2​y2​K22​−b​ 由此可得： v1−v2=h(x1)−h(x2)−(y1K11−y1K12)α1−(y2K21−y2K22)α2v_1-v_2=h(x_1)-h(x_2)-(y_1K_{11}-y_1K_{12})\\alpha_1-(y_2K_{21}-y_2K_{22})\\alpha_2 v1​−v2​=h(x1​)−h(x2​)−(y1​K11​−y1​K12​)α1​−(y2​K21​−y2​K22​)α2​ 代入 α1=(ξ−y2α2)y1 ~\\alpha_1=(\\xi-y_2\\alpha_2)y_1~ α1​=(ξ−y2​α2​)y1​ 可得： v1−v2=h(x1)−h(x2)−(K11−K12)(ξ−y2α2)−(y2K21−y2K22)α2 =h(x1)−h(x2)+(K11+K22−2K12)y2α2+(K12−K11)ξ\\begin{aligned} &amp;v_1-v_2=h(x_1)-h(x_2)-(K_{11}-K_{12})(\\xi-y_2\\alpha_2)-(y_2K_{21}-y_2K_{22})\\alpha_2\\\\ &amp;~~~~~~~~~~~~~~=h(x_1)-h(x_2)+(K_{11}+K_{22}-2K_{12})y_2\\alpha_2+(K_{12}-K_{11})\\xi \\end{aligned} ​v1​−v2​=h(x1​)−h(x2​)−(K11​−K12​)(ξ−y2​α2​)−(y2​K21​−y2​K22​)α2​ =h(x1​)−h(x2​)+(K11​+K22​−2K12​)y2​α2​+(K12​−K11​)ξ​ 将 v1−v2 ~v_1-v_2~ v1​−v2​ 代入导数为0的式子可得： (K11+K22−2K12)α2new=([h(x1)−y1]−[h(x2)−y2])y2+(K11+K22−2K12)α2old(K_{11}+K_{22}-2K_{12})\\alpha_2^{new}=(\\big[h(x_1)-y_1\\big]-\\big[h(x_2)-y_2\\big])y_2+(K_{11}+K_{22}-2K_{12})\\alpha_2^{old} (K11​+K22​−2K12​)α2new​=([h(x1​)−y1​]−[h(x2​)−y2​])y2​+(K11​+K22​−2K12​)α2old​ 根据我们之前设置的误差： Ei=h(xi)−yiE_i=h(x_i)-y_i Ei​=h(xi​)−yi​ 代入可得： α2new=α2old+(E1−E2)y2K11+K22−2K12\\alpha_2^{new}=\\alpha_2^{old}+\\frac{(E_1-E_2)y_2}{K_{11}+K_{22}-2K_{12}} α2new​=α2old​+K11​+K22​−2K12​(E1​−E2​)y2​​ 综上我们终于得到了 α2 ~\\alpha_2~ α2​ 更新的递归式，但是不要忘记了约束条件，于是我们将该 α2 ~\\alpha_2~ α2​ 记作未经修剪的（unclipped）： α2new,unc ~\\alpha_2^{new,unc}~ α2new,unc​ 前面我们已经求出了 α2 ~\\alpha_2~ α2​ 上下界应该满足的条件，即： {L=max⁡(0,α2−α1) ,H=min⁡(C,C+α2−α1) if y1=y2L=max⁡(0,α1+α2−C),H=min⁡(C,α1+α2) if y1=y2\\left\\{ \\begin{aligned} &amp;L=\\max(0,\\alpha_2-\\alpha_1)~~~~~~~~,H=\\min(C,C+\\alpha_2-\\alpha_1)~~~~\\text{if }y_1\\not=y_2\\\\ &amp;L=\\max(0,\\alpha_1+\\alpha_2-C),H=\\min(C,\\alpha_1+\\alpha_2)~~~~~~~~~~~~\\text{if }y_1=y_2 \\end{aligned} \\right. {​L=max(0,α2​−α1​) ,H=min(C,C+α2​−α1​) if y1​​=y2​L=max(0,α1​+α2​−C),H=min(C,α1​+α2​) if y1​=y2​​ 所以可以得到修剪后的 α2 ~\\alpha_2~ α2​ ： α2new={ H , α2new,unc &gt;H α2new,unc , L≤α2new,unc ≤H L , α2new,unc &lt;L\\alpha_2^{new}=\\left\\{ \\begin{aligned} &amp;~~~~~H~~~~~~,~\\alpha_2^{new,unc}~&gt;H\\\\ &amp;~\\alpha_2^{new,unc}~,~L\\le\\alpha_2^{new,unc}~\\le H\\\\ &amp;~~~~~L~~~~~~,~\\alpha_2^{new,unc}~&lt;L \\end{aligned} \\right. α2new​=⎩⎪⎨⎪⎧​​ H , α2new,unc​ &gt;H α2new,unc​ , L≤α2new,unc​ ≤H L , α2new,unc​ &lt;L​ 选取修剪后的 α2 ~\\alpha_2~ α2​ 的代码如下所示： 我们还知道： α1newy1+α2newy2=α1oldy1+α2oldy2\\alpha_1^{new}y_1+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2 α1new​y1​+α2new​y2​=α1old​y1​+α2old​y2​ 由此我们也可以得到 α1 ~\\alpha_1~ α1​ 的更新公式： α1new=α1old+y1y2(α2old−α2new)\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new}) α1new​=α1old​+y1​y2​(α2old​−α2new​) b ~b~ b 的更新 我们每更新一次 α ~\\alpha~ α 后，都要对 b ~b~ b 进行一次更新，因为这关系到 h(x) ~h(x)~ h(x) 的计算，进而关系到 Ei ~E_i~ Ei​ 的计算。 ①当 0&lt;α1new&lt;C ~0&lt;\\alpha_1^{new}&lt;C~ 0&lt;α1new​&lt;C 时， x1 ~x_1~ x1​ 为支持向量，则有： y1(wTx1+b1)=1b1new=y1−∑i=1nαiyiKi1=y1−v1−α1newy1K11−α2newy2K12v1=h(x1)−α1oldy1K11−α2oldy2K21−boldy_1(w^Tx_1+b_1)=1\\\\ b_1^{new}=y_1-\\sum_{i=1}^n\\alpha_iy_iK_{i1}=y_1-v_1-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{12}\\\\ v_1=h(x_1)-\\alpha_1^{old}y_1K_{11}-\\alpha_2^{old}y_2K_{21}-b^{old} y1​(wTx1​+b1​)=1b1new​=y1​−i=1∑n​αi​yi​Ki1​=y1​−v1​−α1new​y1​K11​−α2new​y2​K12​v1​=h(x1​)−α1old​y1​K11​−α2old​y2​K21​−bold 则我们可以得到： b1new=bold−E1+(α1old−α1new)y1K11+(α2old−α2new)y2K21b_1^{new}=b^{old}-E_1+(\\alpha_1^{old}-\\alpha_1^{new})y_1K_{11}+(\\alpha_2^{old}-\\alpha_2^{new})y_2K_{21} b1new​=bold−E1​+(α1old​−α1new​)y1​K11​+(α2old​−α2new​)y2​K21​ ②当 0&lt;α2new&lt;C ~0&lt;\\alpha_2^{new}&lt;C~ 0&lt;α2new​&lt;C 时， x2 ~x_2~ x2​ 为支持向量，同理有： b2new=bold−E2+(α1old−α1new)y1K12+(α2old−α2new)y2K22b_2^{new}=b^{old}-E_2+(\\alpha_1^{old}-\\alpha_1^{new})y_1K_{12}+(\\alpha_2^{old}-\\alpha_2^{new})y_2K_{22} b2new​=bold−E2​+(α1old​−α1new​)y1​K12​+(α2old​−α2new​)y2​K22​ ③当 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 都不满足上述情况时，有： bnew=b1new+b2new2b^{new}=\\frac{b_1^{new}+b_2^{new}}2 bnew=2b1new​+b2new​​ 综上我们可以得到更新 b ~b~ b 的通式： bnew={ b1new ,0&lt;α1new&lt;C b2new , 0&lt;α2new&lt;Cb1new+b2new2,otherwiseb^{new}=\\left\\{ \\begin{aligned} &amp;~~~~~~b_1^{new}~~~~~~~,0&lt;\\alpha_1^{new}&lt;C\\\\ &amp;~~~~~~b_2^{new}~~~~~~~,~0&lt;\\alpha_2^{new}&lt;C\\\\ &amp;\\frac{b_1^{new}+b_2^{new}}2,\\text{otherwise} \\end{aligned} \\right. bnew=⎩⎪⎪⎪⎨⎪⎪⎪⎧​​ b1new​ ,0&lt;α1new​&lt;C b2new​ , 0&lt;α2new​&lt;C2b1new​+b2new​​,otherwise​ α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的选取 有了上述更新的方法，我们只剩下一个问题，就是如何选择 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ ，选择方法如下： ①选取 α1 ~\\alpha_1~ α1​ ：遍历所有 αi ~\\alpha_i~ αi​ ，把第一个不满足 KKT ~KKT~ KKT 条件的作为 α1 ~\\alpha_1~ α1​ ②选取 α2 ~\\alpha_2~ α2​ ：在所有不违反 KKT ~KKT~ KKT 条件的 αi ~\\alpha_i~ αi​ 中选取 ∣E1−E2∣ ~|E_1-E_2|~ ∣E1​−E2​∣ 最大的作为 α2 ~\\alpha_2~ α2​ 根据上述规则我们可以得到选取 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的代码： 我们知道 KKT ~KKT~ KKT 条件为： {yi⋅h(xi)≥1→ αi=0 ,xi在边界内yi⋅h(xi)=1→0&lt;αi&lt;C,xi在边界上，是支持向量yi⋅h(xi)≤1→ αi=C ,xi在两条边界之间\\begin{aligned} &amp;\\left\\{ \\begin{aligned} &amp;y_i\\cdot h(x_i)\\ge1\\rightarrow ~~~~\\alpha_i=0~~~~,x_i在边界内\\\\ &amp;y_i\\cdot h(x_i)=1\\rightarrow 0&lt;\\alpha_i&lt; C,x_i在边界上，是支持向量\\\\ &amp;y_i\\cdot h(x_i)\\le1\\rightarrow ~~~~\\alpha_i=C~~~,x_i在两条边界之间 \\end{aligned} \\right. \\end{aligned} ​⎩⎪⎨⎪⎧​​yi​⋅h(xi​)≥1→ αi​=0 ,xi​在边界内yi​⋅h(xi​)=1→0&lt;αi​&lt;C,xi​在边界上，是支持向量yi​⋅h(xi​)≤1→ αi​=C ,xi​在两条边界之间​​ 那么违反 KKT ~KKT~ KKT 条件的情况为： ① yih(xi)&gt;1 但 αi&gt;0② yih(xi)=1 但 αi=0或C③ yih(xi)&lt;1 但 αi&lt;C\\begin{aligned} &amp;①~y_ih(x_i)&gt;1~但~\\alpha_i&gt;0\\\\ &amp;②~y_ih(x_i)=1~但~\\alpha_i=0或C\\\\ &amp;③~y_ih(x_i)&lt;1~但~\\alpha_i&lt;C\\\\ \\end{aligned} ​① yi​h(xi​)&gt;1 但 αi​&gt;0② yi​h(xi​)=1 但 αi​=0或C③ yi​h(xi​)&lt;1 但 αi​&lt;C​ 为了便于判断我们可以进一步得进行处理： yih(xi)−1=yiEiy_ih(x_i)-1=y_iE_i yi​h(xi​)−1=yi​Ei​ 那么违反 KKT ~KKT~ KKT 条件的情况为： ① yiEi&lt;0 但 αi&lt;C② yiEi&gt;0 但 αi&gt;0\\begin{aligned} &amp;①~y_iE_i&lt;0~但~\\alpha_i&lt;C\\\\ &amp;②~y_iE_i&gt;0~但~\\alpha_i&gt;0 \\end{aligned} ​① yi​Ei​&lt;0 但 αi​&lt;C② yi​Ei​&gt;0 但 αi​&gt;0​ 但在实际中 KKT ~KKT~ KKT 条件是极其苛刻的，我们通过引入容错率 tol ~tol~ tol 来缓解， tol ~tol~ tol 一般取值为 0.0001 ~0.0001~ 0.0001 ① yiEi&lt;tol 但 αi&lt;C② yiEi&gt;tol 但 αi&gt;0\\begin{aligned} &amp;①~y_iE_i&lt;tol~但~\\alpha_i&lt;C\\\\ &amp;②~y_iE_i&gt;tol~但~\\alpha_i&gt;0 \\end{aligned} ​① yi​Ei​&lt;tol 但 αi​&lt;C② yi​Ei​&gt;tol 但 αi​&gt;0​ 由此我们可以定义 KKT ~KKT~ KKT 条件的相关代码： 综上，我们可以实现 SMO ~SMO~ SMO 算法+核函数的高效的支持向量机模型： 为了避免随机性影响模型评估，我们使用固定的数据集，并且固定得划分训练集与测试集，用到的测试集如下图所示： 我们可以发现对于上述线性不可分的数据，我们使用高斯核，通过调整参数达到了0.99的正确率，效果很好。 5.3 可视化 我们可以通过如下代码实现对决策边界的可视化： 我们可以观察不同的核函数得到的决策边界： ①高斯核函数： ②线性核函数： ③多项式核函数： 我们可以发现对于线性不可分数据高斯核函数的效果非常好。 5.4 调库实现模型 当然sklearn也提供了可以利用核函数的支持向量机，主要用到的函数为SVC，其函数原型为： 其中一些重要的参数为： ①C：软间隔的惩罚系数 ②kernel：用到的核函数：线性核：”linear“，多项式核：”poly“，高斯核：”rbf“，sigmoid核：”sigmoid“，预计算：”precomputed“ ③degree：多项式核的指数 ④gamma：”poly“，”rbf“，”sigmoid“核的系数选择方式 ● if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, ● if ‘auto’, uses 1 / n_features. 更多参数作用请详见官方说明文档：sklearn.svm.SVC 注意可视化处作了部分调整： 最终我们得到了0.995的准确率，决策边界如下图所示： 对比之下不难发现我们的模型虽然正确率也很高，但是存在一定程度的过拟合，调库所得的决策边界更好一些。 ","link":"https://2006wzt.github.io/post/机器学习实战（十三）：核函数/"},{"title":"机器学习实战（十二）：参数权衡","content":"参数权衡 一、模型评估 我们需要确定自己的模型当前效果如何来制定合理的优化策略，因此需要在线下对模型进行评估。 1.1 K折交叉验证 将数据集按层划分为 K ~K~ K 个互斥的子集，每次选择 K−1 ~K-1~ K−1 个子集作为训练集，剩下的一个子集作为验证集总共训练 K ~K~ K 次，将 K ~K~ K 次训练的测试结果取平均。 K ~K~ K 折交叉验证的误差为： ϵK−Fold=1n∑k=1K∑xi∈DkI(h(xi;D−Dk)=yi)\\epsilon_{K-Fold}=\\frac1n\\sum_{k=1}^K\\sum_{x_i\\in D_k}I(h(x_i;D-D_k)\\not=y_i) ϵK−Fold​=n1​k=1∑K​xi​∈Dk​∑​I(h(xi​;D−Dk​)​=yi​) 其中 D−Dk ~D-D_k~ D−Dk​ 是用 Dk ~D_k~ Dk​ 作验证集时的训练集， h(xi;D−Dk) ~h(x_i;D-D_k)~ h(xi​;D−Dk​) 是在 D−Dk ~D-D_k~ D−Dk​ 上训练出的模型。 1.2 排除交叉验证 在极端情况下，可以使用 K=n ~K=n~ K=n ，即只保留一个数据点（这通常称为LOOCV-保留一个交叉验证）。如果我们的数据集很小，并且无法保留许多数据点进行评估，则LOOCV很重要。 它的误差为： ϵLOOCV=1n∑i=1nI(h(xi;D−i)=yi)\\epsilon_{LOOCV}=\\frac1n\\sum_{i=1}^n I(h(x_i;D_{-i})\\not= y_i) ϵLOOCV​=n1​i=1∑n​I(h(xi​;D−i​)​=yi​) 其中 D−i ~D_{-i}~ D−i​ 是 D ~D~ D 去掉第 i ~i~ i 个数据点后的数据集， h(xi;D−i) ~h(x_i;D_{-i})~ h(xi​;D−i​) 是在 D−i ~D_{-i}~ D−i​ 上训练出的模型。 二、参数调整 我们以经验风险最小化为例： l(w)=1n∑i=1nI(h(xi),yi)+λr(w)l(w)=\\frac1n\\sum_{i=1}^nI(h(x_i),y_i)+\\lambda r(w) l(w)=n1​i=1∑n​I(h(xi​),yi​)+λr(w) 我们应该思考如何选择 λ ~\\lambda~ λ 才能尽可能得优化模型。 2.1 欠拟合与过拟合 回顾我们之前所学：在数据集上学习分类器时，可能会出现欠拟合和过拟合两种情况，每种情况都与训练集中的数据外推到未知数据的程度有关： ①欠拟合：在训练集上学习的分类器表达能力不足，甚至无法解释所提供的数据。 在这种情况下，训练误差和测试误差都会很高，因为分类器没有考虑训练集中存在的相关信息。 ②过拟合：在训练集上学习的分类器过于具体，无法用于准确推断有关未看到数据的任何内容。 虽然随着时间的推移，训练错误会继续减少，但随着分类器开始根据仅存在于训练集中而不存在于更广泛分布中的模式做出决策，测试错误会再次增加。 我们所追求的是模型在验证集上的误差最小，即寻找甜点。 2.2 显微镜式搜索 以经验风险最小化的参数 λ ~\\lambda~ λ 为例： ①我们首先找到 λ ~\\lambda~ λ 的最佳数量级：比如 λ ~\\lambda~ λ 数量级为：0.01,0.1,1,10,1000.01,0.1,1,10,1000.01,0.1,1,10,100中的 10 ~10~ 10 ②围绕最佳的数量级进行更细致的搜索：比如围绕 λ ~\\lambda~ λ 最佳的数量级 10 ~10~ 10 搜索： 5,10,15,20,...,95 ~5,10,15,20,...,95~ 5,10,15,20,...,95 三、偏差-方差权衡 3.1 问题假设 首先我们对数据集进行假设：数据集 D ~D~ D 是从某个分布 P(x,y) ~P(x,y)~ P(x,y) 的独立同分布中获取的。 ①期望标签：对于一个特征向量 x ~x~ x ，可能并不存在唯一的标签 y ~y~ y ，比如 x ~x~ x 描述一所房子的特征， y ~y~ y 是房子的价格，但是两所相同条件的房子可能会以不同的价格出售，所以对于任何给定的特征向量 x ~x~ x ，在可能的标签上都有一个分布。为此我们引入期望标签： y‾=Ey∣x[Y]=∫yy⋅P(y∣x) ∂y\\overline{y}=E_{y|x}[Y]=\\int_yy\\cdot P(y|x)~\\partial y y​=Ey∣x​[Y]=∫y​y⋅P(y∣x) ∂y 期望标签表示给定特征向量 x ~x~ x 时预期获得的标签。 ②期望测试误差：我们用算法 A ~\\mathcal{A}~ A 学习出一个模型 h ~h~ h ，记作： hD=A(D) ~h_D=\\mathcal{A}(D)~ hD​=A(D) ，由此我们可以定义泛化误差： E(x,y)∼P[(hD(x)−y)2]=∫x∫y(hD(x)−y)2P(x,y) ∂y∂xE_{(x,y)\\sim P}\\big[(h_D(x)-y)^2\\big]=\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x E(x,y)∼P​[(hD​(x)−y)2]=∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x 我们也可以假设 hD ~h_D~ hD​ 是一个随机的模型，我们目前只知道算法 A ~\\mathcal{A}~ A ，对测试误差求取期望： E(x,y)∼P,D∼Pn[(hD(x)−y)2]=∫D∫x∫y(hD(x)−y)2P(x,y) ∂y∂x∂DE_{(x,y)\\sim P,D\\sim P^n}\\big[(h_D(x)-y)^2\\big]=\\int_D\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x\\partial D E(x,y)∼P,D∼Pn​[(hD​(x)−y)2]=∫D​∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x∂D ③期望模型：模型 h ~h~ h 其实也是数据集 D ~D~ D 的一个函数，对于给定的算法 A ~\\mathcal{A}~ A ，我们可以对模型 h ~h~ h 求取期望： h‾=ED∼Pn[hD]=∫DhDP(D)∂D\\overline{h}=E_{D\\sim P^n}\\big[h_D\\big]=\\int_D h_DP(D)\\partial D h=ED∼Pn​[hD​]=∫D​hD​P(D)∂D P(D) ~P(D)~ P(D) 是从分布 Pn ~P^n~ Pn 中取出数据集 D ~D~ D 的概率， h‾ ~\\overline{h}~ h 是模型的加权平均。 3.2 期望测试误差的分解 我们可以对期望测试误差进行分解： E(x,y)∼P,D∼Pn[(hD(x)−y)2]=∫D∫x∫y(hD(x)−y)2P(x,y) ∂y∂x∂DE_{(x,y)\\sim P,D\\sim P^n}\\big[(h_D(x)-y)^2\\big]=\\int_D\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x\\partial D E(x,y)∼P,D∼Pn​[(hD​(x)−y)2]=∫D​∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x∂D 分解过程如下： Ex,y,D[(hD(x)−y)2]=Ex,y,D[(hD(x)−h‾(x)+h‾(x)−y)2]=Ex,D[(hD(x)−h‾(x))2]+2Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]+Ex,y[(h‾(x)−y)2]\\begin{aligned} &amp;E_{x,y,D}\\big[(h_D(x)-y)^2\\big]=E_{x,y,D}\\big[ (h_D(x)-\\overline{h}(x)+\\overline{h}(x)-y)^2\\big]\\\\ &amp;=E_{x,D}\\big[(h_D(x)-\\overline{h}(x))^2 \\big]+2E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]+E_{x,y}\\big[(\\overline{h}(x)-y)^2\\big] \\end{aligned} ​Ex,y,D​[(hD​(x)−y)2]=Ex,y,D​[(hD​(x)−h(x)+h(x)−y)2]=Ex,D​[(hD​(x)−h(x))2]+2Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]+Ex,y​[(h(x)−y)2]​ 我们对第2项继续分解： Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]=Ex,y[(ED[hD(x)]−h‾(x))(h‾(x)−y)]∵ED[hD(x)]=h‾(x)∴Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]=0\\begin{aligned} &amp;E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]=E_{x,y}\\big[(E_D\\big[h_D(x)\\big]-\\overline{h}(x))(\\overline{h}(x)-y)\\big]\\\\ &amp;\\because E_D\\big[h_D(x)\\big]=\\overline{h}(x)\\therefore E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]=0 \\end{aligned} ​Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]=Ex,y​[(ED​[hD​(x)]−h(x))(h(x)−y)]∵ED​[hD​(x)]=h(x)∴Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]=0​ 我们对第3项继续分解： Ex,y[(h‾(x)−y)2]=Ex,y[(h‾(x)−y‾(x)+y‾(x)−y)2]=Ex[(h‾(x)−y‾(x))2]+2Ex,y[(h‾(x)−y‾(x))(y‾(x)−y)]+Ex,y[(y‾(x)−y)2]\\begin{aligned} &amp;E_{x,y}\\big[(\\overline{h}(x)-y)^2\\big]=E_{x,y}\\big[(\\overline{h}(x)-\\overline{y}(x)+\\overline{y}(x)-y)^2\\big]\\\\ &amp;=E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))^2\\big]+2E_{x,y}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]+E_{x,y}\\big[(\\overline{y}(x)-y)^2\\big] \\end{aligned} ​Ex,y​[(h(x)−y)2]=Ex,y​[(h(x)−y​(x)+y​(x)−y)2]=Ex​[(h(x)−y​(x))2]+2Ex,y​[(h(x)−y​(x))(y​(x)−y)]+Ex,y​[(y​(x)−y)2]​ 继续分解： Ex[(h‾(x)−y‾(x))(y‾(x)−y)]=Ey[(h‾(x)−y‾(x))(y‾(x)−Ey∣x[y])]∵Ey∣x[y]=y‾(x)∴Ex[(h‾(x)−y‾(x))(y‾(x)−y)]=0\\begin{aligned} &amp;E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]=E_y\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-E_{y|x}\\big[y\\big])\\big]\\\\ &amp;\\because E_{y|x}\\big[y\\big]=\\overline{y}(x)\\therefore E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]=0 \\end{aligned} ​Ex​[(h(x)−y​(x))(y​(x)−y)]=Ey​[(h(x)−y​(x))(y​(x)−Ey∣x​[y])]∵Ey∣x​[y]=y​(x)∴Ex​[(h(x)−y​(x))(y​(x)−y)]=0​ 综上可得： Ex,y,D[(hD(x)−y)2]=Ex,D[(hD(x)−h‾(x))2]+Ex[(h‾(x)−y‾(x))2]+Ex,y[(y‾(x)−y)2]E_{x,y,D}\\big[(h_D(x)-y)^2\\big]=E_{x,D}\\big[(h_D(x)-\\overline{h}(x))^2 \\big]+E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))^2\\big]+E_{x,y}\\big[(\\overline{y}(x)-y)^2\\big] Ex,y,D​[(hD​(x)−y)2]=Ex,D​[(hD​(x)−h(x))2]+Ex​[(h(x)−y​(x))2]+Ex,y​[(y​(x)−y)2] 其中：Variance为方差，Noise为噪声，Bias为偏差。 3.3 方差、偏差、噪声 ①方差：它可以衡量我们用另一个训练集训练模型时，分类器的变化程度，如果存在最优分类器，则方差则衡量它离最优分类器有多远 ②偏差：偏差是模型所固有的，它可以用来衡量分类器有多偏向于某个特定的方案 ③噪声：噪声是数据所固有的，它可以用来衡量由于数据分布和特征表示而产生的歧义 如下图所示，假设靶心为我们所期望的最优模型的测试结果，利用不同的训练集 D ~D~ D 训练出的模型测试结果如各点分布，离靶心越近说明模型的偏差越小，测试的点越聚合说明模型的方差越小。 偏差和方差随模型复杂性的变化：由图可知模型越复杂，偏差越小，方差越大，因此我们需要对二者进行权衡 3.4 调试算法 此处我们讨论如何对机器学习算法进行调试：如果分类器性能不佳（例如，如果测试或训练误差过高）。有几种方法可以提高性能，要找出这些技术中哪一种适合这种情况，第一步是确定问题的根源。 如下图所示是训练误差和测试误差随着训练集大小的变化： 上图中有两个主要区域： ①Regime#1：训练误差低于期望的误差阈值 ε ~\\varepsilon~ ε ，但是测试误差较高，出现过拟合 ②Regime#2：训练误差和测试误差很接近，但二者都高于期望的误差阈值 ε ~\\varepsilon~ ε 3.4.1 高方差 区域Regime#1所反映的问题是高方差所导致的，由于方差较高，导致训练出的模型与我们所期望的模型差距较大。 辨别高方差的方法为： ①训练误差低于测试误差 ②训练误差下小于 ε ~\\varepsilon~ ε ，测试误差高于 ε ~\\varepsilon~ ε 解决高方差的方法为：利用更多的训练实例、降低模型复杂度等 3.4.2 高偏差 区域Regime#2所反映的问题是高偏差所导致的，由于偏差较高，导致模型拟合的结果与我们所期望的结果差距较大。 辨别高偏差的方法为：训练误差大于 ε ~\\varepsilon~ ε 解决高偏差的方法为：使用更复杂的模型、添加功能 ","link":"https://2006wzt.github.io/post/机器学习实战（十二）：参数权衡/"},{"title":"机器学习实战（十一）：经验风险最小化","content":"经验风险最小化 一、形式化定义 1.1 知识回顾 我们首先回顾一下SVM模型： (w,b)=argminw,b wTw+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ wTw+Ci=1∑n​max(1−yi​(wTxi​+b),0) 其对应的损失函数为： l(w,b)=∣∣w∣∣22+C∑i=1nmax⁡(1−yi(wTxi+b),0)l(w,b)=||w||_2^2+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) l(w,b)=∣∣w∣∣22​+Ci=1∑n​max(1−yi​(wTxi​+b),0) 我们将 C∑i=1nmax⁡(1−yi(wTxi+b),0) ~C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0)~ C∑i=1n​max(1−yi​(wTxi​+b),0) 称为铰链损失函数， ∣∣w∣∣22 ~||w||_2^2~ ∣∣w∣∣22​ 称为 l2 ~l2~ l2 正则化项 损失函数是惩罚训练错误的连续函数，正则化器是惩罚分类器复杂性的连续函数。 经验风险最小化模型的一般形式为：由损失函数 I(h(x),y) ~I(h(x),y)~ I(h(x),y) 和正则化项 r(w) ~r(w)~ r(w) 构成： min⁡w1n∑i=1nI(h(xi),yi)+λr(w)\\min_{w}\\frac1n\\sum_{i=1}^n I(h(x_i),y_i)+\\lambda r(w) wmin​n1​i=1∑n​I(h(xi​),yi​)+λr(w) 经验风险最小化（Empirical Risk Minimization）策略认为经验风险最小的模型是最优的模型，当样本容量足够大时，经验风险最小化能保证有很好的学习效果。比如，极大似然估计就是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但当样本容量很小时，经验风险最小化容易导致“过拟合”。 1.2 核心思想 我们考虑一个监督学习常见的场景： （1）我们有两个对象空间： X ~\\mathcal{X}~ X 和 Y ~\\mathcal{Y}~ Y ，X∈X,Y∈YX\\in \\mathcal{X},Y\\in\\mathcal{Y}X∈X,Y∈Y 我们要建模一个函数 h∈H ~h\\in\\mathcal{H}~ h∈H ，函数 h ~h~ h 是 X→Y ~X\\rightarrow Y~ X→Y 的映射，对于任意 x∈X ~x\\in X~ x∈X ，有 h(x)=y∈Y ~h(x)=y\\in Y~ h(x)=y∈Y （2）而模型的训练建立在我们所拥有的数据集： D={(x1,y1),(x2,y2),...,(xn,yn)} ~D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}~ D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)} 我们假设存在一个建立在 X ~X~ X 和 Y ~Y~ Y 上的联合分布 P(x,y) ~P(x,y)~ P(x,y) ， D ~D~ D 是从 P(x,y) ~P(x,y)~ P(x,y) 对应的一个独立同分布中获得的数据 独立同分布：每次抽样之间独立而且数据属于同一分布 注意：联合概率分布的假设允许我们对预测中的不确定性进行建模，因为 y ~y~ y 不是 x ~x~ x 的确定函数，而是一个具有固定 x ~x~ x 的条件分布 P(y∣x) ~P(y|x)~ P(y∣x) 的随机变量。 （3）我们还假设我们得到了一个非负的损失函数 L(h(x),y) ~L(h(x),y)~ L(h(x),y) ，它能反映出我们对标签的估计值和实际值之间的差距 然后我们将与假设 h(x) ~h(x)~ h(x) 的相关风险定义为损失函数的期望值： R(h)=E[L(h(x),y)]=∫L(h(x),y) dP(x,y)R(h)=E\\big[L(h(x),y)\\big]=\\int L(h(x),y)~\\mathrm{d}P(x,y) R(h)=E[L(h(x),y)]=∫L(h(x),y) dP(x,y) （4）我们的最终目标为找到一个： h∗=argminh R(h)h^*=\\underset{h}{argmin}~R(h) h∗=hargmin​ R(h) （5）通常，风险 R(h) ~R(h)~ R(h) 无法计算，因为学习算法并不知道联合分布 P(x,y) ~P(x,y)~ P(x,y) ，这种情况称为不可知学习 然而，我们可以通过平均训练集上的损失函数来计算近似值，我们称之为经验风险： Remp=1n∑i=1n L(h(xi),yi)R_{emp}=\\frac1n\\sum_{i=1}^n~L(h(x_i),y_i) Remp​=n1​i=1∑n​ L(h(xi​),yi​) 相应的，我们经验风险最小化所要建立的模型即为： h∗=argminh∈H Remp(h)h^*=\\underset{h\\in\\mathcal{H}}{argmin}~R_{emp}(h) h∗=h∈Hargmin​ Remp​(h) 经验风险最小化（ERM）是统计学习理论中的一个原则，它定义了一系列学习算法，并用于给出其性能的理论界限 二、损失函数 2.1 分类问题损失函数 （1）Hinge-Loss：在 p=1 ~p=1~ p=1 时，是软约束的支持向量机所引入的铰链损失函数 l(h(xi),yi)=max⁡(1−yi(wTxi+b),0)pl(h(x_i),y_i)=\\max(1-y_i(w^Tx_i+b),0)^p l(h(xi​),yi​)=max(1−yi​(wTxi​+b),0)p （2）Log-Loss：用于逻辑回归的损失函数 l(h(xi),yi)=log⁡(1+e−yih(xi))l(h(x_i),y_i)=\\log(1+e^{-y_ih(x_i)}) l(h(xi​),yi​)=log(1+e−yi​h(xi​)) （3）Exponential-Loss：指数损失函数，一般用于自适应提升算法 l(h(xi),yi)=e−yih(xi)l(h(x_i),y_i)=e^{-y_ih(x_i)} l(h(xi​),yi​)=e−yi​h(xi​) （4）Zero-One-Loss：0-1损失函数，用于分类问题 l(h(xi),yi)=I(h(xi)=yi)l(h(x_i),y_i)=I(h(x_i)\\not= y_i) l(h(xi​),yi​)=I(h(xi​)​=yi​) 如上图所示，横坐标为 yih(xi) ~y_ih(x_i)~ yi​h(xi​) 的值，纵坐标代表损失函数的值。 2.2 回归问题损失函数 （1）Squared-Loss：平方损失函数 l(h(xi),yi)=(h(xi)−yi)2l(h(x_i),y_i)=(h(x_i)-y_i)^2 l(h(xi​),yi​)=(h(xi​)−yi​)2 （2）Absolute-Loss：绝对值损失函数 l(h(xi),yi)=∣h(xi)−yi∣l(h(x_i),y_i)=|h(x_i)-y_i| l(h(xi​),yi​)=∣h(xi​)−yi​∣ （3）Huber-Loss：对 MSE ~MSE~ MSE 和 MAE ~MAE~ MAE 的结合，超参数 δ ~\\delta~ δ 决定了Huber-Loss对二者的侧重性 l(h(xi),yi)={ 12(h(xi)−yi)2 ,∣h(xi)−yi∣&lt;δδ⋅(∣h(xi)−yi∣−δ2),∣h(xi)−yi∣≥δl(h(x_i),y_i)=\\left\\{ \\begin{aligned} &amp;~~~~~\\frac12(h(x_i)-y_i)^2~~~~~,|h(x_i)-y_i|&lt;\\delta\\\\ &amp;\\delta\\cdot(|h(x_i)-y_i|-\\frac{\\delta}2),|h(x_i)-y_i|\\ge\\delta \\end{aligned} \\right. l(h(xi​),yi​)=⎩⎪⎪⎨⎪⎪⎧​​ 21​(h(xi​)−yi​)2 ,∣h(xi​)−yi​∣&lt;δδ⋅(∣h(xi​)−yi​∣−2δ​),∣h(xi​)−yi​∣≥δ​ （4）Log-Cosh-Loss： cosh⁡(x)=ex+e−x2 ~\\cosh(x)=\\frac{e^x+e^{-x}}{2}~ cosh(x)=2ex+e−x​ l(h(xi),yi)=log⁡cosh⁡(h(xi)−yi)l(h(x_i),y_i)=\\log\\cosh(h(x_i)-y_i) l(h(xi​),yi​)=logcosh(h(xi​)−yi​) 如上图所示，横坐标为 h(xi)−yi ~h(x_i)-y_i~ h(xi​)−yi​ 的值，纵坐标为损失函数的值。 三、正则化 在数学、统计学和计算机科学中，特别是在机器学习问题中，正则化是指添加信息以解决不确定问题或防止过度拟合的过程。 正则化器有助于改变优化问题的公式，以获得更好的几何直觉。 （1） l1 ~l1~ l1 正则化项： r(w)=∣∣w∣∣1=∑i=1dwir(w)=||w||_1=\\sum_{i=1}^d w_i r(w)=∣∣w∣∣1​=i=1∑d​wi​ （2） l2 ~l2~ l2 正则化项： r(w)=∣∣w∣∣22=∑i=1dwi2r(w)=||w||_2^2=\\sum_{i=1}^d w_i^2 r(w)=∣∣w∣∣22​=i=1∑d​wi2​ （3） lp ~lp~ lp 范数： r(w)=∣∣w∣∣p=(∑i=1dwip)1pr(w)=||w||_p=(\\sum_{i=1}^d w_i^p)^{\\frac1p} r(w)=∣∣w∣∣p​=(i=1∑d​wip​)p1​ （4）弹性网络（Elastic-Net）：对 l1 ~l1~ l1 和 l2 ~l2~ l2 正则化项的结合，超参数 α ~\\alpha~ α 决定了弹性网络对二者的侧重性， α ∈[0,1)~\\alpha~\\in[0,1) α ∈[0,1) r(w)=α∣∣w∣∣1+(1−α)∣∣w∣∣22r(w)=\\alpha||w||_1+(1-\\alpha)||w||_2^2 r(w)=α∣∣w∣∣1​+(1−α)∣∣w∣∣22​ 一些著名特例为： （1）最小二乘法： w^MLE=argminw 1n∑i=1n(xiTw−yi)2\\hat{w}_{MLE}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2 w^MLE​=wargmin​ n1​i=1∑n​(xiT​w−yi​)2 （2）岭回归： w^MAP=argminw 1n∑i=1n(xiTw−yi)2+λ∣∣w∣∣22\\hat{w}_{MAP}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_2^2 w^MAP​=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣22​ （3）Lasso回归： w^=argminw 1n∑i=1n(xiTw−yi)2+λ∣∣w∣∣1\\hat{w}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_1 w^=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣1​ （4）弹性网： w^=argminw 1n∑i=1n(xiTw−yi)2+α∣∣w∣∣1+(1−α)∣∣w∣∣22\\hat{w}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\alpha||w||_1+(1-\\alpha)||w||_2^2 w^=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+α∣∣w∣∣1​+(1−α)∣∣w∣∣22​ （5）逻辑回归： w^=argminw log⁡(1+e−yi(wTxi+b))\\hat{w}=\\underset{w}{argmin}~\\log(1+e^{-y_i(w^Tx_i+b)}) w^=wargmin​ log(1+e−yi​(wTxi​+b)) （6）支持向量机： (w,b)=argminw,b ∣∣w∣∣22+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~||w||_2^2+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ ∣∣w∣∣22​+Ci=1∑n​max(1−yi​(wTxi​+b),0) ","link":"https://2006wzt.github.io/post/机器学习实战（十一）：经验风险最小化/"},{"title":"机器学习实战（十）：支持向量机","content":"支持向量机 一、基本思想 支持向量机（Support Vector Machine）可以看作是感知机的扩展。如果存在一个超平面可以将数据点分为两类，感知机将会找到这个超平面，而支持向量机则是找到一个具有最大边距的超平面。 其形式化定义与感知机相同： 数据集：D={(x1,y1),(x2,y2),...,(xn,yn)},yi∈{−1,+1}模型：h(x)=sign(wTx+b)\\begin{aligned} &amp;数据集：D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\},y_i\\in\\{-1,+1\\}\\\\ &amp;模型：h(x)=\\text{sign}(w^Tx+b) \\end{aligned} ​数据集：D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)},yi​∈{−1,+1}模型：h(x)=sign(wTx+b)​ 如果数据是二元可分的，感知机可以找到很多不同的超平面，但是哪个超平面是最好的呢？这就需要我们用SVM去寻找 SVM找到的是一个具有最大边距的超平面如下图所示，γ是超平面到两类点之间的最小距离，而我们要找到一个超平面使得γ最大，这也意味着该超平面在这两类数据点的中间，即SVM找到的超平面到两类点的最小距离是相等的。 二、线性支持向量机 2.1 计算边距γ\\gammaγ 假设当前的超平面为： H={x∣wTx+b=0}\\mathcal{H}=\\{x|w^Tx+b=0\\} H={x∣wTx+b=0} 任取超平面中的一个点xxx，则它到超平面的距离为：设xxx到超平面的距离为ddd，它在超平面上的投影为xPx^PxP 根据向量关系，则有：xP+d=x，xP∈H∵w为H的法向量 ∴w//d，d=αw∴xP=x−d=x−αw∴wTxP+b=wT(x−αw)+b=0∴α=wTx+bwTw，d=αw=dTd=∣α∣wTw=∣wTx+b∣wTw\\begin{aligned} &amp;根据向量关系，则有：x^P+d=x，x^P\\in \\mathcal{H}\\\\ &amp;\\because w为\\mathcal{H}的法向量~~~~\\therefore w//d，d=\\alpha w\\\\ &amp;\\therefore x^P=x-d=x-\\alpha w\\\\ &amp;\\therefore w^Tx^P+b=w^T(x-\\alpha w)+b=0\\\\ &amp;\\therefore \\alpha =\\frac{w^Tx+b}{w^Tw}，d=\\alpha w=\\sqrt{d^Td}=|\\alpha|\\sqrt{w^Tw}=\\frac{|w^Tx+b|}{\\sqrt{w^Tw}} \\end{aligned} ​根据向量关系，则有：xP+d=x，xP∈H∵w为H的法向量 ∴w//d，d=αw∴xP=x−d=x−αw∴wTxP+b=wT(x−αw)+b=0∴α=wTwwTx+b​，d=αw=dTd​=∣α∣wTw​=wTw​∣wTx+b∣​​ 由此我们得到： γ(w,b)=min⁡x∈D∣wTx+b∣wTw\\gamma(w,b)=\\min_{x\\in D}\\frac{|w^Tx+b|}{\\sqrt{w^Tw}} γ(w,b)=x∈Dmin​wTw​∣wTx+b∣​ 根据比例关系，我们可以进一步得简化计算过程： γ(w,b)=min⁡x∈D∣wTx+b∣wTw=min⁡x∈Dβ∣wTx+b∣βwTw=r(βw,βb),β=0\\begin{aligned} &amp;\\gamma(w,b)=\\min_{x\\in D}\\frac{|w^Tx+b|}{\\sqrt{w^Tw}}=\\min_{x\\in D}\\frac{\\beta|w^Tx+b|}{\\beta\\sqrt{w^Tw}}=r(\\beta w,\\beta b),\\beta\\not=0 \\end{aligned} ​γ(w,b)=x∈Dmin​wTw​∣wTx+b∣​=x∈Dmin​βwTw​β∣wTx+b∣​=r(βw,βb),β​=0​ 我们可以找到一个合适的β\\betaβ，对于使得γ(w,b)\\gamma(w,b)γ(w,b)取得最小值的xxx有： β∣wTx+b∣=1\\beta|w^Tx+b|=1 β∣wTx+b∣=1 我们对空间进行比例变换：w=βw，b=βbw=\\beta w，b=\\beta bw=βw，b=βb，则γ(w,b)\\gamma(w,b)γ(w,b)可表示为： γ(w,b)=1∣∣w∣∣2\\gamma(w,b)=\\frac{1}{||w||_2} γ(w,b)=∣∣w∣∣2​1​ 2.2 最大化边距γ\\gammaγ 我们可以将求解γ最大值的过程等效为一个约束化问题： 求解问题：(w,b)=argmaxw,b γ(w,b)约束条件：yi(wTxi+b)≥0\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmax}~\\gamma(w,b)\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 0 \\end{aligned} ​求解问题：(w,b)=w,bargmax​ γ(w,b)约束条件：yi​(wTxi​+b)≥0​ 结合我们上述的比例变换，则约束化问题等效为： 求解问题：(w,b)=argmaxw,b γ(w,b)=argmaxw,b 1∣∣w∣∣2=argminw,b wTw约束条件：min⁡x∈D∣wTx+b∣=1,yi(wTx+b)≥0\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmax}~\\gamma(w,b)=\\underset{w,b}{argmax}~\\frac{1}{||w||_2}=\\underset{w,b}{argmin}~w^Tw\\\\ &amp;约束条件：\\min_{x\\in D}|w^Tx+b|=1,y_i(w^Tx+b)\\ge 0 \\end{aligned} ​求解问题：(w,b)=w,bargmax​ γ(w,b)=w,bargmax​ ∣∣w∣∣2​1​=w,bargmin​ wTw约束条件：x∈Dmin​∣wTx+b∣=1,yi​(wTx+b)≥0​ 我们可以对约束条件继续进行整合： min⁡x∈D∣wTx+b∣=1→∣wTx+b∣≥1yi∈{−1,+1}→yi(wTxi+b)≥1\\begin{aligned} &amp;\\min_{x\\in D}|w^Tx+b|=1\\rightarrow |w^Tx+b|\\ge 1\\\\ &amp;y_i\\in\\{-1,+1\\}\\rightarrow y_i(w^Tx_i+b)\\ge 1 \\end{aligned} ​x∈Dmin​∣wTx+b∣=1→∣wTx+b∣≥1yi​∈{−1,+1}→yi​(wTxi​+b)≥1​ 最终，我们得到的约束化问题为： 求解问题：(w,b)=argminw,b wTw约束条件：yi(wTxi+b)≥1\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 1\\\\ \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw约束条件：yi​(wTxi​+b)≥1​ 在我们求得最终的w,bw,bw,b时，有yi(wTxi+b)=1y_i(w^Tx_i+b)=1yi​(wTxi​+b)=1 这是一个二次最优化问题，目标是二次的，约束是线性的，我们可以用任何QCQP（二次约束二次规划解算器）唯一且有效地求解它。 如上图所示，我们将离决策边界最近的几个点称为支持向量，支持向量机的最终模型仅仅与支持向量有关 三、软约束 3.1 噪声影响 如果存在噪声，可能使得数据线性不可分，如下图所示，这就导致我们无法找到一个最终的超平面，而感知机模型是通过限制最大训练轮数避免死循环的，支持向量机则是通过引入软约束处理噪声。 在能找到超平面的情况下也可能存在一些噪声点使得超平面的选取并不好，如下图所示，我们仍要尽量规避掉这类噪声。 3.2 软约束实现 对于上述的例子，我们可以将那些噪音点忽略掉，并将它们看作损失，统计进行忽略后不匹配的点的个数加到目标公式中，则有： 求解问题：(w,b)=argminw,b wTw+C∑i=1nI(yi=sign(wTxi+b))约束条件：正确分类的yi:yi(wTxi+b)≥1，不正确分类的yi(wTxi+b)≥∞\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^nI(y_i\\not=\\text{sign}(w^Tx_i+b))\\\\ &amp;约束条件：正确分类的y_i:y_i(w^Tx_i+b)\\ge 1，不正确分类的y_i(w^Tx_i+b)\\ge\\infty \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw+Ci=1∑n​I(yi​​=sign(wTxi​+b))约束条件：正确分类的yi​:yi​(wTxi​+b)≥1，不正确分类的yi​(wTxi​+b)≥∞​ 即：我们对分类错误的点不加以任何约束，CCC是对大边距和噪声点耐受性的权衡 但是上述计算公式是非线性且不连续的，为此我们引入一个线性松弛变量ξi\\xi_iξi​： 求解问题：(w,b)=argminw,b wTw+C∑i=1nξi约束条件：yi(wTxi+b)≥1−ξi\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\xi_i\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 1-\\xi_i \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw+Ci=1∑n​ξi​约束条件：yi​(wTxi​+b)≥1−ξi​​ 松弛变量 ξi ~\\xi_i~ ξi​ 允许输入xix_ixi​更接近超平面（甚至位于错误的一侧），但这种“松弛”在目标函数中会受到惩罚。 CCC非常大时，SVMSVMSVM将变得非常严格，尝试将所有的点都分到正确的一侧去 CCC非常小时，SVMSVMSVM将变得非常松散，可能会牺牲一些简单的点来获得超平面 松弛变量 ξi ~\\xi_i~ ξi​ 的定义方式如下：我们可以将 ξi ~\\xi_i~ ξi​ 看成损失函数，SVMSVMSVM的建立要尽可能得最小化 ξi ~\\xi_i~ ξi​ ξi={1−yi(wTxi+b) , yi(wTxi+b)&lt;1 0 , yi(wTxi+b)≥1\\begin{aligned} &amp;\\xi_i=\\left\\{ \\begin{aligned} &amp;1-y_i(w^Tx_i+b)~,~y_i(w^Tx_i+b)&lt;1\\\\ &amp;~~~~~~~~~~~~0~~~~~~~~~~~~~~~~,~y_i(w^Tx_i+b)\\ge 1 \\end{aligned} \\right. \\end{aligned} ​ξi​={​1−yi​(wTxi​+b) , yi​(wTxi​+b)&lt;1 0 , yi​(wTxi​+b)≥1​​ 即：ξi=max⁡(1−yi(wTxi+b),0)\\xi_i=\\max(1-y_i(w^Tx_i+b),0)ξi​=max(1−yi​(wTxi​+b),0) 最终我们的软约束模型为： (w,b)=argminw,b wTw+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ wTw+Ci=1∑n​max(1−yi​(wTxi​+b),0) 参数CCC对支持向量机最后拟合结果的影响如图所示： 四、算法实现 支持向量机的手动实现结合核函数最优，因此我们此处仅通过调库实现模型。 4.1 数据集 我们此处选择自己生成数据集，依靠的是sklearn的make_blobs函数，这是一个用于生成聚类数据的函数 我们生成的数据集分布的例子如下： 4.2 模型实现 我们实现的是线性支持向量机，不可以用核函数进行优化，主要用到的是sklearn的LinearSVC函数，它的函数原型如下： ①penalty：正则化项，可选择：&quot;l1&quot;,&quot;l2&quot; ②loss：损失函数，可选择：&quot;hinge&quot;（合页损失函数），&quot;squared_hinge&quot;（合页损失函数的平方） ③dual：求解对偶问题，当样本数量&gt;特征数量时倾向于解决原始问题 ④tol：中止迭代的阈值 ⑤C：软约束的惩罚系数 ⑥multi_class：多分类问题的策略，&quot;ovr&quot;（one-vs-rest分类策略），&quot;crammer_singer&quot;（多类联合分类） ⑦fit_intercept：是否考虑截距b ⑧max_iter：最大训练轮数 五、总结 ①支持向量机（SVM）是一种线性分类器，可以看作是感知器的扩展，支持向量机寻找最大边距分离超平面。 ②边距是从超平面到两个类中最近点的距离。 ③最大边距分类器是在所有数据点必须位于超平面的正确一侧的约束下，使边距最大化。 ④对于最优的w，b对，一些训练点会有严格的约束，我们将这些训练点称为支持向量，严格的约束为： yi(wTxi+b)=1y_i(w^Tx_i+b)=1 yi​(wTxi​+b)=1 ⑤支持向量机是凸二次函数，可以用QCQP求解器求解。 ⑥通过在目标中加入松弛变量，我们可以得到无约束SVM公式：软约束SVM。 ","link":"https://2006wzt.github.io/post/机器学习实战（十）：支持向量机/"},{"title":"机器学习实战（九）：线性回归","content":"线性回归 一、形式化定义 在统计学中，线性回归是一种线性方法，用于建模标量响应与一个或多个解释变量之间的关系。 当只有一个解释变量时，我们称之为简单线性回归： 数据假设：yi∈R模型假设：yi=xiTw+ϵiϵi∼N(0,σ2),yi∣xi∼N(xiTw,σ2)\\begin{aligned} &amp;数据假设：y_i\\in R\\\\ &amp;模型假设：y_i=x_i^Tw+\\epsilon_i\\\\ &amp;\\epsilon_i\\sim N(0,\\sigma^2),y_i|x_i\\sim N(x_i^Tw,\\sigma^2) \\end{aligned} ​数据假设：yi​∈R模型假设：yi​=xiT​w+ϵi​ϵi​∼N(0,σ2),yi​∣xi​∼N(xiT​w,σ2)​ 根据对分布的模型假设，我们最终得到的条件概率为： P(yi∣xi,w)=12πσ2e−(xiTw−yi)22σ2P(y_i|x_i,w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_i^Tw-y_i)^2}{2\\sigma^2}} P(yi​∣xi​,w)=2πσ2​1​e−2σ2(xiT​w−yi​)2​ 我们假设模型是一条过原点的直线（类似于感知机通过升维即可使得函数过原点）而对于每个特征xix_ixi​，它的标签yiy_iyi​则从一个平均值为wTxiw^Tx_iwTxi​，方差为σ2σ^2σ2的高斯分布中得出，我们在线性回归建模中的任务便是根据数据集估计斜率www。 单标量预测变量xxx和单标量响应变量yyy的最简单情况称为简单线性回归，而多个预测变量xxx参与的预测为多元线性回归，几乎所有现实世界的回归模型都涉及多个预测因子，线性回归的基本描述通常用多元回归模型来表述。 二、参数估计 线性回归模型中的重要参数就是“斜率”www，我们同样可以通过MLEMLEMLE和MAPMAPMAP两种方式对它进行估计。 进行下列推导之前，我们先了解几个常用的矩阵求导公式： ∂Ax∂x=AT,∂xTA∂x=A,∂ATxB∂x=ABT∂ATxTB∂x=BAT,∂ATxxTB∂x=(ABT+BAT)x,∂xTAx∂x=2Ax\\frac{\\partial Ax}{\\partial x}=A^T,\\frac{\\partial x^TA}{\\partial x}=A,\\frac{\\partial A^TxB}{\\partial x}=AB^T\\\\\\frac{\\partial A^Tx^TB}{\\partial x}=BA^T,\\frac{\\partial A^Txx^TB}{\\partial x}=(AB^T+BA^T)x,\\frac{\\partial x^TAx}{\\partial x}=2Ax ∂x∂Ax​=AT,∂x∂xTA​=A,∂x∂ATxB​=ABT∂x∂ATxTB​=BAT,∂x∂ATxxTB​=(ABT+BAT)x,∂x∂xTAx​=2Ax 再了解一些矩阵的相关性质： (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT(A^T)^T=A,(A+B)^T=A^T+B^T,(AB)^T=B^TA^T (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT 2.1 极大似然估计MLE w^MLE=argmaxw P(Y,X∣w)=argmaxw ∏i=1nP(yi,xi∣w) =argmaxw ∏i=1nP(yi∣xi,w)P(xi∣w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡12πσ2e−(wTxi−yi)22σ2 =argminw ∑i=1n(xiTw−yi)2=argminw 1n∑i=1n(xiTw−yi)2\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y,X|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)P(x_i|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(w^Tx_i-y_i)^2}{2\\sigma^2}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2 \\end{aligned} ​w^MLE​=wargmax​ P(Y,X∣w)=wargmax​ i=1∏n​P(yi​,xi​∣w) =wargmax​ i=1∏n​P(yi​∣xi​,w)P(xi​∣w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log2πσ2​1​e−2σ2(wTxi​−yi​)2​ =wargmin​ i=1∑n​(xiT​w−yi​)2=wargmin​ n1​i=1∑n​(xiT​w−yi​)2​ 我们可以发现，线性回归问题的极大似然估计结果与最小二乘法（OLS）的形式完全相同。 此外我们还可以推导出www的闭合形式解，这样会使得计算更加方便：根据上述估计值，我们可以定义损失函数l(w)l(w)l(w) l(w)=(Xw−Y)2\\begin{aligned} &amp;l(w)=(Xw-Y)^2 \\end{aligned} ​l(w)=(Xw−Y)2​ 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www，XXX为n×dn\\times dn×d维矩阵，www为d×1d\\times 1d×1维矩阵，YYY为n×1n\\times1n×1维矩阵： l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂l(w)∂w=2XTXw−2XTY=0则有：w=(XTX)−1XTY\\begin{aligned} &amp;l(w)=(Xw-Y)^T(Xw-Y)=(w^TX^T-Y^T)(Xw-Y)\\\\ &amp;~~~~~~~=w^TX^TXw-w^TX^TY-Y^TXw-Y^TY\\\\ &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY=0\\\\ &amp;则有：w=(X^TX)^{-1}X^TY \\end{aligned} ​l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂w∂l(w)​=2XTXw−2XTY=0则有：w=(XTX)−1XTY​ 2.2 极大后验估计MAP 引入一个附加的模型假设： P(w)=12πτ2e−wTw2τ2P(w)=\\frac1{\\sqrt{2\\pi\\tau^2}}e^{-\\frac{w^Tw}{2\\tau^2}} P(w)=2πτ2​1​e−2τ2wTw​ 则极大后验估计的过程为： w^MAP=argmaxw P(w∣Y,X)=argmaxw P(Y,X∣w)P(w)P(Y,X) =argmaxw P(Y,X∣w)P(w)=argmaxw ∏i=1nP(yi,xi∣w)P(w) =argmaxw ∑i=1nlog⁡P(yi,xi∣w)+log⁡P(w) =argmaxw −∑i=1n(xiTw−yi)2−12τ2wTw =argminw ∑i=1n(xiTw−yi)2+λ∣∣w∣∣22\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|Y,X)=\\underset{w}{argmax}~\\frac{P(Y,X|w)P(w)}{P(Y,X)}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~P(Y,X|w)P(w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i,x_i|w)+\\log P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\sum_{i=1}^n(x_i^Tw-y_i)^2-\\frac{1}{2\\tau^2}w^Tw\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_2^2 \\end{aligned} ​w^MAP​=wargmax​ P(w∣Y,X)=wargmax​ P(Y,X)P(Y,X∣w)P(w)​ =wargmax​ P(Y,X∣w)P(w)=wargmax​ i=1∏n​P(yi​,xi​∣w)P(w) =wargmax​ i=1∑n​logP(yi​,xi​∣w)+logP(w) =wargmax​ −i=1∑n​(xiT​w−yi​)2−2τ21​wTw =wargmin​ i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣22​​ 我们将这一回归称为岭回归，它同样存在闭合形式：我们定义损失函数为： l(w)=(Xw−Y)2+λwTwl(w)=(Xw-Y)^2+\\lambda w^Tw l(w)=(Xw−Y)2+λwTw 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www： ∂l(w)∂w=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY+2\\lambda Iw=0\\\\ &amp;则有：w=(X^TX+\\lambda I)^{-1}X^TY \\end{aligned} ​∂w∂l(w)​=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY​ 三、模型实现 对于简单的线性回归模型，我们通过闭合形式可以直接求解，将分别通过手动实现和调库的方式构造模型。 3.1 数据集 我们使用波士顿房价数据集，查看数据集的内容并对其进行存储依靠以下部分代码： 波士顿房价数据集有506个样本，每个样本有13个特征，接下来我们利用线性回归模型对数据集进行拟合。 3.2 手动实现模型 我们的评估指标选择r2r2r2系数，其定义如下： R2=1−∑i=1n(yi−yi^)2∑i=1n(yi−y‾)2=1−MSEVarR^2=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{\\sum_{i=1}^n(y_i-\\overline{y})^2}=1-\\frac{MSE}{Var} R2=1−∑i=1n​(yi​−y​)2∑i=1n​(yi​−yi​^​)2​=1−VarMSE​ 显然，预测越准r2r2r2系数越趋近于1。 3.3 调库实现模型 调库主要用到的是sklearn中的LinearRegression模型，其具体参数请查阅文档 ","link":"https://2006wzt.github.io/post/机器学习实战（九）：线性回归/"},{"title":"机器学习实战（八）：梯度下降","content":"梯度下降 一、基本思想 1.1 知识回顾 我们首先回顾逻辑回归模型的参数估计： 对于给定的数据集D={(x1,y1),(x2,y2),...,(xn,yn)},xi∈Rd,yi∈{0,1}w^MLE=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))w^MAP=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;对于给定的数据集D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\},x_i\\in R^d,y_i\\in\\{0,1\\}\\\\ &amp;\\hat{w}_{MLE}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;\\hat{w}_{MAP}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​对于给定的数据集D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)},xi​∈Rd,yi​∈{0,1}w^MLE​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))w^MAP​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 由此我们得到两个估计过程的损失函数： l(w)MLE=∑i=1nlog⁡(1+e−yi(wTxi+b))l(w)MAP=∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;l(w)_{MLE}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;l(w)_{MAP}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​l(w)MLE​=i=1∑n​log(1+e−yi​(wTxi​+b))l(w)MAP​=i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 我们的目标是最小化一个凸的、连续的、可微的损失函数l(w)l(w)l(w)。 凸函数定义为： 对∀t∈[0,1],若f(x)满足:f(tx1+(1−t)x2)≤tf(x1)+(1−t)f(x2),则称f(x)为凸函数对\\forall t\\in[0,1],若f(x)满足:f(tx_1+(1-t)x_2)\\le tf(x_1)+(1-t)f(x_2),则称f(x)为凸函数 对∀t∈[0,1],若f(x)满足:f(tx1​+(1−t)x2​)≤tf(x1​)+(1−t)f(x2​),则称f(x)为凸函数 ①l(w)l(w)l(w)是凸函数：这使得我们找到的局部最小值也是全局最小值 ②l(w)l(w)l(w)至少三次连续可微：我们将广泛得使用泰勒近似，这个假设大大简化了讨论 ③在www上没有设置任何约束，添加约束会增加问题的复杂程度，我们在这里将不讨论 1.2 局部最小值 1.2.1 定义 定义：我们将w∗w^*w∗称为l(w)l(w)l(w)的一个局部最小值，如果有： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)≤l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)\\le l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)≤l(w) LPLPLP范数：∣∣x∣∣p=(∑i=1n∣xi∣p)1p||x||_p=(\\sum_{i=1}^n |x_i|^p)^{\\frac1p}∣∣x∣∣p​=(∑i=1n​∣xi​∣p)p1​ 因为我们之前有l(w)l(w)l(w)是凸函数的假设，所以找到的局部最小值w∗w^*w∗也是全局最小值，有： ∀w∈Rd,都有l(w∗)≤l(w)\\forall w\\in R^d,都有l(w^*)\\le l(w) ∀w∈Rd,都有l(w∗)≤l(w) 当满足如下条件时，我们称w∗w^*w∗为l(w)l(w)l(w)的一个严格的局部最小值： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)&lt;l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)&lt; l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)&lt;l(w) 注意不是所有的凸函数都有严格的局部最小值，比如l(w)=1l(w)=1l(w)=1 1.2.2 必要条件 一个点w∗w^*w∗成为局部最小值的必要条件为： ∇l(w∗)=0→\\nabla l(w^*)=\\overrightarrow0 ∇l(w∗)=0 ∇为梯度算子，∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]T\\nabla为梯度算子，\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T ∇为梯度算子，∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 如果点w∗w^*w∗满足上述必要条件，则该点成为严格的局部最小值的充分条件是HessianHessianHessian矩阵为正定的： 即：∇2l(w∗)=H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]是正定的。即：\\nabla^2l(w^*)=H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right]是正定的。 即：∇2l(w∗)=H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​是正定的。 正定矩阵的定义为： 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的设A是n阶方阵，即A\\in R^{n\\times n},如果对于任何非零向量X\\in R^n都有：X^TA~X&gt;0,那么A是正定的 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的 二、泰勒展开 为了理解更新过程中的损失函数变化，我们对损失函数进行泰勒展开： 2.1 一阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]Tl(w+\\Delta w)=l(w)+g(w)^T\\Delta w\\\\ g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 一阶泰勒展开对应于线性近似： 在Δw\\Delta wΔw很小时，这些近似是合理可靠的，线性近似的误差为： O(∣∣Δw∣∣22)=∑i=1dΔwi2O(||\\Delta w||_2^2)=\\sum_{i=1}^d\\Delta w_i^2 O(∣∣Δw∣∣22​)=i=1∑d​Δwi2​ 2.2 二阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔw+12ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j=∂2l(w)∂wi∂wjl(w+\\Delta w)=l(w)+g(w)^T\\Delta w+\\frac12\\Delta w^TH(w)\\Delta w\\\\ H(w)为Hessian矩阵，[H(w)]_{i,j}=\\frac{\\partial^2 l(w)}{\\partial w_i\\partial w_j} l(w+Δw)=l(w)+g(w)TΔw+21​ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j​=∂wi​∂wj​∂2l(w)​ 二阶泰勒展开对应于二次近似： 二次近似的误差为： O(∣∣Δw∣∣23)=(∑i=1nΔwi2)32O(||\\Delta w||_2^3)=(\\sum_{i=1}^n\\Delta w_i^2)^\\frac32 O(∣∣Δw∣∣23​)=(i=1∑n​Δwi2​)23​ 三、搜索方向方法 我们求解问题的目标是：寻找w∗w^*w∗使得l(w)l(w)l(w)取得最小值，我们需要研究寻找w∗w^*w∗的方向。 3.1 核心思想 给定一个初始点w0w^0w0，寻找一个迭代序列：w0→w1→...→wkw^0\\rightarrow w^1\\rightarrow...\\rightarrow w^kw0→w1→...→wk，wkw^kwk表示第kkk次迭代找到的www，我们希望：k→∞,wk→w∗k\\rightarrow\\infty,w^k\\rightarrow w^*k→∞,wk→w∗ 我们的目的就是寻找一个步长sss用于对www进行更新，更新过程： wk+1=wk+sw^{k+1}=w^k+s wk+1=wk+s 由此我们得到了一个梯度下降过程的伪代码： 根据上述算法，我们便需要解决以下两个问题： ①如何寻找一个合理的步长sss ②如何确定我们找到了w∗w^*w∗以跳出循环 3.2 梯度下降 我们需要寻找到一个使得函数梯度下降最快的方向，并朝该方向迈出一步。 考虑一阶泰勒展开： l(wk+s)=l(wk)+g(wk)Tsl(w^k+s)=l(w^k)+g(w^k)^Ts l(wk+s)=l(wk)+g(wk)Ts 那么下降最快的方向就是−g(wk)-g(w^k)−g(wk)，因此我们在梯度下降过程中所作的就是： s=−αg(wk),α&gt;0s=-\\alpha g(w^k),\\alpha&gt;0 s=−αg(wk),α&gt;0 正确性验证：我们需要验证的是总有一些足够小的ααα使得： l(wk−αg(wk))&lt;l(wk)l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) l(wk−αg(wk))&lt;l(wk) 根据泰勒展开： l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)\\begin{aligned} &amp;l\\big(w^k-\\alpha g(w^k)\\big)=l(w^k)-\\alpha g(w^k)^Tg(w^k)+O(\\alpha^2)\\\\ &amp;\\because g(w^k)^Tg(w^k)&gt;0,\\alpha\\rightarrow0 过程中\\alpha^2\\rightarrow0的速度大于\\alpha\\\\ &amp;\\therefore 存在足够小的\\alpha，使得：l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) \\end{aligned} ​l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)​ ααα的设定需要合理，太小的话会导致收敛过慢，太大的话会导致发散。 3.3 Adagrad算法 对于ααα的一个选择是：设置ααα使得步长适合于所有的特征，Adagrad算法通过保持每个优化变量的平方梯度的运行平均值来实现这一点 Adagrad算法为具有大梯度的变量设置小学习率，并为具有小梯度的特征设置大学习率。 Adagrad算法的伪代码如下： 该算法的特点是：每一个维度都有不同的学习率，第jjj维的学习率维： αzj+ϵ\\frac{\\alpha}{\\sqrt{z_j+\\epsilon}} zj​+ϵ​α​ 3.4 牛顿方法 牛顿方法处理梯度下降利用二阶泰勒展开： l(wk+s)=l(wk)+g(wk)Ts+12sTH(wk)sl(w^k+s)=l(w^k)+g(w^k)^Ts+\\frac12s^TH(w^k)s l(wk+s)=l(wk)+g(wk)Ts+21​sTH(wk)s H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​ 因为我们假设l(w)l(w)l(w)是凸函数，则H(w)H(w)H(w)对所有www都是半正定的，则有： sTH(wk)s≥0s^TH(w^k)s\\ge0 sTH(wk)s≥0 事实上，牛顿方法在严格的局部极小值附近有很好的性质，一旦足够接近一个解，它就会迅速收敛，为了便于分析，我们假设H(w)H(w)H(w)是正定的，即： sTH(wk)s&gt;0s^TH(w^k)s&gt;0 sTH(wk)s&gt;0 二次近似的梯度为： ∂l(wk+s)∂s=g(wk)+H(wk)s\\frac{\\partial l(w^k+s)}{\\partial s}=g(w^k)+H(w^k)s ∂s∂l(wk+s)​=g(wk)+H(wk)s 这意味着我们要找到的w∗w^*w∗满足： g(w∗)+H(w∗)s=0g(w^*)+H(w^*)s=0 g(w∗)+H(w∗)s=0 所以我们可以得到步长sss： s=H(wk)−1g(wk)s=H(w^k)^{-1}g(w^k) s=H(wk)−1g(wk) 3.4.1 Example 这有一个简单的例子清楚地说明了利用二阶泰勒展开的牛顿方法的优势： 假设损失函数实际上是严格的凸二次函数，即： l(w)=12wTAw+bTw+cl(w)=\\frac12w^TAw+b^Tw+c l(w)=21​wTAw+bTw+c 其中AAA为正定矩阵，bbb为任意向量，ccc为任意常数，在这种情况下，牛顿方法仅需一步即可收敛： l(w)在Aw+b=0时收敛l(w)在Aw+b=0时收敛 l(w)在Aw+b=0时收敛 我们不妨以二维向量为例： A=[a11 a12a21 a22],w=[w1,w2]T,b=[b1,b2]TA=\\left[ \\begin{aligned} &amp;a_{11}~~~~a_{12}\\\\ &amp;a_{21}~~~~a_{22}\\\\ \\end{aligned} \\right],w=[w_1,w_2]^T,b=[b_1,b_2]^T A=[​a11​ a12​a21​ a22​​],w=[w1​,w2​]T,b=[b1​,b2​]T 则损失函数则变为： l(w1,w2)=12(a11w12+(a12+a21)w1w2+a22w22)+(b1w1+b2w2)+cl(w_1,w_2)=\\frac12\\big(a_{11}w_1^2+(a_{12}+a_{21})w_1w_2+a_{22}w_2^2\\big)+(b_1w_1+b_2w_2)+c l(w1​,w2​)=21​(a11​w12​+(a12​+a21​)w1​w2​+a22​w22​)+(b1​w1​+b2​w2​)+c g(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2]=[a11w1+(a12+a21)w2+b1,a22w2+(a12+a21)w1+b2]\\begin{aligned} &amp;g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2}]=[a_{11}w_1+(a_{12}+a_{21})w_2+b_1,a_{22}w_2+(a_{12}+a_{21})w_1+b_2] \\end{aligned} ​g(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​]=[a11​w1​+(a12​+a21​)w2​+b1​,a22​w2​+(a12​+a21​)w1​+b2​]​ H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2∂2l(w)∂w2∂w1 ∂2l(w)∂w22]=[ a11 a12+a21a12+a21 a22]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_2\\partial w_1}~~~~\\frac{\\partial^2l(w)}{\\partial w_2^2} \\end{aligned} \\right]=\\left[ \\begin{aligned} &amp;~~~~a_{11}~~~~a_{12}+a_{21}\\\\ &amp;a_{12}+a_{21}~~~~a_{22} \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​∂w2​∂w1​∂2l(w)​ ∂w22​∂2l(w)​​⎦⎥⎥⎥⎤​=[​ a11​ a12​+a21​a12​+a21​ a22​​] H(w)−1=[a22a11a22−(a12+a21)2 −(a12+a21)a11a22−(a12+a21)2−(a12+a21)a11a22−(a12+a21)2 a11a11a22−(a12+a21)2]=β[ a22 −(a12+a21)−(a12+a21) a11],β=1a11a22−(a12+a21)2H(w)^{-1}=\\left[ \\begin{aligned} &amp;\\frac{a_{22}}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{a_{11}}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right]=\\beta\\left[ \\begin{aligned} &amp;~~~~a_{22}~~~~-(a_{12}+a_{21})\\\\ &amp;-(a_{12}+a_{21})~~~~a_{11} \\end{aligned} \\right],\\beta=\\frac1{a_{11}a_{22}-(a_{12}+a_{21})^2} H(w)−1=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2a22​​ a11​a22​−(a12​+a21​)2−(a12​+a21​)​a11​a22​−(a12​+a21​)2−(a12​+a21​)​ a11​a22​−(a12​+a21​)2a11​​​⎦⎥⎥⎥⎤​=β[​ a22​ −(a12​+a21​)−(a12​+a21​) a11​​],β=a11​a22​−(a12​+a21​)21​ s=−H(w)−1g(w)=−β[(a11a22−(a12+a21)2)w1+a22b1−(a12+a21)b2(a11a22−(a12+a21)2)w2+a11b2−(a12+a21)b1]=[−w1+(a12+a21)b2−a22b1a11a22−(a12+a21)2−w2+(a12+a21)b1−a11b2a11a22−(a12+a21)2]s=-H(w)^{-1}g(w)=-\\beta\\left[ \\begin{aligned} &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_1+a_{22}b_1-(a_{12}+a_{21})b_2\\\\ &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_2+a_{11}b_2-(a_{12}+a_{21})b_1 \\end{aligned} \\right]\\\\ =\\left[ \\begin{aligned} &amp;-w_1+\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;-w_2+\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] s=−H(w)−1g(w)=−β[​(a11​a22​−(a12​+a21​)2)w1​+a22​b1​−(a12​+a21​)b2​(a11​a22​−(a12​+a21​)2)w2​+a11​b2​−(a12​+a21​)b1​​]=⎣⎢⎢⎢⎡​​−w1​+a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​−w2​+a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 则更新www的过程为： w=w+s=[(a12+a21)b2−a22b1a11a22−(a12+a21)2(a12+a21)b1−a11b2a11a22−(a12+a21)2]w=w+s=\\left[ \\begin{aligned} &amp;\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] w=w+s=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 为了便于讨论，我们不妨使得A=IA=IA=I，则有w=[−b1,−b2]T=−bw=[-b_1,-b_2]^T=-bw=[−b1​,−b2​]T=−b，则有Aw+b=0Aw+b=0Aw+b=0，仅通过一次更新即实现了收敛。 注意： ①H(w)H(w)H(w)是一个d×dd\\times dd×d的矩阵，它的构造代价很大，一个很好的近似方法是只计算其对角线条目 ②本质上这是梯度下降法和牛顿方法的结合，为了避免牛顿法的发散，一个好的方法是从梯度下降（甚至随机梯度下降）开始，然后完成优化牛顿法。通常，牛顿法使用的二阶近似值更可能接近最佳值 ","link":"https://2006wzt.github.io/post/机器学习实战（八）：梯度下降/"},{"title":"机器学习实战（七）：逻辑回归","content":"逻辑回归 一、基本思想 逻辑回归是一种用于分类任务的经典机器学习算法。 我们之前介绍过机器学习算法大致可以分为两类： ①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)\\begin{aligned} &amp;①生成学习：估计P(x,y)=P(x|y)P(y)\\\\ &amp;②判别学习：估计P(y|x) \\end{aligned} ​①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)​ 上节学习的朴素贝叶斯属于生成学习算法，而逻辑回归则属于判别学习算法，它与高斯朴素贝叶斯相对应，即判别式的高斯朴素贝叶斯。 逻辑回归的核心函数为：sigmoidsigmoidsigmoid函数，也成为激活函数： sigmoid(x)=σ(x)=11+e−xsigmoid(x)=\\sigma(x)=\\frac{1}{1+e^{-x}} sigmoid(x)=σ(x)=1+e−x1​ 我们对逻辑回归模型的建模如下： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 与感知机原理相同，我们利用升维将偏差项bbb处理到www中，则模型简化为： P(y∣x)=11+e−y(wTx)P(y|x)=\\frac{1}{1+e^{-y(w^Tx)}} P(y∣x)=1+e−y(wTx)1​ 二、逻辑回归的参数 在逻辑回归模型中，重要的参数也为www，因此我们需要用到之前学过的几个概率估计方法对其进行估计。 2.1 极大似然估计（MLE） 利用极大似然估计逻辑回归的参数：在MLEMLEMLE中，要极大化的条件数据为P(Y∣X,w)P(Y|X,w)P(Y∣X,w)，X,YX,YX,Y为训练数据，我们要找到一个合适的www使得特征向量集为XXX时，观察到标签YYY的概率最大 X：d×n维矩阵，即X=[x1→,x2→,...,xn→]∈Rd×nY：n维向量，即Y=[y1,y2,...,yn]\\begin{aligned} &amp;X：d\\times n维矩阵，即X=[\\overrightarrow {x_1},\\overrightarrow{x_2},...,\\overrightarrow{x_n}]\\in R^{d\\times n}\\\\ &amp;Y：n维向量，即Y=[y_1,y_2,...,y_n] \\end{aligned} ​X：d×n维矩阵，即X=[x1​​,x2​​,...,xn​​]∈Rd×nY：n维向量，即Y=[y1​,y2​,...,yn​]​ MLEMLEMLE的假设为： P(Y∣X,w)=∏i=1nP(yi∣xi,w)P(Y|X,w)=\\prod_{i=1}^nP(y_i|x_i,w) P(Y∣X,w)=i=1∏n​P(yi​∣xi​,w) 由此，我们对参数www进行的极大似然估计过程为： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡11+e−yi(wTxi) =argmaxw −log⁡(1+e−yi(wTxi))=argminw log⁡(1+e−yi(wTxi))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log \\frac{1}{1+e^{-y_i(w^Tx_i)}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\log(1+e^{-y_i(w^Tx_i)})=\\underset{w}{argmin}~\\log(1+e^{-y_i(w^Tx_i)}) \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log1+e−yi​(wTxi​)1​ =wargmax​ −log(1+e−yi​(wTxi​))=wargmin​ log(1+e−yi​(wTxi​))​ 为了求解www，我们引入函数l(w)l(w)l(w)：在负数域上求解−l(w)-l(w)−l(w)的最大值： l(w)=∑i=1nlog⁡(1+e−yi(wTxi))l(w)=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i)}) l(w)=i=1∑n​log(1+e−yi​(wTxi​)) 我们要寻找max⁡{∣−l(w)∣}\\max\\{|-l(w)|\\}max{∣−l(w)∣}所对应的www。 2.1.1 1-D Example 我们考虑一个1维问题，如下图所示：+++表示正类，ooo表示负类： 如上图所示，x0x_0x0​恒为1，数据点特征的不同仅为x1x_1x1​的不同，本质上是一个1维的问题： x=[x0,x1],w=[w0,w1],l(w)=l([w0,w1])\\begin{aligned} &amp;x=[x_0,x_1],w=[w_0,w_1],l(w)=l([w_0,w_1]) \\end{aligned} ​x=[x0​,x1​],w=[w0​,w1​],l(w)=l([w0​,w1​])​ 右图为l(w)l(w)l(w)所对应的曲面，我们可以确定：w0=1,w1=0.7w_0=1,w_1=0.7w0​=1,w1​=0.7时，∣−l(w)∣|-l(w)|∣−l(w)∣取得最大值，因此我们计算出w=[1,0.7]w=[1,0.7]w=[1,0.7] 我们也可以用热力图更直观得得到www的最佳取值： 以下是部分样例对上述图像的贡献图： 2.1.2 2-D Example 我们考虑一个2维问题，如下图所示：+++表示正类，ooo表示负类： 根据热力图，我们得到∣−l(w)∣|-l(w)|∣−l(w)∣在w=[−0.81.0.81]w=[-0.81.0.81]w=[−0.81.0.81]时取得最大值。 MLEMLEMLE计算结果如下所示，其中红色表示正类别的概率很高。黑线表示MLEMLEMLE学习的决策边界。 决策边界：P(y=−1∣x)=P(y=+1∣x)即：11+ewTx=11+e−wTx→wTx=−wTx→wTx=0w=[w1,w2]=[−0.81,0.81],x=[x1,x2]wTx=w1⋅x1+w2⋅x2=−0.81x1+0.81x2=0→x1=x2\\begin{aligned} &amp;决策边界：P(y=-1|x)=P(y=+1|x)\\\\ &amp;即：\\frac{1}{1+e^{w^Tx}}=\\frac{1}{1+e^{-w^Tx}}\\rightarrow w^Tx=-w^Tx\\rightarrow w^Tx=0\\\\ &amp;w=[w_1,w_2]=[-0.81,0.81],x=[x_1,x_2]\\\\ &amp;w^Tx=w_1\\cdot x_1+w_2\\cdot x_2=-0.81x_1+0.81x_2=0\\rightarrow x_1=x_2 \\end{aligned} ​决策边界：P(y=−1∣x)=P(y=+1∣x)即：1+ewTx1​=1+e−wTx1​→wTx=−wTx→wTx=0w=[w1​,w2​]=[−0.81,0.81],x=[x1​,x2​]wTx=w1​⋅x1​+w2​⋅x2​=−0.81x1​+0.81x2​=0→x1​=x2​​ 2.2 极大后验估计（MAP） 在极大后验估计中，我们将www视为一个随机变量，并且可以预先指定它的先验分布。我们先预先指定www的先验分布： w∼N(0,σ2)P(w)=12πσ2e−wTw2σ2w\\sim \\mathcal{N}(0,\\sigma^2)\\\\ P(w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{w^Tw}{2\\sigma^2}} w∼N(0,σ2)P(w)=2πσ2​1​e−2σ2wTw​ 这是逻辑回归的高斯近似。 以下计算过程中我们要用到链式法则： P(A,B∣C)=P(A∣B,C)P(B∣C)P(A,B|C)=P(A|B,C)P(B|C) P(A,B∣C)=P(A∣B,C)P(B∣C) 同时需要注意XXX与www是无关的。 我们在MAPMAPMAP中的目标是找到给定数据的最可能的模型参数，即使后验值最大化的参数： w^MAP=argmaxw P(w∣D)=argmaxw P(D∣w)P(w)P(D) =argmaxw P(D∣w)P(w)=argmaxw P(Y,X∣w)P(w) =argmaxw P(Y∣X,w)P(X∣w)P(w)=argmaxw P(Y∣X,w)P(w) =argmaxw log⁡P(Y∣X,w)+log⁡P(w) =argmaxw (−∑i=1nlog⁡(1+e−yi(wTxi))+log⁡12πσ2−12σ2wTw) =argminw (∑i=1nlog⁡(1+e−yi(wTxi))+λwTw)\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|D)=\\underset{w}{argmax}~\\frac{P(D|w)P(w)}{P(D)}\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(D|w)P(w)=\\underset{w}{argmax}~P(Y,X|w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(Y|X,w)P(X|w)P(w)=\\underset{w}{argmax}~P(Y|X,w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\log P(Y|X,w)+\\log P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\big(-\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}-\\frac1{2\\sigma^2}w^Tw\\big)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmin}~\\big(\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\lambda w^Tw\\big) \\end{aligned} ​w^MAP​=wargmax​ P(w∣D)=wargmax​ P(D)P(D∣w)P(w)​ =wargmax​ P(D∣w)P(w)=wargmax​ P(Y,X∣w)P(w) =wargmax​ P(Y∣X,w)P(X∣w)P(w)=wargmax​ P(Y∣X,w)P(w) =wargmax​ logP(Y∣X,w)+logP(w) =wargmax​ (−i=1∑n​log(1+e−yi​(wTxi​))+log2πσ2​1​−2σ21​wTw) =wargmin​ (i=1∑n​log(1+e−yi​(wTxi​))+λwTw)​ 其中：λ=12σ2\\lambda=\\frac1{2\\sigma^2}λ=2σ21​，与MLEMLEMLE同理，我们引入函数l(w)l(w)l(w)，在负数域上求解∣−l(w)∣|-l(w)|∣−l(w)∣最大值以得到www： l(w)=∑i=1nlog⁡(1+e−y1(wTxi))+λwTwl(w)=\\sum_{i=1}^n\\log(1+e^{-y_1(w^Tx_i)})+\\lambda w^Tw l(w)=i=1∑n​log(1+e−y1​(wTxi​))+λwTw 三、更新参数 除了上述利用函数求极值的方法求解参数www的方法，我们也可以用梯度下降的方式更新www 3.1 损失函数 根据模型预测的概率为： P(y=1∣x)=h(x)=σ(wTx)=11+e−(wTx) (1)P(y=0∣x)=1−h(x) (2)\\begin{aligned} &amp;P(y=1|x)=h(x)=\\sigma(w^Tx)=\\frac{1}{1+e^{-(w^Tx)}}~~~~~~~~(1)\\\\ &amp;P(y=0|x)=1-h(x)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(2) \\end{aligned} ​P(y=1∣x)=h(x)=σ(wTx)=1+e−(wTx)1​ (1)P(y=0∣x)=1−h(x) (2)​ 合并(1)(1)(1)式(2)(2)(2)式得： P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}\\begin{aligned} &amp;P(y|x,w)=h(x)^y(1-h(x))^{1-y},y\\in\\{0,1\\} \\end{aligned} ​P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}​ 我们进行极大似然估计可得： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∏i=1nh(xi)yi(1−h(xi))1−yi=argmaxw ∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi)))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nh(x_i)^{y_i}(1-h(x_i))^{1-y_i}=\\underset{w}{argmax}~\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i))\\big)\\\\ \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∏n​h(xi​)yi​(1−h(xi​))1−yi​=wargmax​ i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)))​ 由此我们可以定义损失函数： L(w)=−∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi))L(w)=-\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i)\\big) L(w)=−i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)) 我们希望损失函数最小化以得到最优的参数www，与感知机的更新过程相同，我们求导以得到更新项。 3.2 梯度下降 梯度下降的过程为遍历数据集，利用每个样本对参数www进行更新：令z=wTxz=w^Txz=wTx 每一个样本所对应的损失函数为： l(w)=−yln⁡h(x)−(1−y)ln⁡(1−h(x))l(w)=-y\\ln h(x)-(1-y)\\ln(1-h(x)) l(w)=−ylnh(x)−(1−y)ln(1−h(x)) 对www进行求导，求出更新项： ∂l(w)∂w=∂l(w)∂h(x)⋅∂h(x)∂z⋅∂z∂w,h(x)=11+e−(wTx)①∂l(w)∂h(x)=−yh(x)+1−y1−h(x)=h(x)−yh(x)(1−h(x))②∂h(x)∂z=e−x(1+e−x)=h(x)(1−h(x)) ③∂z∂w=x因此：∂l(w)∂w=h(x)−yh(x)(1−h(x))⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=\\frac{\\partial l(w)}{\\partial h(x)}\\cdot \\frac{\\partial h(x)}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w},h(x)=\\frac{1}{1+e^{-(w^Tx)}}\\\\ &amp;①\\frac{\\partial l(w)}{\\partial h(x)}=-\\frac{y}{h(x)}+\\frac{1-y}{1-h(x)}=\\frac{h(x)-y}{h(x)(1-h(x))}\\\\ &amp;②\\frac{\\partial h(x)}{\\partial z}=\\frac{e^{-x}}{(1+e^{-x})}=h(x)(1-h(x))~~③\\frac{\\partial z}{\\partial w}=x\\\\ &amp;因此：\\frac{\\partial l(w)}{\\partial w}=\\frac{h(x)-y}{h(x)(1-h(x))}\\cdot h(x)(1-h(x))\\cdot x=(h(x)-y)\\cdot x \\end{aligned} ​∂w∂l(w)​=∂h(x)∂l(w)​⋅∂z∂h(x)​⋅∂w∂z​,h(x)=1+e−(wTx)1​①∂h(x)∂l(w)​=−h(x)y​+1−h(x)1−y​=h(x)(1−h(x))h(x)−y​②∂z∂h(x)​=(1+e−x)e−x​=h(x)(1−h(x)) ③∂w∂z​=x因此：∂w∂l(w)​=h(x)(1−h(x))h(x)−y​⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x​ 则梯度下降的更新过程为： w→w−η∑i=1n(h(xi)−yi)⋅x=w−η∑i=1n(σ(wTxi)−yi)⋅xiw\\rightarrow w-\\eta\\sum_{i=1}^n(h(x_i)-y_i)\\cdot x=w-\\eta\\sum_{i=1}^n(\\sigma(w^Tx_i)-y_i)\\cdot x_i w→w−ηi=1∑n​(h(xi​)−yi​)⋅x=w−ηi=1∑n​(σ(wTxi​)−yi​)⋅xi​ 四、模型实现 我们将通过手动实现与调库的方式去构造逻辑回归模型。 4.1 数据集 本次我们仍使用乳腺癌数据集，进行逻辑回归二分类器的实现，数据集详细内容可通过如下代码查看，不再过多赘述。 4.2 手动实现模型 我们利用梯度下降的方式对模型进行更新： 4.3 调库实现模型 调库用到的函数为sklearn所提供的LogisticRegression函数，其函数原型如下： 可以发现该模型并不是用梯度下降的方式去更新模型的，而是通过参数的估计，要用到优化求解器，其相关参数如下表所示： 模型参数 Parameter含义 备注 penalty 正则化项 广义线性模型的正则项，可选值包括L1正则项'l1'、L2正则项'l2'、复合正则'elasticnet'和无正则项None，默认值为'l2'。值得注意的是，正则项的选择应与优化求解器相匹配(详见solver参数)。 l1_ratio 正则权重系数 L1正则和L2正则的权重系数，取值空间为0-1。若为0，则相当于为L2正则；若为1，则为L1正则，否则为Elasticnet正则。 dual 对偶问题 默认值False，可设为True将问题转换为对偶问题（详见本博客SVM问题中原问题-对偶问题的推导），仅适用于采用L2正则化且求解器为'liblinear’的情况。当样本数少于特征数时，推荐为True。 tol 迭代阈值 求解器迭代求解时，停止迭代的目标函数改变阈值。 C 正则化系数倒数 注意，其值与正则化强度相反，即C值越小，正则化程度越大。其值必须为正，且默认值为1 fit_intercept 是否预设偏置 控制广义线性模型中是否预设偏置值 intercept_scaling 预设偏置值 广义线性模型中预设的偏置值，仅当求解器为'liblinear'同时fit_intercept时生效。注意：该偏置值会作为新的特征计算其系数，因此也会计入L1和L2正则。 class_weight 样本权重 用于处理样本不均衡问题。默认值为None，即各类别样本权重一样，可通过设置字典定义权重系数，或设为'balanced'，即根据样本数自动计算权重。若在fit函数中设置sample_weight参数，两者作用会叠加。 random_state 随机状态 LR模型中的随机性主要体现在求解器迭代时对样本的随机选取，适用于当求解器为'liblinear'或'sag'时。 solver 求解器 sklearn中共提供了5种优化求解器，分别为'liblinear'、'sag'、'saga'、'newton-cg'和'lbfgs'。各求解器的适用条件不同，具体见后文。默认值为'liblinear'。 max_iter 最大迭代步数 ‘newton-cg’、'sag'和'lbfgs' 求解器所需要的最大迭代步数 multi_class 多分类策略 取值可为'ovr'、'multinomial'和'auto'。'ovr'即采用'one vs rest'策略对二分类模型进行集成；'multinomial'即采用'multinomial loss'直接求解多分类问题。默认为'auto'，其会在两分类问题或求解器为'liblinear'时选择'ovr'，而其它情况下选择'multinomial'。默认值为'auto' 在此我们不对参数作过多解释，具体调参过程就问题而论。 ","link":"https://2006wzt.github.io/post/机器学习实战（七）：逻辑回归/"},{"title":"机器学习实战（六）：朴素贝叶斯","content":"朴素贝叶斯 一、基本思想 在机器学习中，朴素贝叶斯分类器是在强独立假设下基于贝叶斯定理的一系列简单概率分类器，强独立假设为： 对于训练数据：D={(x1,y1),(x2,y2),...,(xn,yn)}\\begin{aligned} &amp;对于训练数据：D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\} \\end{aligned} ​对于训练数据：D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)}​ 我们假设它是从未知分布中抽取的独立同分布，则有： P(D)=P((x1,y1),(x2,y2),...,(xn,yn))=∏α=1nP(xα,yα)P(D)=P((x_1,y_1),(x_2,y_2),...,(x_n,y_n))=\\prod_{\\alpha=1}^nP(x_\\alpha,y_\\alpha) P(D)=P((x1​,y1​),(x2​,y2​),...,(xn​,yn​))=α=1∏n​P(xα​,yα​) 如果我们有足够的数据，我们可以估算P(X,Y)P(X,Y)P(X,Y)，类似于上一节的硬币例子，我们想象一个巨大的骰子，每个可能的(X,Y)(X,Y)(X,Y)值都有一面。我们可以通过计数来估计某一特定方面出现的概率。 P^(x,y)=∑i=1nI(xi=x∩yi=y)nxi=x且yi=y时，I(xi=x∩yi=y)=1，否则为0\\hat{P}(x,y)=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{n}\\\\ x_i=x且y_i=y时，I(x_i=x\\cap y_i=y)=1，否则为0 P^(x,y)=n∑i=1n​I(xi​=x∩yi​=y)​xi​=x且yi​=y时，I(xi​=x∩yi​=y)=1，否则为0 当然，如果我们主要感兴趣的是从特征xxx预测标签yyy，我们可以直接估计P(y∣x)P(y | x)P(y∣x)，而不是P(x，y)P(x，y)P(x，y)。我们可以使用贝叶斯最优分类器对P(y∣x)P(y | x)P(y∣x)进行预测： P^(y∣x)=P^(x∣y)P^(y)P(x)=P^(x,y)P(x)=∑i=1nI(xi=x∩yi=y)/n∑i=1nI(xi=x)/n=∑i=1nI(xi=x∩yi=y)∑i=1nI(xi=x)\\hat{P}(y|x)=\\frac{\\hat{P}(x|y)\\hat{P}(y)}{P(x)}=\\frac{\\hat{P}(x,y)}{P(x)}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)/n}{\\sum_{i=1}^nI(x_i=x)/n}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{\\sum_{i=1}^nI(x_i=x)} P^(y∣x)=P(x)P^(x∣y)P^(y)​=P(x)P^(x,y)​=∑i=1n​I(xi​=x)/n∑i=1n​I(xi​=x∩yi​=y)/n​=∑i=1n​I(xi​=x)∑i=1n​I(xi​=x∩yi​=y)​ 如上图韦恩图所示，我们最终的预测结果可以表示为： P^(y∣x)=∣C∣∣B∣\\hat{P}(y|x)=\\frac{|C|}{|B|} P^(y∣x)=∣B∣∣C∣​ 二、朴素贝叶斯算法 2.1 贝叶斯规则 朴素贝叶斯算法的核心是贝叶斯公式： P(y∣x)=P(x∣y)P(y)P(x)P(y|x)=\\frac{P(x|y)P(y)}{P(x)} P(y∣x)=P(x)P(x∣y)P(y)​ 根据上述贝叶斯公式我们可以知道：如果我们可以预测P(x∣y)P(x|y)P(x∣y)和P(y)P(y)P(y)，那么我们即可预测P(y∣x)P(y|x)P(y∣x) ①预测P(y)P(y)P(y)很容易，假如yyy取离散的值，我们只需要记录观察到结果为yyy的次数即可： P^(y=c)=∑i=1nI(yi=c)n=π^c\\hat{P}(y=c)=\\frac{\\sum_{i=1}^nI(y_i=c)}{n}=\\hat{\\pi}_c P^(y=c)=n∑i=1n​I(yi​=c)​=π^c​ ②但是预测P(x∣y)P(x|y)P(x∣y)并不容易，因此我们需要引入朴素贝叶斯假设。 2.2 朴素贝叶斯假设 朴素贝叶斯假设为： P(x∣y)=∏α=1dP(xα∣y)x=[x1,x2,...,xd],xα是d维特征向量x在第α维度上的值P(x|y)=\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\\\ x=[x_1,x_2,...,x_d],x_\\alpha是d维特征向量x在第\\alpha维度上的值 P(x∣y)=α=1∏d​P(xα​∣y)x=[x1​,x2​,...,xd​],xα​是d维特征向量x在第α维度上的值 即：对于给定的标签，其特征值是独立的，这是一个非常大胆的假设 经常使用朴素贝叶斯分类器的一个例子是垃圾邮件的过滤，此处数据为电子邮件，标签为是垃圾邮件还是非垃圾邮件； 朴素贝叶斯假设意味着电子邮件中的单词在条件上是独立的，因为我们知道电子邮件是否是垃圾邮件，显然这不是真的。无论是垃圾邮件还是非垃圾邮件都不是独立随机抽取的。然而，即使违反了这一假设，由此产生的分类器在实践中也可以很好地工作。 由此我们可以对P(x∣y)P(x|y)P(x∣y)进行估计：假设朴素贝叶斯假设成立，则贝叶斯分类器定义如下： h(x)=argmaxy P(y∣x)=argmaxy P(x∣y)P(y)P(x)=argmaxy P(x∣y)P(y) =argmaxy (∏α=1dP(xα∣y))P(y)=argmaxy (∑α=1dlog⁡P(xα∣y))+log⁡P(y)\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~P(y|x)=\\underset{y}{argmax}~\\frac{P(x|y)P(y)}{P(x)}=\\underset{y}{argmax}~P(x|y)P(y)\\\\ &amp;~~~~~~~~~=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\big)P(y)=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log P(x_\\alpha|y)\\big)+\\log P(y) \\end{aligned} ​h(x)=yargmax​ P(y∣x)=yargmax​ P(x)P(x∣y)P(y)​=yargmax​ P(x∣y)P(y) =yargmax​ (α=1∏d​P(xα​∣y))P(y)=yargmax​ (α=1∑d​logP(xα​∣y))+logP(y)​ 估计P(xα∣y)P(x_\\alpha|y)P(xα​∣y)和P(y)P(y)P(y)都很容易，因此我们则可实现贝叶斯分类器。 三、估算P(xα∣y)P(x_\\alpha|y)P(xα​∣y) 3.1 Case#1：分类特征 分类特征：对于ddd维特征向量xxx，它的第α\\alphaα维特征xαx_\\alphaxα​有KαK_\\alphaKα​个取值，即分类问题的特征 例如：年龄、性别、省份等特征，特们都有固定个数的KαK_\\alphaKα​个取值，xα∈{f1,f2,...,fKα}x_\\alpha\\in\\{f_1,f_2,...,f_{K_\\alpha} \\}xα​∈{f1​,f2​,...,fKα​​} P(xα=fj∣y=c)=[θjc]α,∑j=1Kαθjc=1P(x_\\alpha=f_j|y=c)=[\\theta_{jc}]_\\alpha,\\sum_{j=1}^{K_\\alpha}\\theta_{jc}=1 P(xα​=fj​∣y=c)=[θjc​]α​,j=1∑Kα​​θjc​=1 [θjc]α[θ_{jc}]_α[θjc​]α​是特征ααα在假设标签是ccc时，具有值fjf_jfj​的概率。约束表明xαx_αxα​必须具有一个类别{1，…，Kα}\\{1，…，K_α\\}{1，…，Kα​} 下面我们对[θjc]α[\\theta_{jc}]_\\alpha[θjc​]α​进行估计： [θjc]^α=∑i=1nI(xi=fj∩yi=c)+l∑i=1nI(yi=c)+lKα\\hat{[\\theta_{jc}]}_\\alpha=\\frac{\\sum_{i=1}^nI(x_i=f_j\\cap y_i=c)+l}{\\sum_{i=1}^nI(y_i=c)+lK_\\alpha} [θjc​]^​α​=∑i=1n​I(yi​=c)+lKα​∑i=1n​I(xi​=fj​∩yi​=c)+l​ l ~l~ l 是一个平滑参数： ①l=0l=0l=0时，我们得到MLEMLEMLE估计量，l&gt;0l&gt;0l&gt;0时，我们得到MAPMAPMAP估计量 ②l=1l=1l=1时，我们得到拉普拉斯平滑 最终我们得到的模型为： h(x)=argmaxy (∏α=1d[θjc]^α)π^c=argmaxy (∑α=1dlog⁡[θjc]α^)+log⁡π^c\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d\\hat{[\\theta_{jc}]}_\\alpha\\big)\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log\\hat{[\\theta_{jc}]_\\alpha}\\big)+\\log\\hat\\pi_c \\end{aligned} ​h(x)=yargmax​ (α=1∏d​[θjc​]^​α​)π^c​=yargmax​ (α=1∑d​log[θjc​]α​^​)+logπ^c​​ 3.2 Case#2：多项式特征 多项式特征：特征值不是诸如男女之类的分类特征，而是计数值，即回归问题的特征，但是计数值是有限的 例如：垃圾邮件过滤的例子中，各个特征是不同的单词，维度ddd即为单词表的大小，特征值是一个单词出现的次数，比如某个单词ααα出现十次，即xαx_αxα​=10，可能意味着该邮件为垃圾邮件。 xα∈{0,1,2,...,m},m=∑α=1dxαx_\\alpha\\in\\{0,1,2,...,m\\},m=\\sum_{\\alpha=1}^dx_\\alpha xα​∈{0,1,2,...,m},m=α=1∑d​xα​ 新的估计方式如下：P(x∣m,y=c)P(x|m,y=c)P(x∣m,y=c)表示标签y=cy=cy=c时，一个文本长度为mmm的邮件中特征向量为xxx的概率 P^(x∣m,y=c)=m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θαc)xα\\hat{P}(x|m,y=c)=\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha} P^(x∣m,y=c)=x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θαc​)xα​ θαc\\theta_{\\alpha c}θαc​是在标签y=cy=cy=c时，选中特征向量中的xαx_\\alphaxα​的概率，有∑α=1dθαc=1\\sum_{\\alpha=1}^d\\theta_{\\alpha c}=1∑α=1d​θαc​=1，以垃圾邮件为例，θαc\\theta_{\\alpha c}θαc​即为单词xαx_\\alphaxα​在文本中所占的比例。 下面我们对θαc\\theta_{\\alpha c}θαc​进行估计： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ xiαx_{i\\alpha}xiα​是第iii个电子邮件的特征向量的第α\\alphaα维度的值，mi=∑α=1dxiαm_i=\\sum_{\\alpha=1}^dx_{i\\alpha}mi​=∑α=1d​xiα​即第iii个电子邮件中的文本总数 最终我们得到的模型为： h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 3.3 Case#3：连续特征 连续特征：计数值是连续的值，例如身高体重，连续特征所对应的朴素贝叶斯也称为高斯朴素贝叶斯 xα∈Rx_\\alpha\\in R xα​∈R 新的估计方式如下： P^(xα∣y=c)=N(μαc,σαc2)=12πσαce−12(xα−μαcσαc)2\\hat{P}(x_\\alpha|y=c)=\\mathcal{N}(\\mu_{\\alpha c},\\sigma_{\\alpha c}^2)=\\frac1{\\sqrt{2\\pi}\\sigma_{\\alpha c}}e^{-\\frac12(\\frac{x_\\alpha-\\mu_{\\alpha c}}{\\sigma_{\\alpha c}})^2} P^(xα​∣y=c)=N(μαc​,σαc2​)=2π​σαc​1​e−21​(σαc​xα​−μαc​​)2 注意，上面指定的模型基于我们对数据的假设，即每个特征α来自一类条件高斯分布。 对参数进行估计： μ^αc=1nc∑i=1nI(yi=c)xiασ^αc=1nc∑i=1nI(yi=c)(xiα−μαc)2nc=∑i=1nI(yi=c)\\hat\\mu_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}\\\\\\hat\\sigma_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)(x_{i\\alpha}-\\mu_{\\alpha c})^2\\\\ n_c=\\sum_{i=1}^nI(y_i=c) μ^​αc​=nc​1​i=1∑n​I(yi​=c)xiα​σ^αc​=nc​1​i=1∑n​I(yi​=c)(xiα​−μαc​)2nc​=i=1∑n​I(yi​=c) 高斯朴素贝叶斯分类器本质上为逻辑回归模型： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 四、朴素贝叶斯分类器 朴素贝叶斯分类器为一个线性分类器： 对于一个多项式特征的数据集，其进行分类的过程如下：假设yi∈{−1,+1}y_i\\in\\{-1,+1\\}yi​∈{−1,+1}且特征都是多项式特征，有： h(x)=argmaxc P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x∣y=+1)P(y=+1)P(x)&gt;P(x∣y=−1)P(y=−1)P(x)即：P(y=+1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(+1))xα&gt;P(y=−1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(−1))xα即：log⁡P(y=+1)+∑α=1dxαlog⁡θα(+1)&gt;log⁡P(y=−1)+∑α=1dxαlog⁡θα(−1)即：[log⁡P(y=+1)−log⁡P(y=−1)]+∑α=1dxα(log⁡θα(+1)−log⁡θα(−1))&gt;0\\begin{aligned} &amp;h(x)=\\underset{c}{argmax}~P(y=c|x)\\\\ &amp;设h(x)=+1,则P(y=+1|x)&gt;P(y=-1|x)\\\\ &amp;根据贝叶斯公式，则有：\\frac{P(x|y=+1)P(y=+1)}{P(x)}&gt;\\frac{P(x|y=-1)P(y=-1)}{P(x)}\\\\ &amp;即：P(y=+1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (+1)})^{x_\\alpha}&gt;P(y=-1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (-1)})^{x_\\alpha}\\\\ &amp;即：\\log P(y=+1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(+1)}&gt;\\log P(y=-1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(-1)}\\\\ &amp;即：\\big[\\log P(y=+1)-\\log P(y=-1)\\big]+\\sum_{\\alpha=1}^dx_\\alpha(\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)})&gt;0 \\end{aligned} ​h(x)=cargmax​ P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x)P(x∣y=+1)P(y=+1)​&gt;P(x)P(x∣y=−1)P(y=−1)​即：P(y=+1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(+1)​)xα​&gt;P(y=−1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(−1)​)xα​即：logP(y=+1)+α=1∑d​xα​logθα(+1)​&gt;logP(y=−1)+α=1∑d​xα​logθα(−1)​即：[logP(y=+1)−logP(y=−1)]+α=1∑d​xα​(logθα(+1)​−logθα(−1)​)&gt;0​ 我们可以发现上述正确分类的形式与感知机很像，我们可以进行如下变换： 令：b=log⁡P(y=+1)−log⁡P(y=−1),wα=log⁡θα(+1)−log⁡θα(−1)则：h(x)=+1↔wTx+b&gt;0\\begin{aligned} &amp;令：b=\\log P(y=+1)-\\log P(y=-1),w_\\alpha=\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)}\\\\ &amp;则：h(x)=+1\\leftrightarrow w^Tx+b&gt;0 \\end{aligned} ​令：b=logP(y=+1)−logP(y=−1),wα​=logθα(+1)​−logθα(−1)​则：h(x)=+1↔wTx+b&gt;0​ 与感知机的形式完全相同，最终我们将朴素贝叶斯分类器等效为了感知机。 五、算法实现 我们将通过手动实现与调库的方式去构造朴素贝叶斯模型。 5.1 数据集 本次我们使用的数据集为垃圾邮件数据集，其下载链接为：UCI Machine Learning Repository: SMS Spam Collection Data Set 下载后的SMSSpamCollection中的内容如下图所示，显然我们需要对文件进行处理整合，数据处理过程如下： 上述数据集预处理过程中有几点需要注意： ①nltk报错问题，不妨尝试运行以下两部分代码完善nltk库的安装 ②TF-IDF特征矩阵：TF-IDF是Term Frequency - Inverse Document Frequency的缩写，即“词频——逆文本频率”。它由两部分组成，TF和IDF，也就是这两部分的乘积。TF指的就是常用的词频，即某个单词在当前文本中出现的频率。IDF，即“逆文本频率”，反映了一个单词在当前文本中的重要性 TF(t,D)=单词t在文本D中出现的次数文本D的总单词数IDF(t)=log⁡语料库文本总数+1包含词语t的文本总数+1TF−IDF(t,D)=TF(t,D)⋅IDF(t)\\begin{aligned} &amp;TF(t,D)=\\frac{单词t在文本D中出现的次数}{文本D的总单词数}\\\\ &amp;IDF(t)=\\log\\frac{语料库文本总数+1}{包含词语t的文本总数+1}\\\\ &amp;TF-IDF(t,D)=TF(t,D)\\cdot IDF(t) \\end{aligned} ​TF(t,D)=文本D的总单词数单词t在文本D中出现的次数​IDF(t)=log包含词语t的文本总数+1语料库文本总数+1​TF−IDF(t,D)=TF(t,D)⋅IDF(t)​ 我们处理后得到的 vectorizer.fit_transform(X) 输出的特征矩阵形式为(A,B) C(A,B)~C(A,B) C，AAA为文件索引，BBB为特定词的向量索引，CCC为文件AAA中单词BBB的TF−IDFTF-IDFTF−IDF分数 5.2 手动实现模型 我们选择多项式特征的数据集，即在上述数据处理的基础上将文本转为统计单词数量的矩阵，数据预处理与存储过程如下： 参考多项式特征的模型，我们可以构造我们的训练代码： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 5.3 调库实现模型 调库实现模型时，我们可以使用另一种方法，即上面提到过的TF-IDF特征矩阵，即连续特征的数据集，数据预处理与存储过程如下： 我们直接利用sklearn所提供的朴素贝叶斯分类器，观察分类情况，可以发现调库的准确率达到了0.9596412556053812，也较为准确。 ","link":"https://2006wzt.github.io/post/机器学习实战（六）：朴素贝叶斯/"},{"title":"机器学习实战（五）：概率估计","content":"概率估计 一、分布预测分类 我们在KNNKNNKNN算法中已经学习过了贝叶斯最优分类器： 如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx=arg⁡max⁡yP(y∣x)\\begin{aligned} &amp;如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签~label_x=\\arg\\max_{y}P(y|x) \\end{aligned} ​如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx​=argymax​P(y∣x)​ 如果我们可以根据训练集得到一个大致的分布P(X,Y)，则可以利用贝叶斯最优分类器进行分类，对分布进行预测的学习分为两类： ①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)\\begin{aligned} &amp;①生成学习：预测P(X,Y)=P(X|Y)P(Y)\\\\ &amp;②判别学习：直接预测P(Y|X) \\end{aligned} ​①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)​ 二、极大似然估计 Maximum Likelihood Estimation (MLE) 2.1 简单场景：掷硬币 我们投十次硬币，假设投掷结果为：D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T}，则我们一般会进行以下预测： nH=4,nT=6,P(H)=θ≈nHnH+nT=0.4n_H=4,n_T=6,P(H)=\\theta\\approx\\frac{n_H}{n_H+n_T}=0.4 nH​=4,nT​=6,P(H)=θ≈nH​+nT​nH​​=0.4 2.2 形式化定义 上述掷硬币的例子就是极大似然估计的过程，对于MLEMLEMLE，一般分为两步： ①对分布类型进行明确的建模假设 ②设置分布中所涉及的参数 对于掷硬币问题的分布，我们易知这是一个经典的二项分布，他有两个参数：抛硬币次数nnn，某个事件（例如：硬币正面朝上）发生的概率θ\\thetaθ，我们不妨假设 P(H)=θ ~P(H)=\\theta~ P(H)=θ ，则有： P(D∣θ)=CnH+nTnH⋅θnH⋅(1−θ)nT\\begin{aligned} &amp;P(D|\\theta)=C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}\\\\ \\end{aligned} ​P(D∣θ)=CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​​ P(D∣θ) P(D|\\theta)~P(D∣θ) 表示θ\\thetaθ为某个值时，抛硬币结果为DDD的概率，比如D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T} MLEMLEMLE的规则：对于固定的事件DDD，找到一个θ\\thetaθ使得P(D∣θ)P(D|\\theta)P(D∣θ)最大： θ^MLE=argmaxθ P(D∣θ)=argmaxθ CnH+nTnH⋅θnH⋅(1−θ)nT=argmaxθ (nHln⁡θ+nTln⁡(1−θ))\\begin{aligned} &amp;\\hat{\\theta}_{MLE}=\\underset{\\theta}{argmax}~P(D|\\theta)=\\underset{\\theta}{argmax}~C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}=\\underset{\\theta}{argmax}~(n_H\\ln\\theta+n_T\\ln(1-\\theta)) \\end{aligned} ​θ^MLE​=θargmax​ P(D∣θ)=θargmax​ CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​=θargmax​ (nH​lnθ+nT​ln(1−θ))​ 我们对函数进行求导即可得到θ\\thetaθ的极大似然估计值： 令f(θ)=nHln⁡θ+nTln⁡(1−θ)df(θ)dθ=nHθ−nT1−θ=0→θ=nHnH+nT,即θ^MLE=nHnH+nT\\begin{aligned} &amp;令f(\\theta)=n_H\\ln\\theta+n_T\\ln(1-\\theta)\\\\ &amp;\\frac{df(\\theta)}{d\\theta}=\\frac{n_H}{\\theta}-\\frac{n_T}{1-\\theta}=0\\rightarrow \\theta=\\frac{n_H}{n_H+n_T},即\\hat{\\theta}_{MLE}=\\frac{n_H}{n_H+n_T} \\end{aligned} ​令f(θ)=nH​lnθ+nT​ln(1−θ)dθdf(θ)​=θnH​​−1−θnT​​=0→θ=nH​+nT​nH​​,即θ^MLE​=nH​+nT​nH​​​ 可以发现极大似然估计的预测结果与我们的直观预测相同，这就是MLEMLEMLE 三、先验估计 3.1 简单场景：掷硬币 我们仍可以用掷硬币的场景去理解先验估计：假设我们预感θ\\thetaθ接近0.50.50.5。但我们的样本量很小，所以我们对此估计并不确信，可以作如下处理： θ^=nH+mnH+nT+2m\\hat{\\theta}=\\frac{n_H+m}{n_H+n_T+2m} θ^=nH​+nT​+2mnH​+m​ 当nnn很大时，该处理对θ\\thetaθ的影响微不足道；但是当nnn较小时，该处理可以使得θ\\thetaθ更接近我们的猜测。 3.2 形式化定义 假设θθθ是根据分布P(θ)P(θ)P(θ)得到的一个随机值，DDD为一个事件，则有如下贝叶斯公式： P(θ∣D)=P(D∣θ)P(θ)P(D)=P(D,θ)P(D)P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D,\\theta)}{P(D)} P(θ∣D)=P(D)P(D∣θ)P(θ)​=P(D)P(D,θ)​ ①P(θ)P(θ)P(θ)是我们在看到数据之前θθθ的先验分布 ②P(D∣θ)P(D|\\theta)P(D∣θ)是对于给定的参数θ\\thetaθ，事件DDD发生的可能性 ③P(θ∣D)P(\\theta|D)P(θ∣D)是我们观察数据后得到的θ\\thetaθ的后验分布 我们常常利用BetaBetaBeta分布得到θθθ的先验分布： P(θ)=θα−1(1−θ)β−1B(α,β)(其中B(α,β)=Γ(α)Γ(β)Γ(α+β),其目的是对P(θ)进行归一化)P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\ (其中B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)},其目的是对P(\\theta)进行归一化) P(θ)=B(α,β)θα−1(1−θ)β−1​(其中B(α,β)=Γ(α+β)Γ(α)Γ(β)​,其目的是对P(θ)进行归一化) P(θ∣D)∝P(D∣θ)P(θ)∝θnH+α−1(1−θ)nT+β−1P(\\theta|D)\\propto P(D|\\theta)P(\\theta)\\propto\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1} P(θ∣D)∝P(D∣θ)P(θ)∝θnH​+α−1(1−θ)nT​+β−1 四、极大后验估计 在我们知道关于θ\\thetaθ的分布，可以利用极大后验估计得到θ\\thetaθ的估计值 MAPMAPMAP规则：对于一个固定的事件DDD，找到一个θ\\thetaθ，使得P(θ∣D)P(\\theta|D)P(θ∣D)最大： θ^MAP=argmaxθ P(θ∣D)=argmaxθ P(D∣θ)P(θ)P(D)=argmaxθ P(D∣θ)P(θ) =argmaxθ (θnH+α−1(1−θ)nT+β−1)=argmaxθ (nH+α−1)ln⁡θ+(nT+β−1)ln⁡(1−θ) =nH+α−1nH+nT+α+β−2\\begin{aligned} &amp;\\hat\\theta_{MAP}=\\underset{\\theta}{argmax}~P(\\theta|D)=\\underset{\\theta}{argmax}~\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\underset{\\theta}{argmax}~P(D|\\theta)P(\\theta)\\\\ &amp;~~~~~~~~~~~=\\underset{\\theta}{argmax}~(\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1})=\\underset{\\theta}{argmax}~(n_H+\\alpha-1)\\ln\\theta+(n_T+\\beta-1)\\ln(1-\\theta)\\\\ &amp;~~~~~~~~~~~~=\\frac{n_H+\\alpha-1}{n_H+n_T+\\alpha+\\beta-2} \\end{aligned} ​θ^MAP​=θargmax​ P(θ∣D)=θargmax​ P(D)P(D∣θ)P(θ)​=θargmax​ P(D∣θ)P(θ) =θargmax​ (θnH​+α−1(1−θ)nT​+β−1)=θargmax​ (nH​+α−1)lnθ+(nT​+β−1)ln(1−θ) =nH​+nT​+α+β−2nH​+α−1​​ 当n→∞n\\rightarrow\\inftyn→∞时，α−1\\alpha-1α−1和β−2\\beta-2β−2与nHn_HnH​和nTn_TnT​相比可以忽略，即θ^MAP→θ^MLE\\hat\\theta_{MAP}\\rightarrow\\hat\\theta_{MLE}θ^MAP​→θ^MLE​ 五、总结 在监督学习中有一个数据集DDD，我们运用它来训练模型，它有一个参数 θθθ，利用这个模型我们希望对测试点xtx_txt​进行预测，则有如下方法： MLE:P(y∣xt;θ) learning:θ^MLE=arg⁡max⁡θP(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt,θ) learning:θ^MAP=arg⁡max⁡θP(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt,θ)=∫θP(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的\\begin{aligned} &amp;MLE:P(y|x_t;\\theta)~learning:\\hat\\theta_{MLE}=\\arg\\max_\\theta P(D|\\theta),此处的\\theta为一个模型参数\\\\ &amp;MAP:P(y|x_t,\\theta)~learning:\\hat\\theta_{MAP}=\\arg\\max_\\theta P(\\theta|D)\\propto P(D|\\theta)P(\\theta)，此处的\\theta为随机变量\\\\ &amp;True~Bayesian:P(y|x_t,\\theta)=\\int_\\theta P(y|\\theta)P(\\theta|D)d\\theta,此处的\\theta是考虑所有可能模型积分出来的 \\end{aligned} ​MLE:P(y∣xt​;θ) learning:θ^MLE​=argθmax​P(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt​,θ) learning:θ^MAP​=argθmax​P(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt​,θ)=∫θ​P(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的​ ","link":"https://2006wzt.github.io/post/机器学习实战（五）：概率估计/"},{"title":"机器学习实战（四）：感知机算法","content":"感知机算法 一、形式化定义 在机器学习中，感知机是一种用于处理监督学习的二分类问题的分类器，它是一种线性分类器，即一种基于线性预测函数（将一组权重与特征向量相结合）进行预测的分类算法，该模型有着如下的假设： ①处理的问题是二分类问题：e.g. yi∈{−1,+1}②数据是线性可分的\\begin{aligned} &amp;①处理的问题是二分类问题：e.g.~~y_i\\in\\{-1,+1\\}\\\\ &amp;②数据是线性可分的 \\end{aligned} ​①处理的问题是二分类问题：e.g. yi​∈{−1,+1}②数据是线性可分的​ 感知机模型最终所要求解的是权重向量www，其形式化定义如下： 权重向量：w=[w1,w2,...,wd]T特征向量：x=[x1,x2,...,xd]T解空间：H={h(x)=wTx+b=0}\\begin{aligned} &amp;权重向量：w=[w_1,w_2,...,w_d]^T\\\\ &amp;特征向量：x=[x_1,x_2,...,x_d]^T\\\\ &amp;解空间：\\mathcal{H}=\\{h(x)=w^Tx+b=0\\} \\end{aligned} ​权重向量：w=[w1​,w2​,...,wd​]T特征向量：x=[x1​,x2​,...,xd​]T解空间：H={h(x)=wTx+b=0}​ 二、感知机实现 2.1 权重向量 根据形式化定义，我们知道我们的求解目标是一个权重向量www，最终得到模型函数：h(x)=wTx+bh(x)=w^Tx+bh(x)=wTx+b 即我们求解得到www对应的是多维空间中的一个超平面wTx+b=0w^Tx+b=0wTx+b=0，该平面在多维空间中将数据点分为两部分 而我们最后的分类只需要根据h(x)h(x)h(x)的正负判断数据点在哪一侧即可。 权重向量www即为所要求解的超平面的法向量 2.2 偏差项 我们注意到模型函数中还有一项常数bbb，我们称之为偏差项，如果没有偏差项，www所定义的超平面一定经过原点。 为了便于后续处理，我们通过升维操作将偏差项bbb吸收到www中去： 目标超平面：wTx+b=0令x′=[x 1],w=[w′ b]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}\\begin{aligned} &amp;目标超平面：w^Tx+b=0\\\\ &amp;令x&#x27;=\\left[ \\begin{aligned} &amp;x\\\\ &amp;~1 \\end{aligned} \\right],w=\\left[ \\begin{aligned} &amp;w&#x27;\\\\ &amp;~b \\end{aligned} \\right]，则有w&#x27;^Tx&#x27;=w^Tx+b=0\\\\ &amp;由此解空间变为：\\mathcal{H}=\\{h(x)=w^Tx=0\\} \\end{aligned} ​目标超平面：wTx+b=0令x′=[​x 1​],w=[​w′ b​]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}​ 通过升维后超平面一定是经过原点的，则有： 设yi∈{−1,+1},h(xi)&gt;0↔ yi=+1,h(xi)&lt;0↔ yi=−1可得：yi⋅h(xi)=yi⋅(wTxi)&gt;0↔xi分类正确\\begin{aligned} &amp;设y_i\\in\\{-1,+1\\},h(x_i)&gt;0\\leftrightarrow~y_i=+1,h(x_i)&lt;0\\leftrightarrow~y_i=-1\\\\ &amp;可得：y_i\\cdot h(x_i)=y_i\\cdot(w^Tx_i)&gt;0\\leftrightarrow x_i分类正确 \\end{aligned} ​设yi​∈{−1,+1},h(xi​)&gt;0↔ yi​=+1,h(xi​)&lt;0↔ yi​=−1可得：yi​⋅h(xi​)=yi​⋅(wTxi​)&gt;0↔xi​分类正确​ 注意我们尽量将二分类问题的标签设置为{−1,+1}\\{-1,+1\\}{−1,+1}，如果设置为{0,+1}\\{0,+1\\}{0,+1}则没有上述结论。 2.3 算法实现 首先我们先看感知机算法的伪码： mmm用于记录在训练集上分类错误的次数，如果出错则m=m+1m=m+1m=m+1，同时对权重向量www进行调整，调整方式为w=w+yxw=w+yxw=w+yx，该调整方式的几何解释如下图所示： 如上图所示，图一为初始的权重向量以及其所对应的超平面，我们以一个标签为−1-1−1的数据点为例，该超平面错误得将该数据点分到了+1+1+1的一侧（即yi⋅(wTxi)≤0y_i\\cdot(w^Tx_i)\\le0yi​⋅(wTxi​)≤0），则要进行调整：w=w+yx=w−xw=w+yx=w-xw=w+yx=w−x，则得到了如图三所示的调整后的超平面。 数学解释如下，w和bw和bw和b的更新是一个梯度下降的过程，而我们用到的损失函数为： L(w,b)=−∑xi∈Dyi(wTxi+b)L(w,b)=-\\sum_{x_i\\in D}y_i(w^Tx_i+b) L(w,b)=−xi​∈D∑​yi​(wTxi​+b) 当分类错误时会有 yi⋅(wTxi+b)≤0 ~y_i\\cdot(w^Tx_i+b)\\le0~ yi​⋅(wTxi​+b)≤0 ，自然就会使得该损失函数变大，该函数对www和bbb求偏导得： ∂L(w,b)∂w=−∑xi∈Dyixi∂L(w,b)∂b=−∑xi∈Dyi\\frac{\\partial L(w,b)}{\\partial w}=-\\sum_{x_i\\in D}y_ix_i\\\\ \\frac{\\partial L(w,b)}{\\partial b}=-\\sum_{x_i\\in D}y_i ∂w∂L(w,b)​=−xi​∈D∑​yi​xi​∂b∂L(w,b)​=−xi​∈D∑​yi​ 由此我们得到更新过程： w→w+yixib→b+yiw\\rightarrow w+y_ix_i\\\\ b\\rightarrow b+y_i w→w+yi​xi​b→b+yi​ 三、感知机的收敛 感知机可以说是第一个具有强大形式保证的算法。如果有数据集合是线性可分的，感知机将在有限更新次数中找到一个可以分离两类数据点的超平面，如果数据不是线性可分的，它将永远循环。 假设∃w∗有:对于∀(xi,yi)∈D，yi⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi∈D,∣∣w∗∣∣=∑i=1n(wi∗)2=1,∣∣xi∣∣≤1对于w∗对应的超平面，令γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣，即离超平面最近的点对应的距离\\begin{aligned} &amp;假设∃w^∗有:对于∀(x_i, y_i) ∈ D，y_i\\cdot(x^Tw^∗) &gt; 0\\\\ &amp;对每个数据进行等比例收敛使得：对于\\forall x_i\\in D,||w^*||=\\sqrt{\\sum_{i=1}^n(w^*_i)^2}=1,||x_i||\\le1\\\\ &amp;对于w^*对应的超平面，令\\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||，即离超平面最近的点对应的距离 \\end{aligned} ​假设∃w∗有:对于∀(xi​,yi​)∈D，yi​⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi​∈D,∣∣w∗∣∣=i=1∑n​(wi∗​)2​=1,∣∣xi​∣∣≤1对于w∗对应的超平面，令γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣，即离超平面最近的点对应的距离​ 由此，我们得到一个关于更新次数和γ\\gammaγ之间关系的定理： 定理 3-1 感知机算法最多对超平面进行1γ2次调整感知机算法最多对超平面进行\\frac1{\\gamma^2}次调整 感知机算法最多对超平面进行γ21​次调整 此处的调整次数（更新次数）即为上述伪码中for循环中执行 w=w+yx ~w=w+yx~ w=w+yx 的次数，对上述定理的证明如下： γ\\gammaγ是基于上述w∗w^*w∗所对应的超平面得来的，证明过程主要讨论wTw∗w^Tw^*wTw∗和wTww^TwwTw的大小关系： 1)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw∗=(w+yixi)Tw∗=(wT+xiTyiT)w∗=wTw∗+yi⋅(xiTw∗)∵γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣ 且 yi⋅(xiTw∗)&gt;0 ∴yi⋅(xiTw∗)≥γ∴wTw∗=wTw∗+yi⋅(xiTw∗)≥wTw∗+γ2)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw=(w+yixi)T(w+yixi)=wTw+2yi⋅(wTxi)+yi2xiTxi∵yi∈{−1,+1} 且 yi⋅(wTxi)≤0,数据收敛后∣∣xi∣∣2≤1 ∴ yi2=1,xiTxi≤1∴ wTw=wTw+2yi⋅(wTxi)+yi2xiTxi≤wTw+1\\begin{aligned} 1)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw^*=(w+y_ix_i)^Tw^*=(w^T+x_i^Ty_i^T)w^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\\\ &amp;\\because \\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||~且~ y_i\\cdot(x_i^Tw^*)&gt;0~\\therefore y_i\\cdot(x_i^Tw^*)\\ge \\gamma\\\\ &amp;\\therefore w^Tw^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\ge w^Tw^*+\\gamma\\\\ 2)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw=(w+y_ix_i)^T(w+y_ix_i)=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\\\ &amp;\\because y_i\\in\\{-1,+1\\}~且~y_i\\cdot(w^Tx_i)\\le0,数据收敛后||x_i||_2\\le1~\\therefore~y_i^2=1,x_i^Tx_i\\le 1\\\\ &amp;\\therefore~w^Tw=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\le w^Tw+1 \\end{aligned} 1)2)​如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw∗=(w+yi​xi​)Tw∗=(wT+xiT​yiT​)w∗=wTw∗+yi​⋅(xiT​w∗)∵γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣ 且 yi​⋅(xiT​w∗)&gt;0 ∴yi​⋅(xiT​w∗)≥γ∴wTw∗=wTw∗+yi​⋅(xiT​w∗)≥wTw∗+γ如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw=(w+yi​xi​)T(w+yi​xi​)=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​∵yi​∈{−1,+1} 且 yi​⋅(wTxi​)≤0,数据收敛后∣∣xi​∣∣2​≤1 ∴ yi2​=1,xiT​xi​≤1∴ wTw=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​≤wTw+1​ 调整的过程就是www向w∗w^*w∗趋近的过程，在算法的forforfor循环中，如果进行了调整，则wTw∗w^Tw^*wTw∗至少增加了γ\\gammaγ，wTww^TwwTw至多增加了1 设总共调整了M次，则wTw≤M≤wTw∗γ∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw≤M∴M≤1γ2\\begin{aligned} &amp;设总共调整了M次，则w^Tw\\le M\\le\\frac{w^Tw^*}{\\gamma}\\\\ &amp;\\because ||w^*||=1~\\therefore~\\gamma M\\le w^Tw^*\\le||w^T||\\cdot||w^*||=||w||=\\sqrt{w^Tw}\\le\\sqrt{M}\\\\ &amp;\\therefore M\\le\\frac1{\\gamma^2} \\end{aligned} ​设总共调整了M次，则wTw≤M≤γwTw∗​∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw​≤M​∴M≤γ21​​ 四、算法实现 根据上述伪码，我们将通过手动实现与调库的方式，分别实现状态机。 4.1 数据集 在本次算法实战中，我们将使用到的数据集为sklearn所提供的乳腺癌数据集，我们首先先来了解一下数据集的内容： 最终我们得到的数据集如下图所示，总共有569个数据，每条数据有30个特征，标签0，1分别代表是否患癌症。 4.2 手动实现模型 参考伪码，我们可以实现感知机的fitfitfit函数，在此基础上我们引入了两个新的量lr和max\\text{_}iter，学习率lrlrlr控制每次对超平面的更新程度，最大训练轮数max\\text{_}iter避免数据线性不可分时出现死循环。 上述的参数得到的准确率为0.9035087719298246，我们可以通过调参过程使得准确率趋于最优。 4.3 调库实现模型 模型的训练主要用到sklearn所提供的库函数Perceptron，它的函数原型如下： ①penalty：正则化项，l2l2l2、l1l1l1或弹性网络。 ②alpha：正则化项系数，如果使用正则化项，则在正则化项前乘上该系数 ③fit_intercept：是否需要计算截距 ④max_iter：最大训练轮数 ⑤tol：训练的停止标准，训练将在loss&gt;previous\\text{_}loss-tol时停止 ⑥shuffle：是否在每轮训练后对训练数据进行随机排列 ⑦eta0：学习率 4.4 可视化 我们不妨仅选取两个特征对模型进行训练，观察感知机模型最终得到的超平面划分是什么样的，我们利用make_classification创造具有两个特征的用于二分类的数据集，调用库函数进行预测： ","link":"https://2006wzt.github.io/post/机器学习实战（四）：感知机算法/"},{"title":"机器学习实战（三）：KNN算法","content":"K-NN算法 一、形式化定义 K-NN算法（k-nearest neighbor）是一种基本的分类与回归算法，在本节我们主要以分类问题为例介绍K-NN算法。 K-NN模型是一个非参数化模型，参数数量随着训练数据的规模增长，更加的灵活，但是却又较高的计算成本。 ①模型假设：相似的输入有着相似的输出 ②分类规则：对于测试输入x，在其k个最相似的训练输入之间分配最常见的标签 ③回归规则：输出是对象的属性值，该值是k个最近邻值的平均值。 设训练集为DDD，输入的测试点的特征向量为xxx，xxx的kkk个近邻点表示为集合SxS_xSx​，SxS_xSx​有如下性质： ①Sx∈D②∣Sx∣=k③∀(x′,y′)∈D−Sx,dist(x,x′)≥max⁡(x′′,y′′)∈Sxdist(x,x′′)\\begin{aligned} &amp;①S_x\\in D\\\\ &amp;②|S_x|=k\\\\ &amp;③\\forall (x&#x27;,y&#x27;)\\in D-S_x,\\text{dist}(x,x&#x27;)\\ge \\max_{(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x}\\text{dist}(x,x&#x27;&#x27;) \\end{aligned} ​①Sx​∈D②∣Sx​∣=k③∀(x′,y′)∈D−Sx​,dist(x,x′)≥(x′′,y′′)∈Sx​max​dist(x,x′′)​ 根据分类规则，我们可以得到模型函数： h(x)=mode({y′′:(x′′,y′′)∈Sx})mode返回集合中出现频率最高的标签y′′h(x)=\\text{mode}(\\{y&#x27;&#x27;:(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x\\})\\\\ \\text{mode}返回集合中出现频率最高的标签y&#x27;&#x27; h(x)=mode({y′′:(x′′,y′′)∈Sx​})mode返回集合中出现频率最高的标签y′′ 根据上述定义，我们可以通过下图更直观得认识KNN算法，当K=1时，新的数据点被分类为正方形，当K=3时，新的数据点被分为三角形 二、参数选择 2.1 距离函数 K-NN分类器基本上依赖于距离函数的选择，距离度量越能反映标签的相似性，分类器的性能就越好。 最为常用的距离函数为闵可夫斯基距离（Minkowski distance），设数据点的特征向量为ddd维向量，则(x,y)(x,y)(x,y)和(x′,y′)(x&#x27;,y&#x27;)(x′,y′)之间的距离为： dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ 其中x=(x1,x2,...,xd),x′=(x1′,x2′,...,xd′)x=(x_1,x_2,...,x_d),x&#x27;=(x&#x27;_1,x&#x27;_2,...,x&#x27;_d)x=(x1​,x2​,...,xd​),x′=(x1′​,x2′​,...,xd′​)，当ppp取值不同时，得到不同的距离函数。 ①p=2p=2p=2时，为欧氏距离： dist(x,x′)=∑r=1d∣xr−xr′∣2\\text{dist}(x,x&#x27;)=\\sqrt{\\sum_{r=1}^d|x_r-x&#x27;_r|^2} dist(x,x′)=r=1∑d​∣xr​−xr′​∣2​ 欧几里得度量（Euclidean Metric）是一个通常采用的距离定义，指在d维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离），在二维和三维空间中的欧氏距离就是两点之间的实际距离。 ②p=1p=1p=1时，为曼哈顿距离： dist(x,x′)=∑r=1d∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\sum_{r=1}^d|x_r-x&#x27;_r| dist(x,x′)=r=1∑d​∣xr​−xr′​∣ 想象你在城市道路里，要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。 实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源，曼哈顿距离也称为城市街区距离（City Block distance）。 ③p=∞p=\\inftyp=∞时，为切比雪夫距离： dist(x,x′)=max⁡r∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\max_r|x_r-x&#x27;_r| dist(x,x′)=rmax​∣xr​−xr′​∣ 二个点之间的切比雪夫距离定义是其各坐标数值差绝对值的最大值。 国际象棋棋盘上二个位置间的切比雪夫距离是指王要从一个位子移至另一个位子需要走的步数。由于王可以往斜前或斜后方向移动一格,因此可以较有效率的到达目的的格子。上图是棋盘上所有位置距f6f6f6位置的切比雪夫距离。 2.2 K参数 我们应该如何选择一个合适的K值使得分类器的效果达到最优？ 1）一个较小的K值： ①减少在学习过程中的近似误差，即减小在训练集上的误差 ②扩大在学习过程中的估计误差，即增大在测试集上的误差 ③会使得模型更加复杂，容易导致过拟合 2）一个较大的K值： ①减少在学习过程中的估计误差，即减小在测试集上的误差 ②扩大在学习过程中的近似误差，即增大在训练集上的误差 ③会使得模型更加简单 因此我们需要对K值进行一个权衡，这取决于所提供的数据集。通常，较大的k值会减少噪声对分类的影响，但会使类之间的边界不那么明显，在二分类问题中，选择k为奇数有助于避免并列。 三、特殊的K-NN分类器 3.1 1-NN分类器 当K=1K=1K=1时，我们得到的分类器便是1-NN分类器，这是一个最为直观的K-NN分类器，选取离得最近的那个数据点的标签作为预测标签。 1-NN边界划分的方法：选定每一个点最近的那个邻居，作它们连线间的中垂线，中垂线相连形成边界，得到的分类边界如下图所示 3.2 贝叶斯最优分类器 假如我们知道了任何xxx对应的yyy，即知道分布P(y∣x)P(y|x)P(y∣x)（这在现实中不可能），这样对于一个xxx你就可以简单地预测最有可能的标签，Bayes optimal classifier预测为： y^=hopt(x)=argmaxy P(y∣x)\\hat y=h_{opt}(x)=\\underset{y}{argmax}~P(y|x) y^​=hopt​(x)=yargmax​ P(y∣x) 即：对于一个xxx，取概率最高的标签作为预测的标签，Bayes optimal classifier 仍然可能出错，它的出错率即为： ϵBayesOpt=1−P(hopt(x)∣x)=1−P(y^∣x)\\epsilon_{BayesOpt}=1-P(h_{opt}(x)|x)=1-P(\\hat y|x) ϵBayesOpt​=1−P(hopt​(x)∣x)=1−P(y^​∣x) 例：假设一个二分类问题，标签仅有+1和-1两种，并且对于某个xxx，有： P(+1∣x)=0.8,P(−1∣x)=0.2P(+1|x)=0.8,P(-1|x)=0.2 P(+1∣x)=0.8,P(−1∣x)=0.2 因此根据Bayes optimal classifier可得： y^=+1,ϵBayesOpt=0.2\\hat y=+1,\\epsilon_{BayesOpt}=0.2 y^​=+1,ϵBayesOpt​=0.2 由此我们可以知道贝叶斯最优分类器的错误率是在给定数据分布时的最小可实现错误率。同时我们也能得到如下定理： 定理 3-1 证明如下： 如上图所示，在数据集趋近于无穷大时，对于任何一个数据点，它与K个近邻的距离趋近于0，即： 设测试点为xt，它的近邻为xNN,当数据集大小n→∞时，dist(xt,xNN)→0xt→ xNN,因此xt预测的标签将为xNN的标签\\begin{aligned} &amp;设测试点为x_t，它的近邻为x_{NN},当数据集大小n\\rightarrow\\infty时，\\text{dist}(x_t,x_{NN})\\rightarrow0\\\\ &amp;x_t\\rightarrow~x_{NN},因此x_t预测的标签将为x_{NN}的标签 \\end{aligned} ​设测试点为xt​，它的近邻为xNN​,当数据集大小n→∞时，dist(xt​,xNN​)→0xt​→ xNN​,因此xt​预测的标签将为xNN​的标签​ 如上图所示，我们以垃圾邮件分类为例，SpamSpamSpam为垃圾邮件，HamHamHam为正常邮件，假设xNNx_{NN}xNN​的标签为SpamSpamSpam，则有： ϵBayesOpt=1−P(s∣x)ϵ1−NN=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt\\begin{aligned} &amp;\\epsilon_{BayesOpt}=1-P(s|x)\\\\ &amp;\\epsilon_{1-NN}=P(s|x)(1-P(s|x))+(1-P(s|x))P(s|x)=2(1-P(s|x))P(s|x)&lt;2(1-P(s|x))=2\\epsilon_{BayesOpt} \\end{aligned} ​ϵBayesOpt​=1−P(s∣x)ϵ1−NN​=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt​​ 四、维度灾难 当向量的维度很高时，KNN模型将变得非常不稳定。 4.1 数据点之间的距离 首先让我们想象一个ddd维的单位超立方体[0,1]d[0,1]^d[0,1]d，所有的数据都在此超立方体内均匀采样，即对于任意的xix_ixi​，都有xi∈[0,1]dx_i\\in[0,1]^dxi​∈[0,1]d。接着我们不妨考虑一下K=10K=10K=10时，对于一个测试点它的近邻所在的超立方体，如下图所示： 设 l ~l~ l 是包含KKK个近邻的最小超立方体的边长， n ~n~ n 是取样的数据点数，因为数据集是在该单位超立方体中均匀取样，则有： ld=kn,l=(kn)1dl^d=\\frac{k}{n},l=(\\frac{k}{n})^{\\frac1d} ld=nk​,l=(nk​)d1​ 对于K=10,n=1000K=10,n=1000K=10,n=1000时的情况，则有： d l 2 0.1 10 0.63 100 0.955 1000 0.9954 因此，当维度很高（d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0）时，几乎需要整个空间才能找到某个测试点的10个近邻。 从而导致最近的KKK个邻居并不比训练集中其他的点离测试点更近。 这是因为，在ddd维空间中随机分布的点之间的距离逐渐趋近于一个很小的范围内，如下图所示： 4.2 到超平面的距离 对于二分类问题，我们往往是在ddd维空间中寻找一个超平面，将空间一分为二，超平面两侧的数据即为不同的标签。 如上图所示，维度灾难对于两点之间的距离和点与超平面之间的距离有不同的影响。 同一维度中的移动不能增加或减少到超平面的距离，点只是四处移动并与超平面保持相同的距离。但随着维度的升高，成对点之间的距离变得非常大，到超平面的距离变得相对较小。 维数灾难的一个后果是，大多数数据点往往非常接近这些超平面，并且通常可能轻微的扰动就会更改分类。 4.3 数据降维 为了解决维度灾难，最好的方法便是对数据进行降维操作，我们以图像识别为例：虽然一张脸的图像可能需要1800万像素，但是我们可能只需要用少于50个属性（例如男性/女性、金发/深色头发等）就可以描述一个人，这些属性会随着脸的变化而变化。 如上图所示，这是从底层二维流形绘制的三维数据集示例。蓝色点位于粉色表面之上，而粉色表面则嵌入在三维空间之中，粉色表面映射为二维平面，进而将数据集从三维映射到二维，实现数据降维。 五、算法实战 本次我们使用Scikit-learn所提供的鸢尾花数据集，通过调库的方式实现K-NN算法。 5.1 数据集 我们首先了解一下该数据集的特点，读取数据并将其存储为csv文件的方式如下： 我们最终得到的数据集如下图所示，总共有150个数据，每条数据有4个特征，分别为花萼长度、花萼宽度、花瓣长度、花瓣宽度，相应的不同特征所对应的标签共有3种，分别为setosa、versicolor、virginica，即鸢尾花的三个种类。 5.2 模型训练 模型的训练主要用到sklearn所提供的库函数KNeighborsClassifier，它的函数原型如下： ①n_neighbors：即K值的大小，默认为5 ②weights：用于预测的权重函数。可选参数如下: ③algorithm：计算最近邻居用的算法。可选参数如下： 球树与kd树将会在后续文章中进行介绍，这是一种优化KNN算法的方式。 ④leaf_size：传入BallTree或者KDTree算法的叶子数量，此参数会影响构建、查询BallTree或者KDTree的速度，以及存储BallTree或者KDTree所需要的内存大小。 ⑤p：用于Minkowski metric的超参数，即当我们用闵可夫斯基距离作为度量时，p值的大小。 dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ ⑥metric：计算距离的方式，默认为闵可夫斯基距离。 我们不妨先用简单的欧几里得距离构建模型，观察模型的预测效果： 在这里准确率的计算方法比较简单，即Accuracy=预测准确的数据点总测试点数Accuracy=\\frac{预测准确的数据点}{总测试点数}Accuracy=总测试点数预测准确的数据点​ 上述模型中，我们最终在测试集上得到的准确率为0.9666666666666667，可以看出模型的效果已经非常好了。 5.3 可视化 接下来让我们进行一些调参过程，观察能否让准确率再高一些，首先调整KKK值，对于每个KKK值我们进行十折交叉验证： 如上图所示，是在KKK在[1,30][1,30][1,30]上取不同值时十折交叉验证的准确率，因为是随机对数据集进行划分，所以每次所得的曲线图是不同的，不过其大致趋势是相同的，我们最终可以得到KKK取到[12,18][12,18][12,18]能得到最高准确率的概率最高。 而其他参数的调参过程完全可以效仿上述K值调参可视化的过程，只需要修改需要遍历的取值范围以及参数名即可，即这部分代码： 5.4 特征重要性 因为在鸢尾花数据集中仅有4个特征，所以我们在此讨论特征重要性其实是没必要的，但是对于具有很多特征的数据集我们则需要依据特征的重要性对特征进行筛选，一方面可以提高预测的准确性，另一方面可以提高模型的效率，我们不妨先观察鸢尾花数据集的4个特征对分类准确性的影响。 我们可以通过各个特征对所对应的标签分布直观得判断特征的重要性，可视化代码如下： 我们任取两个特征，观察特征对所对应的数据分布即可判断特征的重要性，如上图所示，是我们选择petal_length,petal_width两个特征可视化的结果，可以发现三种标签的分布并没有过多的交织，说明这两个特征对于分类是较为重要的。 4个特征所对应的6个特征对的标签分布如下图所示，由此我们可以大致判断特征的重要性： 六、总结 1）如果距离可以可靠得反映相异性，则KNN是一个简单高效的模型。 2）在数据集大小n→∞n\\rightarrow\\inftyn→∞时，KNNKNNKNN模型将变得非常精确，但是也会变得非常缓慢。 3）当数据维度d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0时，会发生维度灾难 4）优点：①没有关于数据的假设，例如：对非线性数据非常有用 ②算法简单，易于理解 ③具有较高的精度，但是与更好的监督学习模型相比没有竞争力 ④用途广泛，适用于分类以及回归问题 5）缺点：①具有较高的计算成本，因为算法要用到所有的训练数据 ②具有较高的内存要求，需要存储几乎所有的训练数据 ③对无关特征和数据规模较为敏感，存在维度灾难 ","link":"https://2006wzt.github.io/post/机器学习实战（三）：KNN算法/"},{"title":"机器学习实战（二）：监督学习","content":"监督学习 一、形式化定义 在监督学习的过程中，我们输入的训练数据是成对输入的(x,y)(x,y)(x,y)，x∈Rdx\\in R_dx∈Rd​是输入实例，yyy是其标签，整个训练数据表示为： D={(x1,y1),...,(xn,yn)}⊆Rd×CD = \\{(x_1, y_1), . . . ,(x_n, y_n)\\} ⊆ R_d × C D={(x1​,y1​),...,(xn​,yn​)}⊆Rd​×C 其中RdR_dRd​是ddd维特征空间，CCC是标签空间，xix_ixi​是第iii个样本的特征向量，yiy_iyi​是第i个样本的标签 数据点(xi,yi)(x_i,y_i)(xi​,yi​)来自一些未知的分布P(X,Y)P(X,Y)P(X,Y) 最终我们希望学习出一个模型函数hhh，对于一个新的数据点(x,y)(x,y)(x,y)，我们有较高概率的 h(x)=y 或 h(x)≈y~h(x)=y~或~h(x)\\approx y h(x)=y 或 h(x)≈y 监督学习的标签空间决定了问题的类型，典型的标签空间如下： Type Lable Space E.g. 二分类 C={0,1} or C={−1,+1}C = \\{0, 1\\}~or~C = \\{−1, +1\\}C={0,1} or C={−1,+1} E.g. 垃圾邮件过滤问题. 一个邮件要么是垃圾邮件(-1)要么不是(+1) 多分类 C={1,2,⋅⋅⋅,K}(K≥2)C = \\{1, 2, · · · , K\\} (K ≥ 2)C={1,2,⋅⋅⋅,K}(K≥2) E.g. 人脸识别问题.一个人的身份可以是K个身份中的一个 回归 C=RC = RC=R E.g.预测某一天的温度或者某个人的身高 二、损失函数 2.1 定义 什么是损失函数？顾名思义，它是一个用于评估模型对于数据集拟合的损失程度的函数，我们希望预测的效果越差，损失函数的值越大，这也为我们在搭建模型算法的过程中提供了方向：我们要尽可能得使得模型的损失函数输出最小化，以达到较好的拟合效果。 同时在优化过程中，损失函数输出的值变化也可以说明我们的在模型优化上的进展。 事实上，我们可以设计一个非常基本的损失函数来进一步解释它是如何工作的。对于我们所做的每个预测，我们的损失函数将简单地测量预测值和实际值之间的绝对差，即： L(h)=1n∑i=1n∣h(xi)−yi∣L(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| L(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 其中LLL是损失函数，hhh是我们训练出的模型函数，nnn是验证集的样本数量，h(xi)h(x_i)h(xi​)是特征向量xix_ixi​的预测标签，yiy_iyi​则是实际的标签，显然当我们预测完全准确时L(h)=0L(h)=0L(h)=0，预测值与实际值差别越大，损失函数的值越大，这满足损失函数的特性。 2.2 经典的损失函数 2.2.1 Zero-one loss 0-1损失函数的形式化定义如下： L1/0(h)=1n∑i=1nδh(xi)=yi对于分类问题：δh(xi)=yi={1,if h(xi)=yi0,o.w.对于回归问题：δh(xi)=yi={1,if ∣h(xi)−yi∣&gt;t0,o.w.L_{1/0}(h)=\\frac1n\\sum_{i=1}^n\\delta_{h(x_i)\\not=y_i}\\\\ 对于分类问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~h(x_i)\\not=y_i\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right.\\\\ 对于回归问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~|h(x_i)-y_i|&gt;t\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right. L1/0​(h)=n1​i=1∑n​δh(xi​)​=yi​​对于分类问题：δh(xi​)​=yi​​={​1,if h(xi​)​=yi​0,o.w.​对于回归问题：δh(xi​)​=yi​​={​1,if ∣h(xi​)−yi​∣&gt;t0,o.w.​ 根据δ\\deltaδ函数的定义，当我们的预测值准确时，会给损失函数加上0，对于不准确的预测值则会给损失函数加上1 0-1损失函数也有明显的缺点，对于分类问题影响并不大，但是对于回归问题，ttt是某个自定义的阈值，δ\\deltaδ函数对于所有的误差大于阈值的惩罚相同，即错误的预测所带来的惩罚都为1，对于一些很离谱的预测（比如1预测成了10000）并没有额外的惩罚。 2.2.2 Squared loss 平方损失函数的形式化定义如下： Lsq(h)=1n∑i=1n(h(xi)−yi)2L_{sq}(h)=\\frac1n\\sum_{i=1}^n(h(x_i)-y_i)^2 Lsq​(h)=n1​i=1∑n​(h(xi​)−yi​)2 平方损失函数常用于分类问题，它有两个主要特点： ①损失函数的值永远是非负的 ②损失函数的值与预测的绝对误差呈二次关系 显然平方损失函数规避了0-1损失函数的缺点，对于预测误差较大的样本，所带来的惩罚也越大，但同时对于预测较准确的点，所带来的惩罚也会变得更小，对于噪声数据的处理会使得模型的效果变差。 2.2.3 Absolute loss 绝对损失函数的形式化定义如下： Labs(h)=1n∑i=1n∣h(xi)−yi∣L_{abs}(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| Labs​(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 绝对损失函数的值随着预测失误而线性增长，因此更适合于噪声数据 三、泛化能力 学习方法的泛化能力，是指由该学习方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。 对于给定的一个损失函数LLL，我们可以得到一个使得损失函数最小化的模型hhh，即： h=argminh∈H L(h)h=\\underset{h\\in H}{argmin}~L(h) h=h∈Hargmin​ L(h) 机器学习的很大一部分集中在这个问题上，即如何有效地进行最小化。如果我们得到一个模型函数h(⋅)h(·)h(⋅)，它在我们的训练数据DDD上的损失很小，我们该如何确定它在DDD之外的数据上的损失也很小呢？这就是泛化的问题，一个泛化能力较差的模型如下： Bad example： h(x)={yi,(xi,yi)∈D∩x=xi0 ,o.w.h(x)=\\left\\{ \\begin{aligned} &amp;y_i,(x_i,y_i)\\in D \\cap x=x_i\\\\ &amp;0~,o.w. \\end{aligned} \\right. h(x)={​yi​,(xi​,yi​)∈D∩x=xi​0 ,o.w.​ 对于这个模型函数，我们在训练的数据集DDD上的误差为0，但是对于数据集DDD外的数据点，显然将会有很大的误差，这就是过拟合问题。 四、过拟合 什么是过拟合？ 在模型的监督学习过程中，我们往往是在训练集上设计模型，在测试集上评估模型的准确性，如上图所示，图1是欠拟合的情况，即模型在训练集上也没有很好的准确性，因此在测试集上的准确性也不会高；图2是较为合适的拟合，它规避了噪声的影响，较为准确得对两类点进行了划分；图3则是过拟合的情况，它最大化了模型在训练集上的准确度而忽略了噪声点的影响，使得模型在训练集上表现得很好，但是在测试集上却表现不佳。 在西瓜书中也有一个比较形象的例子： 五、训练与测试 5.1 定义 训练是使得模型可以学习的过程，而测试则是模型进行预测的过程，训练集的标签是已知的，即可观察的，而测试集的正确标签则是未知的，我们的模型要从已知的特征与标签的对应关系中进行学习，然后对未知的测试集进行预测。 **No free lunch rule：**一个模型hah_aha​即使在某些问题上比另一个模型hbh_bhb​好，也必然存在另一些问题使得hbh_bhb​效果比hah_aha​好 5.2 数据集的划分 对于所提供的数据集，为了训练模型，我们往往需要对数据集DDD进行划分，一般划分成训练集DTRD_{TR}DTR​、验证集DVAD_{VA}DVA​、测试集DTED_{TE}DTE​三部分。 一般的划分比例为80%（DTRD_{TR}DTR​），10%（DVAD_{VA}DVA​），10%（DTED_{TE}DTE​），对于不同的数据集可以进行调整。 为什么我们需要验证集DVAD_{VA}DVA​？ DVAD_{VA}DVA​用于检查从DTRD_{TR}DTR​中获得的模型函数h(⋅)h(\\cdot)h(⋅)是否存在过拟合的问题，也就是我们所追求的目标不只是hhh在训练集DTRD_{TR}DTR​上的损失最小化，也要兼顾在验证集DVAD_{VA}DVA​上的损失最小化，如果h(⋅)h(\\cdot)h(⋅)在DVAD_{VA}DVA​上的损失很大，h(⋅)h(·)h(⋅)将根据DTRD_{TR}DTR​进行修订，并在DVAD_{VA}DVA​上再次验证。该过程将不断来回，直到在DVAD_{VA}DVA​上产生低损失，在验证集上的误差是接近泛化误差的。 在DTRD_{TR}DTR​和DVAD_{VA}DVA​的大小之间有一个权衡：对于较大的DTRD_{TR}DTR​，训练结果会更好，但如果DVAD_{VA}DVA​较大，验证会更可靠（噪音更少）。 对于监督学习实际的应用问题，我们一般只需要在提供的已知标签的数据集上进行训练集与验证集的划分，根据所提供的数据集的特点，有如下的划分规则： ①含有时间成分的数据集：一定要遵循过去预测未来的规则，不能利用未来的数据去预测过去的数据。 ②不含有时间成分的数据集：可以均匀随机得进行划分 利用验证集进行模型评估的常用方法为k折交叉验证，其原理如下图所示： 六、总结 1）学习的过程： Learning:h(⋅)=argminh∈H 1∣DTR∣∑(x,y)∈DTRL(x,y∣h(⋅))Learning:h(\\cdot)=\\underset{h\\in \\mathcal{H}}{argmin}~\\frac1{|D_{TR}|}\\sum_{(x,y)\\in D_{TR}}L(x,y|h(\\cdot)) Learning:h(⋅)=h∈Hargmin​ ∣DTR​∣1​(x,y)∈DTR​∑​L(x,y∣h(⋅)) 2）评估的过程： Evaluation:ϵTE=1∣DTE∣∑(x,y)∈DTEL(x,y∣h(⋅))Evaluation:\\epsilon_{TE}=\\frac{1}{|D_{TE}|}\\sum_{(x,y)\\in D_{TE}}L(x,y|h(\\cdot)) Evaluation:ϵTE​=∣DTE​∣1​(x,y)∈DTE​∑​L(x,y∣h(⋅)) 根据监督学习的上述基本原理，我们将在后续文章中开始相关算法的实战！ ","link":"https://2006wzt.github.io/post/机器学习实战（二）：监督学习/"},{"title":"机器学习实战（一）：机器学习导论","content":"机器学习导论 一、什么是机器学习 机器学习是人工智能的一个分支，涉及算法的设计和开发，允许计算机根据经验数据进化行为。由于智能需要知识，计算机必须获取知识。Tom Mtichell于1997年对机器学习的定义：一个计算机程序A，从经验E中学习关于某个任务T的性能度量P，E有助于提高计算机在任务T上的性能表现，这是基于统计和优化的，而不是基于逻辑的。 二、ML VS CS 机器学习的核心是程序，但是最终目标是结果。以垃圾邮件分类为例，我们所要研究的是进行分类的算法，但是我们的最终目标是邮件的类型。而传统的计算机科学则是根据一定的逻辑规则由输入得到正确的输出。 三、基本分类 3.1 监督学习 在监督学习（supervised learning）的过程中，算法从我们所提供的数据集中进行学习。 监督学习与非监督学习的最大区别在于我们知道用于训练的数据集的正确结果或期望输出，算法对验证集进行预测，由监督者对预测结果进行评估与纠正，进而优化算法，当算法达到可接受的性能水平时，则停止学习。 监督学习有两种主要类型：分类与回归 3.1.1 分类问题 顾名思义，就是根据特征对事物进行分类，一些经典的分类问题例子： ①垃圾邮件过滤问题：根据邮件内容对邮件是否为垃圾邮件进行分类，这是一个二分类问题。 ②人脸识别问题：显然这就是一个多分类问题，根据照片中所包含的数据判断出人脸所对应的身份 3.1.2 回归问题 回归用于预测输入和输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值也随之发生变化，经典的回归问题如下： ①房价预测：根据所提供的相关特征对某个楼盘的房价进行预测 ②天气预测：根据相关指标对未来的天气，例如PM2.5值进行预测 3.2 非监督学习 无监督学习（unsupervised learning）是一类用于在数据中寻找模式的机器学习技术。无监督学习算法使用的输入数据都是没有标注过的，这意味着数据只给出了输入变量（自变量 X）而没有给出相应的输出变量（因变量y）。 在无监督学习中，算法本身将发掘数据中有趣的结构。人工智能研究的领军人物 Yann LeCun，解释道：无监督学习能够自己进行学习，而不需要被显式地告知他们所做的一切是否正确。 非监督学习最为经典的例子便是聚类问题。 3.2.1 聚类问题 顾名思义，聚类就是根据所提供的相关数据，对具有相似特征的事物进行分类，比如照片的类型、网页主题的聚类。 在后期的学习中我们主要关注监督学习的相关算法。 3.3 强化学习 强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优化行为策略的机器学习问题，强化学习的本质是学习最优的序贯决策。 智能系统与环境的互动如下图所示，在每一步t，智能系统从环境中观测到一个状态（state）sts_tst​与一个奖励rtr_trt​，采取一个动作（action）ata_tat​。环境根据只能系统选择的动作，决定下一步t+1的状态st+1s_{t+1}st+1​与奖励rt+1r_{t+1}rt+1​。要学习的策略表示为给定的状态下采取的动作。 智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化，强化学习的过程中系统不断试错，以达到学习最优策略的目的。 3.4 半监督学习 半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。通常有少量标注数据、大量未标注数据，因为标注数据的构成往往需要人工，成本较高，未标注的数据的收集则不需要太多成本，半监督学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。 四、经典的机器学习过程 如下图所示，这是一个属于监督学习的二分类问题的机器学习过程，通过所提供的训练数据使用学习算法训练出模型，再将我们所要预测的不带标签（label）的新数据样本输入到模型中去，进而预测出该样本所属的类别。 在后续的博客中，我将会介绍机器学习中的重要概念与相关算法，还请大家多多支持与关注😄！ ","link":"https://2006wzt.github.io/post/机器学习实战（一）：机器学习导论/"},{"title":"Python入门与实战","content":"Python 一.Python初探 1.Python语言的基本要素 （1）符号和注释 ①符号：需要用英文字符,程序前面不能随便加空格 ②注释：单行注释用#开头，选中部分Ctrl+/可以代码和注释间相互转化 （2）变量 变量有名字，其值可以存储数据 （3）赋值语句 变量=表达式，使变量的值变得跟表达式一样 2.初步认识字符串 （1）字符串初步 字符串可以且必须用单引号，双引号和三引号括起来；当字符串太长时，可以用\\进行分行输出 三双引号字符串中可以包含换行符，制表符等其他字符，双引号中也可以有\\n 有n个字符的字符串，编号从左向右为0-&gt;n-1，从右往左编号为-1-&gt;-n 字符串可以用+号连接 注意字符串不可以修改，在初始化后不可以修改该字符串中任何一个字符 可以用in和not in判断子串 （2）字符串和数的转换 3.简单的输入输出 （1）输入输出初步 输出语句print()，可以输出一项或多项，输出多项时用，隔开，输出效果为用空格隔开，输出后自动换行 有时不想换行可以在print的括号最后加上end=&quot;&quot;,缺省的情况下为end=&quot;\\n&quot; 输入语句x=input(y)，先输出y再等待输入，最后将输入的内容以字符串的形式赋值给x。 要求y一定要是字符串，且赋值输入字符串给x时不带最后的回车 4.初步认识列表 （1）列表初步 列表可以有0到多个元素，并且可以通过下标访问，下标的概念与字符串相同，列表中的元素类型可以不同 可以用in判断列表中是否有某个元素 输入两个整数并求和 二.基本运算、条件分支和输出格式控制 1.算术运算、逻辑运算和分支语句 （1）算术运算 +，-，*，/结果为小数，%取模，//求商结果为整数，**求幂 （2）关系运算符，逻辑运算符，逻辑表达式 ①关系运算符：==,&lt;,&lt;=,&gt;,&gt;=,!=，比较结果为bool值，True或者False，字符串按照字典排序进行比较 ②逻辑运算符：and（与），or（或），not（非），False等于0，True等于1，优先级not&gt;and&gt;or 0，&quot;&quot;(空字符串)，[]（空表）都相当于False；非0数，非空字符串，非空列表都相当于True ③逻辑表达式：逻辑表达式是短路计算的，当表达式的值可以确定时则会停止计算 （3）条件分支语句 字符串切片：若s为一个字符串，则s[x:y]为从下标x到下标y-1的子串 2.输出格式控制 输出格式控制符与C语言的大致相同，且输出格式控制符仅能作用在字符串的输出中 三.循环语句 1.for循环语句 forin: ​ 令variable的值依次是sequence里面的每一个元素，并对每个variable值进行操作statements1 2.break、continue语句 break语句用于跳出循环，continue语句用于直接进入下一层循环，其用法与C语言类似 下为一些for循环语句的例题: 3.while循环语句 4.异常处理 常见的异常有： （1）不合适的转换：int(&quot;abc&quot;),float(&quot;abc&quot;),int(12.34) （2）输入已经结束后还执行input() （3）除法的除数为0 （4）整数和字符串相加 （5）列表下标越界 5.循环综合例题 四.函数和递归 1.函数的概念和用法 函数中的变量：在函数中定义的变量不在函数外起到作用，如果函数内的变量x与全局变量x重名，则如果在函数中未对变量x进行赋值，则认为x为全局变量，反之则为函数内部的变量，在函数内可以用global x声明x为全局变量 python还有很多内置函数，除了前面所讲，还有abs(x)求x绝对值，max(x),min(x)求列表x中元素的的最大最小值，max(x1,x2...),min(x1,x2...)等 2.递归的概念 五.字符串和元组 1.Python变量的指针本质 isinstance函数： python中的变量都是指针，赋值的过程即为指针指向某个内存的过程，列表中的元素也可以进行赋值，因此列表中的每个元素都是指针 is运算符和==运算符的区别： ①a is b 为True代表a，b指向同一个内存 ②a==b代表a，b指向内存中的值相等 2.字符串详解 转义字符：\\与其后面的一个字符一同构成，\\n为换行符，\\t为制表符等等 字符串切片：s[x:y]代表下标x到y-1的子串，字符串切片的方法也适用于元组和列表 字符串的分割：s.split(x)为以x为分隔符，将字符串s分割成一个列表，列表中的每个元素为字符串的子串 字符串的函数： 字符串编码：字符串的编码在内存中的编码是unicode的 字符串的格式化： 3.元组 元组：一个元组由数个逗号分隔的值组成，前后的括号可加可不加 元组与列表的区别在于：元组不可修改，不可增删元素，不可对元素进行赋值，不可修改元素的排序，对元组进行处理的速度比列表快 注意元组的元素的内容可以被修改。 元组的元素本质上是指针，元组的元素不可修改指的是元组元素的指向不可被改变，但指向的内容可修改 元组的下标访问、切片规则以及运算法则与字符串完全相同 元组的运算、迭代和赋值 元组比大小：类似于字符串按照字典顺序和数字大小一一对应进行比较，但是数字于字符串不能比大小，此时会报错 我们可以用列表或元组取代复杂的分支结构 六.列表 1.列表的基本操作 列表是可以增删元素的，并且列表中的元素也可以进行修改 列表的运算 列表的切片 2.列表的排序 选择排序：基础的排序算法，但是效率较低 python自带的排序函数： 对于不同的需求，我们可以自定义比较函数用在sort函数上 3.复杂列表的自定义排序 （1）lambda表达式 lambda x:x[2]，表示一个函数，参数是x，返回值是x[2]，相当于是一个匿名函数 （2）自定义排序 （3）元组的排序 元组的元素不能被修改，因此没有相应的sort函数，但是有sorted函数可以作用于元组，但是返回值是一个列表 4.列表和元组的高级用法 列表的相关函数： 列表映射： 列表过滤： 列表生成式： 二维列表的定义： 列表的拷贝： 列表的深拷贝： 元组和列表的互转： 元组列表和字符串的互转： 5.例题 七.字典和集合 1.字典的基本概念 字典中的每个元素由“键：值”两部分组成，每个元素的键是唯一且不与其他元素的键相同的，可以根据键对元素进行快速查找 键必须是不可变的数据类型，如字符串，整数，小数，元组；而列表，集合，字典等可变数据类型不能作为字典元素的键 字典的增删元素： 字典元素不能重复，如果重复则保留后一个出现的元素： 字典的构造： 2.字典的相关函数 遍历字典： 字典的浅拷贝与深拷贝： 3.字典例题 4.集合 集合的定义同数学上的集合，集合中元素类型可以不同，不会有重复的元素，可以增删元素，集合中的元素类型应为可变数据类型如：字符串，整数，小数，复数，元组，而不能为列表、字典和集合 集合的作用是可以快速地判断某个元素是否在一堆元素里面 集合的构造： 集合常用的函数： 集合的逻辑运算： 集合的比较： 八.文件读写和文件夹操作和数据库 1.文本文件的读写 创建文本文件并写入内容： 读取现有文件： 在文件中添加内容： 2.文本文件的编码 常见文件的编码有gbk和utf-8两种，ANSI对应gbk，创建和读写文件时都可以指定编码，如果不指定则默认为缺省的编码 .py文件必须存成utf-8格式才能运行，如果存成ansi格式（即为gbk格式），应在文件开头写#coding =gbk 3.文件的路径 相对路径：文件路径没有包含盘符 当前文件夹：一般来说.py文件所在的文件夹就是当前文件夹 绝对路径：文件路径指明了盘符 4.文件夹操作 python的文件夹和文件操作函数： 有时文件夹非空但也想要删除文件夹，则不能调用os.rmdir(x)，可以自己定义删除文件夹的函数： 获取文件夹总大小也可以同理自己定义函数： 5.命令行参数 Win+R并输入cmd即可打开控制台，输入python ×××.py即可运行×××.py，注意需要先将控制台转到.py当前文件夹下 当.py文件需要输入参数时，我们希望可以直接在命令行直接输入，可进行如下操作： 6.文件处理实例 7.数据库和SQL语言简介 （1）数据库： 数据库可以存放大量数据，并且提供了方便快捷的检索手段，一个数据库可以是一个文件 一个数据库中可以有多张表，每张表又又不同的字段，字段又有其相应的类型 （2）SQL语言： SQL命令是用于进行数据库操作的标准语句 用sqlite3.exe打开.db文件时，输入select * from 表名;即可显示表中信息，注意一定要加分号 （3）数据库的查询和修改 数据库的查询： 数据库的修改： （4）数据库二进制字段处理 九.正则表达式 1.正则表达式的概念 正则表达式是某些字符有特殊含义的字符串，可以用相关函数用来判断其他字符串是否能与正则表达式进行匹配 正则表达式中的功能字符：\\d等不是转义字符，都是两个字符 正则表达式中的特殊字符：. + ? * ( ) [ ] { } ^ \\ $，如果要在正则表达式中表示这几个字符本身，应该在前面加\\ 2.字符范围和量词 字符的复合： 正则表达式示例： 3.正则表达式的函数 使用正则表达式要import re (1)re.match(pattern,string,flags=0)：从string的起始位匹配一个模式pattern，flags标志位用于控制模式串的匹配方式，如果匹配的上则返回一个匹配对象，否则返回None (2)re.search(pattern,string,flags=0)：查找字符串中第一个可以匹配成功的子串，查找成功返回匹配对象，否则返回None (3)re.findall(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个列表，如果匹配失败则返回空表 (4)re.finditer(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个序列，序列中的每个元素都是一个匹配对象 4.边界符号 边界符号本身不与任何字符匹配 5.分组 括号中的一个表达式就是分组，多个分组按照左括号，从左到右从1开始编号 在分组的右边可以通过分组的编号引用该分组所匹配的子串 分组作为一个整体，后面可以跟量词，且不一定需要匹配相同的字符串 当正则表达式中没有分组时，re.findall返回所有匹配子串构成的列表 当正则表达式有且仅有一个分组时，re.findall返回的是一个子串的列表，每个元素是一个匹配子串中分组对应的内容 当正则表达式中有超过一个分组时，re.findall返回的是一个元组的列表，元组中的每个元素依次是各个分组的内容 6.|的用法 |表示或，如果没有放在分组中，|的起作用范围是直到整个正则表达式开头或结尾的另一个”|“ |从左到右短路匹配，当匹配到左边的后不会再匹配右边 |也可以用在分组中，起作用的范围就仅限于分组 7.贪婪模式和懒惰模式 （1）贪婪模式 量词+,*,?,{m,n}都默认匹配尽可能长的子串，存在很明显的弊端 （2）懒惰模式 即为非贪婪模式，在量词+,*,?,{m,n}后面加上？则匹配尽可能短的子串 8.匹配对象的函数 十.玩转Python生态 1.用datatime库处理日期、时间 处理日期： 处理时刻： 2.用random库处理随机事务 设置随机数种子：如果没有设置则是按照系统时间作为随机数种子，随机数种子设定后每次随机结果都是唯一的 random库应用实例： 3.用jieba库进行分词和中文词频统计 不仅可以将一个词加入jieba的字典，也可以将一个文件中的词加入jieba的字典 4.用openpyxl库处理excel文档 （1）用openpyxl库读取excel文档 有时候单元格中的内容是公式，希望打开文档的时候自动计算公式的值，则可进行如下操作： （2）用openpyxl库创建excel文档 （3）用openpyxl库设定excel文档单元格样式 4.用Pillow处理图像 （1）图像基本常识： 图像由像素构成：屏幕上每个像素点由三个距离非常近的点构成，分别显示红绿蓝三种颜色，每个像素可以由一个元组表示(r,g,b)， r，g，b通常是不大于255的整数，分别表示红绿蓝三原色的深浅程度 图像模式： ①RGB：一个像素有红绿蓝三个分量 ②RGBA：一个像素有红绿蓝三个分量以及透明分量 ③CYMK：一个像素有青色（Cyan），洋红色（Magenta），黄色（Yellow），黑色（K代表黑）四个分量构成，每个像素用元组(c,y,m,k)表示，对应于彩色打印机或者印刷机的四种颜色墨水 L：黑白图像，像素就是一个整数，代表灰度 （2）图像的基本操作： ①图像的缩放： ②图像的旋转、翻转和滤镜效果： ③图像的裁剪： ④图像素描化： ⑤为图像添加水印： 原理：在将一个图像粘贴到另一个图像上时，会用到paste函数，paste函数还有一个参数mask称为“掩膜”指定img的每个像素粘贴过去的透明度，如果透明度为0则完全透明，如果透明度为255则完全遮盖原图像的像素 mask本质上是一个模式为“L”的图片即一个Image对象 十一.数据分析和展示 1.numpy库的使用 numpy是一个多维数组库，创建多维数组很方便，可以代替多维列表，速度比多维列表快，支持向量和矩阵的各种数学运算，所有元素类型必须相同 （1）用numpy库创建数组 （2）numpy常用的属性与函数 （3）numpy数组元素增删 numpy数组一旦生成，则不能增删元素，此处的增删指的是增删之后生成一个新数组，原数组保持不变 numpy增添数组元素： numpy删除数组元素： （4）在numpy数组中查找元素 （5）numpy数组的数学运算 （6）numpy数组的切片 不同于列表的切片，numpy数组的切片是“视图”，即为原数组的一部分，而非原数组一部分的拷贝，切片改变原数组也改变 2.数据分析库pandas pandas的核心功能是在二维表格上做各种操作，需要numpy库的支持，在openpyxl库的支持下还可以读取excel文档 pandas中最为关键的类是DataFrame，用于表示二维表格 （1）pandas的重要类Series Series为一维表格，每个元素带有标签与下标，类似于列表和字典的结合 （2）DataFrame的构造和访问 DataFrame是带行列标签的二维表格，它的每一行都是一个Series （3）DataFrame的切片、增删和统计 ①DataFrame的切片： ②DataFrame的分析统计： ③DataFrame的修改和增删： 修改和增加： 删除： （4）用pandas库读取excel和csv文档 需要依赖于openpyxl库 ①用pandas读excel文档：读取的每张工作表都是一个DataFrame ②用pandas写excel文档 3.用matplotlib进行数据展示 在此感慨：清华镜像源yyds！！！ （1）绘制直方图 （2）绘制堆叠直方图 （3）绘制对比直方图 （4）绘制折线图和散点图 （5）绘制饼图 （6）绘制热力图 热力图用于展示二维数据： （7）绘制雷达图 （8）一个窗口绘制多幅子图 十二.网络爬虫设计 1.爬虫的用途和原理 1）爬虫的用途： ①在网络上搜集数据（搜索引擎） ②模拟浏览器快速操作（如抢票，选课） ③模拟浏览器操作，替代填表等重复操作 2）最基本的爬虫写法：数据获取型爬虫的本质就是自动获取网页并抽取其中的内容 ①手工找出合适的url（网址） ②用浏览器手工查找url对应的网页，并查看网页源码，找出包含想要内容的字符串的模式 ③在程序中获取url对应的网页 ④在程序中用正则表达式或BeautifulSoup库抽取网页中想要的内容并保存 上述代码原来if语句写的是： if not(x.lower().endswith(&quot;.jpg&quot;) or x.lower().endswith(&quot;.jpeg&quot;) or x.lower().endswith(&quot;.png&quot;)): continue#只取.png和.jpg图片 但是发现爬取失败，经过查看各个图片网址可以发现，各个图片网址格式为：u=623374478,4175757280&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG (500×658) (baidu.com) 因此并不能正确得爬取图片，因此对if语句进行了修改，查找字符串中得jpeg，jpg和png用于判断是否进行爬取 同时文件名的处理也做了相应的修改，最终爬取成功！！！ 局限是requests库获取网络资源很容易被反爬虫，且不能获取JavaScript编写的动态网页源代码，可以尝试pyppeteer库，速度快且暂未被许多网站反爬 2.用pyppeteer库获取网页 pyppeteer的工作原理： ①启动一个浏览器Chromium，用浏览器装入网页 ②从浏览器可以获取网页源代码，如果网页有JavaScript程序，获取到的是JavaScript被浏览器执行后的网页源代码 ③可以向浏览器发出命令，模拟用户在浏览器上的键盘输入、鼠标点击等操作，让浏览器转到其他网页 十三.面向对象程序设计 1.类和对象的概念 类：类是用来代表事物的，对于一种事物，可以用一个类来概括其属性 对象：类的实例称为对象，类主要用于将数据和操作数据的函数捆绑在一起，便于当作一个整体使用 2.对象的比较 python中所有的类都有eq,ne等方法用于比较（注意双下划线） 自定义对象的比较： 3.继承与派生 定义一个新类B时，如果发现B类具有A类的全部特点，则可以在定义B类时以A为基类，定义B类为A类的派生类 object类：python中所有类都是object类的派生类，因而具有object类的各种属性和方法 4.静态属性和静态方法 静态属性：静态属性被所有对象所共享，一共只有一份 静态方法：静态方法不是作用在具体的某个对象上，因此不能访问非静态属性 静态属性和静态方法这种机制存在的目的就是少写全局变量和全局函数 5.对象作为集合元素或字典的键 （1）可哈希：可哈希的东西才可以作为字典的键和集合的元素，hash(x)有定义即hash(x)有返回值，说明x是可哈希的 （2）哈希值和字典、集合的关系 字典和集合都是一种称为”哈希表“的数据结构，根据元素的哈希值为元素寻找存放的槽，哈希值可以看作是槽编号，一个槽中可以放多个哈希值相同的元素。 两个对象的哈希值不同可以作为同一集合的不同元素和同一字典的不同键，当hash(a)==hash(b)但是a!=b时，a，b也可以作为同一集合的不同元素和同一字典的不同键，自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址），而非对象的值，可以重写自定义类的hash方法 若 dt 是个字典，dt[x] 计算过程如下： 根据hash(x)去找x应该在的槽的编号 如果该槽没有元素，则认为dt中没有键为x的元素 如果该槽中有元素，则试图在槽中找一个元素y，使得 y的键 == x。 如果找到，则dt[x] 即为 y的值，如果找不到，则dt[x]没定义，即 认为dt中不存在键为x的元素 （3）自定义类的对象是否可哈希 自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址） 如果为自定义的类重写了eq(self,other)成员函数，则其 hash成员函数会被自动设置为None。这种情况下，该类就变成不可哈希的 一个自定义类，只有在重写了eq方法却没有重写hash方法的情况下，才是不可哈希的。 十四.tkinter图形界面程序设计 1.控件概述 控件(widgets)： 按钮、列表框、单选框、多选框、编辑框.... 布局 ：如何将控件摆放在窗口上合适的位置 事件响应： 对鼠标点击、键盘敲击、控件被点击等操作进行响应 对话框 ：弹出一个和用户交互的窗口接受一些输入 tkinter的扩展控件： from tkinter import ttk，tk中的控件ttk都有且更加美观，并且ttk中还有一些tk不具有的控件 2.布局基础 用grid进行布局，grid布局在窗口上布置网格，控件放在网格单元里面居中摆放 默认情况下的grid规则： 1）一个单元格只能放一个控件，控件在单元格中居中摆放。 2）不同控件高宽可以不同，因此网格不同行可以不一样高，不同列也 可以不一样宽。但同一行的单元格是一样高的，同一列的单元格也 是一样宽的。 3）一行的高度，以该行中包含最高控件的那个单元格为准。单元格的 高度，等于该单元格中摆放的控件的高度（控件如果有上下留白， 还要加上留白的高度）。列宽度也是类似的处理方式。 4）若不指定窗口的大小和显示位置，则窗口大小和网格的大小一样， 即恰好能包裹所有控件；显示位置则由Python自行决定。 3.使用Frame控件进行布局 1）控件多了，要算每个控件行、列、rowspan,columnspan很麻烦 2）Frame控件上面还可以摆放控件，可以当作底板使用 3）可以在Frame控件上面设置网格进行Grid布局，摆放多个控件 4.控件属性和事件响应 （1）控件属性 1）有的控件有函数可以用来设置和获取其属性，或以字典下标的形式获取和设置其属性 2）有的控件必须和一个变量相关联，取变量值或设置变量值，就是取或设置该控件的属性 （2）事件响应 1）创建有些控件时，可以用command参数指定控件的事件响应函数 2）可以用控件的bind函数指定事件响应函数 示例： 5.Python实例：火锅店点菜系统 十五.Python游戏设计 外星人入侵主要依靠pygame库进行设计，在此仅附上代码 1.aline_invasion.py 2.game_stats.py 3.scoreboard.py 4.button.py 5.aline.py 6.ship.py 7.settings.py 8.bullet.py 十六.用opencv进行人脸识别 1.读取图片、灰度转换、修改尺寸 2.绘制图形 3.人脸检测 详解detectMultiScale： 此方法的任务是检测不同大小的对象，并返回矩形的列表。 第1个参数img需要是灰度图片 第2个参数scaleFactor，很重要 第3个参数minNeighbors，很重要 第4个参数flag=0即可 第5，6个参数minSize和maxSize 设置检测对象的最大最小值，低于minSize和高于maxSize的话就不会检测出来。参数类型为二元组，指定矩形的最小范围和最大范围 scaleFactor默认为1.1，Haar cascade的工作原理是一种“滑动窗口”的方法，通过在图像中不断的“滑动检测窗口”来匹配人脸。 因为图像的像素有大有小，图像中的人脸因为远近不同也会有大有小，所以需要通过scaleFactor参数设置一个缩小的比例，对图像进行逐步缩小来检测，这个参数设置的越大，计算速度越快，但可能会错过了某个大小的人脸。可以根据图像的像素值来设置此参数，像素大缩小的速度就可以快一点，通常在1~1.5之间。 minNeighbors默认为3，指定每个候选矩形有多少个“邻居”，指定每个候选矩形有多少个“邻居”，也可以理解为检测次数，只有检测minNeighbors次，某处都识别为人脸才会显示出来矩形 4.视频人脸检测 5.人脸录入 6.数据训练 7.人脸识别 十七.scikit-learn实现机器学习 1.机器学习介绍及其原理 ①人工智能：就其本质而言，是及其对人思维信息过程的模拟，让它能像人一样去思考 人工智能可以根据输入信息进行模型结构，权重更新，实现最终优化 特点：信息处理，自我学习，优化升级 ②人工智能的核心方法：机器学习，深度学习 机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术 机器学习：使用算法来解析数据，从中学习，然后对真实世界中的事件进行决策和预测 深度学习：模仿人类的神经网络，建立模型，进行数据分析 ③机器学习分类： 监督式学习：基于数据及结果进行预测 非监督式学习：从数据中挖掘关联性 强化学习：强调如何基于环境而行动以获取最大利益 ④监督式学习核心步骤： 1.使用标签数据训练机器学习模型 ●&quot;标签数据&quot; 是指由输入数据对应的正确的输出结果 ●&quot;机器学习模型&quot;将学习输入数据与之对应的输出结果间的函数关系 2.调用训练好的机器学习模型,根据新的输入数据预测对应的结果 相比于监督式学习,非监督式学习不需要标签数据,而是通过引入预先设定的优化准则进行模型训练,比如自动将数据分为三类 2.机器学习开发环境部署 1）Scikit-learn：是python语言中专门针对机器学习应用而发展起来的开源框架，可以实现数据预处理，分类，回归，降维，模型选择等常用的机器学习算法 2）Jupyter notebook 3）Anaconda 3.机器学习实现之数据预处理 1）Iris数据集： 2）使用scikit-learn进行数据处理的四个关键点 ①区分开属性数据和结果数据 ②属性数据和结果数据都是量化的 ③运算过程中，属性数据与结果数据的类型都是numpy数组 ④属性数据与结果数据的维度是对应的，即行数应该对应 4.机器学习实现之模型训练 1）分类：根据数据集目标的特征或属性，划分到已有的类别中 2）常用的算法：K近邻（KNN），逻辑回归，决策树，朴素贝叶斯 3）K近邻分类模型（KNN）：给定一个训练数据集，对新的输入实例，在训练的数据集中找到与该实例最邻近的K个实例，这个K实例的多数属于某一类，就把该输入实例归到这个类中 4）使用scikit-learn进行建模的四个步骤： ①调用需要使用的模型类 ②模型初始化：创建一个模型实例 ③模型训练 ④模型预测 5.机器学习实现之模型评估 1）为了对模型的准确率进行评估，我们不能用全部的数据进行训练，因为这样无法判断模型的准确性，不妨将数据集分为训练集与测试集两部分，一部分用于训练，一部分用于测试，这样便于对模型进行评估，下面是不进行数据分离的结果，显然K=1时，正确率100% 2）下面进行数据分离然后进行模型评估 3）确定最合适的K值，遍历所有可取的K值 K值越小，模型越复杂，并且因为每次数据分离是随机的，训练出的结果也不相同 6.逻辑回归模型 1）逻辑回归模型：用于解决分类问题的一种模型。根据数据特征或属性,计算其归属于某一类别的概率P(x) ,根据概率数值判断其所属类别。主要应用场景: 二分类问题。 2）皮马印第安人糖尿病数据集 注意数据集的特点是有缺失数据 3）使用准确率进行模型评估的局限性：没有体现数据的实际分布情况，没有体现模型错误预测的类型 空准确率：当模型总是预测比较高的类别，其预测准确率的数值，即比例较高的类别在模型中所占的比例 4）混淆矩阵 比F1更具一般形式的Fβ′:Fβ′=(1+β2)×P×R(β2×P)+R①β=1:标准的F1②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2\\begin{aligned} &amp;比F_1更具一般形式的F_{\\beta&#x27;}:\\\\ &amp;F_{\\beta&#x27;}=\\frac{(1+\\beta^2)\\times P\\times R}{(\\beta^2\\times P)+R}\\\\ &amp;①\\beta=1:标准的F_1\\\\ &amp;②\\beta&lt;1:偏精确率P\\\\ &amp;③\\beta&gt;1:偏召回率R\\\\ &amp;\\beta最常用的值为0.5，1，2 \\end{aligned} ​比F1​更具一般形式的Fβ′​:Fβ′​=(β2×P)+R(1+β2)×P×R​①β=1:标准的F1​②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2​ 计算混淆矩阵进行模型评估 ","link":"https://2006wzt.github.io/post/Python入门与实战/"}]}