{"posts":[{"title":"可靠数据传输协议实验","content":"可靠数据传输协议Rdt 一、实验要求 1.1 实验目标和内容 1.1.1 实验目的 通过该实验了解和掌握运输层可靠数据传输原理以及具体实现方法。 1.1.2 实验环境 ● 语言工具：基于 C++语言实现。 ● 操作系统：Windows、Linux 操作系统。 ● 其它要求：基于模拟网络环境 API 实现。 1.1.3 实验要求 ● 可靠运输层协议实验只考虑单向传输，即：只有发送方发生数据报文，接收方仅仅接收报文并给出确认报文。 ● 要求实现具体协议时，指定编码报文序号的二进制位数（例如 3 位二进制编码报文序号）以及窗口大小（例如大小为 4），报文段序号必须按照指定的二进制位数进行编码。 ● 代码实现不需要基于 Socket API，不需要利用多线程，不需要任何 UI 界面。 ● 提交实验设计报告和源代码；实验设计报告必须按照实验报告模板完成，源代码必须加详细注释。 ● 代码编译运行成功后，运行给实验指导老师或者助教检查。 1.1.4 实验内容 本实验包括三个级别的内容，具体包括： ● 实现基于 GBN 的可靠传输协议，分值为 50%。 ● 实现基于 SR 的可靠传输协议，分值为 30%。 ● 在实现 GBN 协议的基础上，根据 TCP 的可靠数据传输机制实现一个简化版的 TCP协议，分值 20%，要求： ● 报文段格式、接收方缓冲区大小和 GBN 协议一样保持不变； ● 报文段序号按照报文段为单位进行编号； ● 单一的超时计时器，不需要估算 RTT 动态调整定时器 Timeout 参数； ● 支持快速重传和超时重传，重传时只重传最早发送且没被确认的报文段； ● 确认号为收到的最后一个报文段序号； ● 不考虑流量控制、拥塞控制 1.2 检查表 二、模拟网络环境介绍 由于需要模拟真实网络环境下实际可能发生的丢包、报文损坏的情况，因此开发了一个模拟的网络环境。模拟网络环境模拟实现了应用层和网络层，而需要和学生实现的运输层Rdt 协议协同工作完成数据的可靠传输 2.1 模拟网络环境功能 模拟的网络环境实现了以下功能： ● 应用层的数据向下递交给发送方运输层 Rdt 协议； ● 接收方运输层 Rdt 协议收到差错检测无误的报文后向上层应用层递交； ● 将发送方运输层 Rdt 协议准备好的报文通过网络层递交给接收方，在递交过程中按一定的概率会产生丢包、报文损坏； ● 定时器的启动、关闭、定时器 Timeout 后通知发送方运输层 Rdt 协议 2.2 模拟网络环境架构 图 1 给出了模拟网络环境架构和学生实现的 Rdt 协议的之间的关系。二者之间需要协同工作。蓝色背景部分为模拟网络环境，橙色背景部分为 Rdt 协议的发送方（RdtSender）和接收方（RdtReceiver）。红色箭头表示 RdtSender 和 RdtReceiver 调用模拟网络环境的函数，黑色箭头表示模拟环境调用 RdtSender 和 RdtReceiver 的函数。 模拟网络环境实现了以下函数以供 Rdt 协议调用：delivertoAppLayer(Message) 、sendToNetworkLayer(Packet) 、startTimer(int seqNum)、stopTimer(int seqNum)，这些函数功能将在 2.3 节介绍。 Rdt 协议的发送方 RdtSender 和 Rdt 协议的接收方则是学生需要实现的功能，它们共同实现了 Rdt 协议。其中 RdtSender 必须实现三个函数：send(Message)、receive(Packet)、timeoutHandler(int seqNum)；RdtReceiver 则必须实现一个函数：receive(Packet)，这些函数的功能将在 2.3 节介绍。 2.3 模拟网络环境与学生实现的代码之间的调用关系 模拟网络环境模拟实现了应用层和网络层，而需要和学生实现的运输层 Rdt 协议协同工作完成数据的可靠传输。从应用层有数据到来开始，它们之间的调用关系如下所述： ① 模拟网络环境模拟产生应用层数据，调用 Rtdsender 的 Send(Message)方法； ② RtdSender 的 Send(Message)方法调用模拟网络环境的 sendToNetworkLayer(Packet)方法，将数据发送到模拟网络环境的网络层； ③ RtdSender 的 Send(Message)方法调用模拟网络环境的 startTimer( )方法启动定时器； ④ 模拟网络环境调用 RdtReceiver 的 receive(Packet)方法将数据交给 RdtReceiver； ⑤ 如果校验正确，RdtReceiver 调用模拟网络环境的 delivertoAppLayer(Message)方法将数据向上递交给应用层； ⑥ RdtReceiver 调用模拟网络环境的 sendToNetworkLayer(Packet)方法发送确认； ⑦ 模拟网络环境调用 RtdSender 的 receive(Packet)方法递交确认给 RtdSender； ⑧ 如果确认正确，RtdSender 调用模拟网络环境的 stopTimer 方法关闭定时器； ⑨ 如果确认不正确，RtdSender 调用模拟网络环境的 startTimer 方法重启定时器； ⑩ 如果定时器超时，模拟网络环境调用 RtdSender 的 timeoutHandler( )方法。 三、数据结构、接口定义与模拟网络环境 API 介绍 3.1 数据结构定义 DataStructure.h 头文件里定义了应用层消息 Message 和运输层 Packet 类，同时定义了基本的参数，具体如下： **（1）Configuration：**配置类 **（2）Message：**第五层应用层消息 **（3）Packet：**第四层运输层报文段 3.2 RdtSender 和 RdtReceiver 接口定义 在头文件 RdtSender.h 和 RdtReceiver.h 里分别定义 Rdt 协议发送方和接收方必须实现的接口，是通过抽象类 RdtSender 和 RdtReceiver 进行接口的定义，具体分别如下： （1）RdtSender： 这里需要特别说明的是： （1）timeoutHandler 方法的参数 seqNum 为和该定时器关联的 Packet 的序号。虽然对于StopWait、GBN、简化版的 TCP 协议只需要一个定时器，但是该函数还是需要一个序号参数，只不过该序号是最早发出但没有被确认的 Packet 的序号。但是如果实现 SR 协议，那么该参数就有具体的意义了：它指明了是哪个 Packet 的定时器超时了。 （2）getWaitingState 函数返回 RdtSender 是否处于等待状态，对于具体的 Rdt 协议，是否处于等待状态具有不同的含义：例如对于 StopWait 协议，当发送方等待上层发送的 Packet的确认时，getWaitingState 函数应该返回 true；对于 GBN 协议，当发送方的发送窗口满了时，getWaitingState 函数应该返回 true。定义这个接口方法的原因是模拟网络环境需要调用RdtSender 的这个方法来判断是否需要将应用层下来的数据递交给 Rdt，这样学生实现RdtSender 时不需要在内部维护一个 Packet 队列了，因为当 getWaitingState 返回 true 时，应用层不会有数据下来。 （2）RdtReceiver： RdtReceiver 的接口定义就简单的多，这里不再解释。具体的 Rdt 协议实现类必须继承 这二个抽象类，这样就保证了不同的 Rdt 协议接口一致性。否则具体 Rdt 协议实现类的对象无法注入到模拟网络环境一起协同工作 3.3 模拟网络环境 API 接口定义 模拟网络环境 API 接口定义了学生实现的具体 Rdt 协议实现类可以调用的函数，具体定义在 NetworkService.h 头文件中定义： 这里需要特别说明的是： （1）RandomEventTarget 是定义的枚举类型（具体定义在 3.4 里说明），用来标识发送方和接收方。 当调用 startTimer 和 stopTimer 时 ， 该参数设 为 SENDER ； 当调用sendToNetworkLayer 方法时，该参数设为对方，即如果是发送方调用 sendToNetworkLayer 方法时发送数据报文，该参数设为 RECEIVER；如是接收方调用 sendToNetworkLayer 方法时发送确认报文，该参数设为 SENDER；当接收方调用 delivertoAppLayer 方法时，该参数设为 RECEIVER （2）startTimer 和 stopTimer 方法都需要设置 seqNum 参数，该参数应该是和该定时器相关的 Packet 序号。即使是 GBN 和简化版 TCP 这些协议，也要设置该参数（应为最早发出但未确认的 Packet 序号）。另外重新启动一个定时器前，一定要先关闭该定时器（要注意 seqNum 参数的一致性），否则模拟网络环境会提示“试图启动一个已启动的定时器”。 （3）setInputFile 和 setOutputFile 是增加的二个接口函数，设置输入文件和输出文件的路径。这是为了验证协议的正确性而添加的：模拟网络环境的发送方会读取文件，构造应用层的 Message，调用 RdtSender 的 send 方法将 Message 发送到接收方，接收 RdtReceiver 再将正确收到的报文调用 delivertoAppLayer 方法交给接收方模拟网络环境的应用层，最后写入到输出文件。如果输入文件和输出文件内容一样，说明协议工作正确。测试用的输入文件请使用发布的 input.txt 文件，如果协议工作正确，程序会产生输出文件，并且输出文件和输入文件的内容一致。 （4）setRunMode 函数也是增加的接口函数，用于设置网络模拟环境的运行模式。如果设置 mode=0（也是该函数的缺省参数），则为 Verbose 模式，网络模拟环境会输出很多模拟环境的运行信息，可以帮助学生观察协议的工作过程，特别是协议实现有问题时，可以帮助 分析协议出现的问题；如果设置为 mode=1，则为 Silence 模式，这是会关闭掉模拟环境输出的运行信息，而控制台只会输出学生协议实现代码里打印的信息。 3.4 其它定义 在 RandomEventEnum.h 头文件里定义了枚举类型，用来标识发送方和接收方。其定义为： 在 Tool.h 头文件里，定义了可以使用的工具接口，其定义为： 其中 void printPacket(const char * description, const Packet &amp;packet)函数可以用来打印调试信息， 第一 个参数为描述性字符串， 第二个 参 为要 打印 输出 的Packet； int calculateCheckSum(const Packet &amp;packet)函数计算给定 Packet 的校验和；double random()函 数是模拟网络环境所需的。这里要特别说明的是校验和的计算请调用 Tool 接口定义的方法。 在 Global.h 里声明了二个全局指针，分别指向实现了 Tool 接口和 NetworkService 接口的实例，学生代码中通过这二个指针调用工具接口提供的函数和模拟网络环境提供的函数。由于这二个指针没有封装在智能指针里，学生需要在 main 函数结束前 delete 这二个指针。 四、停止等待协议（Rdt3.0）实现示例 4.1 开发环境配置 （1）link 模拟网络环境静态库： 模拟网络环境已经被编译成静态库的 lib 文件（文件名为 netsimlib.lib），因此学生的代码工程编译时需要 link 这个 lib 文件。以 Windows 平台下的 VS2017 为例，可以有很多办法link 一个自定义的静态库 lib 文件，在 StopWait 示例工程中，是在 stdAfx.h 头文件里加上如下的编译预处理指令将静态库链接在一起： （2）需要包含的头文件： 在学生代码工程里，需要包含以下头文件： ● DataStructure.h ● Global.h ● NetworkService.h ● RandomEventEnum.h ● RdtReceiver.h ● RdtSender.h ● Tool.h 另外还需要如下常规的头文件： ● stdio.h ● string.h ● iostream 4.2 停止等待协议发送方实现 停止等待协议发送方是通过类 StopWaitRdtSender 实现，而 StopWaitRdtSender 继承了抽象类 RdtSender，具体定义如下： StopWaitRdtSender 类的函数实现具体为： 4.3 停止等待协议接收方实现 停止等待协议发送方是通过类 StopWaitRdtReceiver 实现，而 StopWaitRdtReceiver 继承了抽象类 RdtReceiver，具体定义如下： StopWaitRdtReceiver 类的函数实现具体为： 4.4 启动模拟网络环境 模拟网络环境的启动在 main 函数里实现，具体代码为： 五、GBN协议的实现 5.1 GBN协议的原理 允许发送方发送多个分组而不需等待确认，但已发送但未确认的分组数不能超过N 发送方： ① 分组首部用k-比特字段表示序号（二进制） ② 已被传输但还未确认的分组的许可序号范围可以看作是一个在序号范围内大小为N的“窗口(window)” 基序号（base）：最早的未确认的分组序号 下一个序号（nextseqnum）：下一个待发分组的序号 ① [0,base−1]: ~[0,base-1]:~ [0,base−1]: 以被确认的分组 ② [base,nextseqnum−1]: ~[base,nextseqnum-1]:~ [base,nextseqnum−1]: 已发送但未确认的分组 ③ [nextseqnum,base+N−1]: ~[nextseqnum,base+N-1]:~ [nextseqnum,base+N−1]: 未发送但可以被发送的分组 ④ 大于等于 base+N ~base+N~ base+N 的分组：不能发送 累加确认： ACK(n) ~ACK(n)~ ACK(n) 对序号n之前包括n在内的所有分组进行确认 ACK-only：只对正确按序到达的分组发送ACK 超时重发：为最早已发送但未确认的分组设置定时器（只需要一个定时器） 为什么要限制滑动窗口大小：为了流量控制 接收方： 失序分组或损坏分组: ① 丢弃 (不缓存) -&gt; 接收方无缓存! ② 重发正确按序到达的最高序号分组的ACK ③ 每次发送的ACK一定是对正确按序到达的最高序号分组的确认 5.2 GBN协议发送方实现 stdafx.h 实现如下： 首先定义发送方的类 GBNSender，实现 GBNSender.h 接着要根据FSM实现相应的成员函数，即编写文件 GBNSender.cpp ： 5.3 GBN协议接收方实现 首先定义接收方的类 GBNReceiver，即编写文件 GBNReceiver.h： 接着我们实现各个成员函数的定义，即编写文件 GBNReceiver.cpp： 接收方基本是与Rdt3.0相同的 5.4 GBN主模块编写 最终运行结果的部分截图如下： 六、SR协议的实现 6.1 SR协议原理 ① 选择重传：解决GBN大量重传分组的问题 ② 接收方逐个对所有正确收到（即使失序）的分组进行确认（不是累积确认） ③ 对接收到的（失序）分组进行缓存（GBN不缓存）, 以便最后对上层进行有序递交 ④ 发送方只重发怀疑丢失或损坏的分组：发送方为每一个没有收到ACK的分组设置定时器 ⑤ 发送窗口：大小为N，范围**[sendbase, sendbase + N - 1]，限制已发送但未被确认的分组数最多为N，sendbase前的分组都被确认 ⑥ 接受窗口：大小为N，范围[recvbase, recvbase + N - 1]**，落在窗口内的序号都是期待收到的分组序号，recvbase前都是按序到达，已发出确认，且已递交给上层 注意上述粉色条的注释 “失序（已缓存）但未被确认” 应改为 “已收到但失序（已缓存）” （1）发送方： ① 从上层收到数据： 如果下一个可用于该分组的序号在窗口内，则将数据打包并发送。否则要么缓存，要么拒绝上层 ② 超时(n)，序号为n的分组超时： 重传分组n，重置定时器 ③ 收到 ACK(n) ~ACK(n)~ ACK(n) 在 [sendbase,sendbase+N-1]范围内： 标记分组 n 为已接收 ④ 如果n是发送窗口基序号sendbase： 则将窗口基序号前推到下一个未确认序号（因此，sendbase前的分组一定都被确认过了），如果不是窗口不动 ⑤ 发送方可能会收到比sendbase还小的序号的分组确认： 这时什么都不做（因为比sendbase还早的分组都被确认） 为什么发送方会收到比sendbase还小的分组确认？——定时太短 例如：窗口位于sendbase-1时，序号为sendbase-1的分组定时器时间到还没收到ACK(sendbase-1)，则发送方重发该分组。 刚重发，收到了Ack(sendbase-1),窗口前移到sendbase，窗口移动后，又收到了Ack(sendbase-1) （2）接收方： ① 分组序号n在[rcvbase, rcvbase+N-1]范围内 （ⅰ）分组正确接收，发送n的确认ACK(n)（不管是否为重复分组及是否失序），如果分组是以前没收到过的：将其缓存 （ⅱ）如果该分组序号=recvbase，则将从该分组开始序号连续的分组一起交给上层，然后，窗口按向上交付的分组的数量向前移动 ② 分组序号n 在 [rcvbase-N,rcvbase-1]范围内： 虽然曾经确认过，仍再次发送n的确认ACK(n)，若不发确认，发送方窗口无法向前移动 ③ 其他情况：忽略该分组 为什么接收方会收到[recvbase-N，recvbase - 1]范围内的分组？并且必须给出确认？ 因为 ACK(n) ~ACK(n)~ ACK(n) 可能会丢失。假设接受方按序收到N个分组，向发送方发送确认后接受窗口向前移动N位，但是确认分组全部丢失，导致发送方重发，发送方最多只能发N个，因此接收方会收到[recvbase-N，recvbase - 1]范围内的分组，接收方这时必须给出确认，否则发送方窗口无法向前移动 为什么接收方收到比recvbase-N更早的分组后不用发确认了？ 因为比recvbase-N更早的分组（如recvbase-N-1 ），发送方一定收到确认了。 当接受窗口位于recvbase时，意味着接收方一定按序收到了从[recvbase-N, recvbase - 1]的分组，这意味着发送方窗口一定到了recvbase-N，因此发送方一定收到了比recvbase-N更早的确认 6.2 SR协议发送方实现 stdafx.h： SRSender.h： SRSender.cpp： 6.3 SR协议接收方实现 SRReceiver.h： SRReceiver.cpp： 6.4 SR主模块实现 SR.cpp： 部分输出结果如下： 七、TCP协议实现 本次实验的实现要求如下： ● 报文段格式、接收方缓冲区大小和 GBN 协议一样保持不变； ● 报文段序号按照报文段为单位进行编号； ● 单一的超时计时器，不需要估算 RTT 动态调整定时器 Timeout 参数； ● 支持快速重传和超时重传，重传时只重传最早发送且没被确认的报文段； ● 确认号为收到的最后一个报文段序号； ● 不考虑流量控制、拥塞控制 7.1 TCP原理 TCP在IP的不可靠服务基础上提供可靠数据传输服务： ① 流水线方式发送报文段 ② 累积确认：只确认最后一个正确按序到达的报文段 ③ TCP使用一个重传定时器（最早未确认的报文段） 从上面几点看，TCP采用的机制很像GBN 处理数据流程： ① 从应用程序接收数据 ② 将数据封装入报文段中，每个报文段都包含一个序号，序号是该报文段第一个数据字节的字节流编号 ③ 启动定时器，超时间隔: TimeOutInterval ④ 超时: 重传认为超时的报文段（最早未确认的报文段），重启定时器 ⑤ 收到Ack: 如果是对以前的未确认报文段的确认(Ack&gt;Sendbase)，更新SendBase，如果当前有未被确认的报文段，TCP要重启定时器 快速重传：超时周期往往太长，增加重发丢失分组的延时，通过重复的ACK检测丢失报文段，在超时到来之前重传报文段 ① 发送方常要连续发送大量报文段，如果一个报文段丢失，会引起很多连续的重复ACK ② 如果发送方收到一个数据的3个重复ACK，它会认为确认数据之后的报文段丢失 我们本个实验实现的TCP与实际的TCP还是有区别的，该TCP几乎与GBN协议相同，但是 7.2 TCP发送方实现 stdafx.h： TCPSender.h： TCPSender.cpp： 7.3 TCP接收方实现 TCPReceiver.h： TCPReceiver.cpp： 7.4 TCP主模块 TCP.cpp： ","link":"https://2006wzt.github.io/post/可靠数据传输协议/"},{"title":"数据库系统实验","content":"数据库实验 一、实训1 数据库、表与完整性约束的定义(Create) 1.1 创建数据库 创建2022年北京冬奥会信息系统数据库 1.2 创建表及表的主码约束 建数据库TestDb，在TestDb下创建表t_emp，表结构如下： 字段名称 数据类型 备注 id INT 员工编号，主码 name VARCHAR(32) 员工名称 deptId INT 所在部门标号 salary FLOAT 工资 1.3 创建外码约束(foreign key) 设有以下两张表： dept(部门) 字段名称 数据类型 备注 deptNo INT 部门号，主键 deptName VARCHAR(32) 部门名称 staff(职工) 字段名称 数据类型 备注 staffNo INT 职工号，主键 staffName VARCHAR(32) 职工姓名 gender CHAR(1) 性别，F-女，M-男 dob date 出生日期 salary numeric(8,2) 工资 deptNo INT 部门号,外键 请创建上述两个表，为表定义主键，并给表staff创建外键，这个外键约束的名称为FK_staff_deptNo。在创建表之前你可能需要先创建数据库：MyDb，并且将两张表创建在MyDb数据库中。不需考虑关于性别的约束。（注意：如果你在实验1之后接着作本实验，数据库MyDb可能已经存在） 1.4 CHECK约束 表products的结构如下： 字段名称 数据类型 备注 pid char(10) 产品户ID,主码 name varchar(32) 产品名称 brand char(10) 品牌，只能是('A','B')中的某一个 price int 价格，必须&gt;0 请在数据库MyDb中创建表products，并分别实现对品牌和价格的约束，两个CHECK约束的名称分别为CK_products_brand和CK_products_price，主码约束不要显示命名。（提示：如果数据库MyDb不存在，请首先创建它，并将它作为工作数据库。） 1.5 DEFAULT约束 表hr的结构如下： 字段名称 数据类型 备注 id char(10) 工号,主码 name varchar(32) 姓名,不允许为空值 mz char(16) 民族, 缺省值为“汉族” 请在数据库MyDb中创建表hr，并实现name列的NOT NULL约束和mz列的Default约束（别忘了主码约束） 1.6 UNIQUE约束 表s的结构如下： 字段名称 数据类型 备注 sno char(10) 学号,主码 name varchar(32) 姓名,不允许为空值 ID char(18) 身份证号, 不允许有两个相同的身份证号 请在数据库MyDb中创建表s，并实现相关约束（主码，NOT NULL和Unique约束）。 注意表名s是小写的，列名ID是全大写的。如果没有数据库MyDb,你需要创建它，并使其成为当前工作数据库。 二、实训2 表结构与完整性的修改(ALTER) 2.1 修改表名 数据库TestDb1中有表your_table，请根据提示，在右侧代码文件编辑窗中添加恰当的语句，将表名your_table更改为my_table。 2.2 添加与删除字段 假设数据库MyDb中有表order(订单)和orderDetail(订单明细) 等表，两表的结构分别如下： order表 字段名称 数据类型 备注 orderNo char(12) 订单号，主码 orderDate date 订购日期 customerNo char(12) 客户编号，外码，与customer.customerNo对应 employeeNo char(12) 雇员工号，外码，与employee.employeeNo对应 orderDetail表 字段名称 数据类型 备注 orderNo char(12) 订单号，主属性，外码，与order.orderNo对应 productNo char(12) 产品编号，主属性，外码，与product.productNo对应 quantityOrdered int 订购数量 orderDate date 订购日期 注：表orderDetail的主码由(orderNo,productNo)组成 编程的任务是对orderDetail表进行修改： orderDetail表的orderDate列明显多余，因为同一订单中的每一笔交易都发生在同一天，这个日期在订单主体表order中已有记录，请删除列orderDate。 产品的单价是订单明细需要记录的内容，请在orderDetail中添加列unitPrice以记录产品的单价: 字段名称 数据类型 备注 unitPrice numeric(10,2) 产品的成交单价 请根据提示，在右侧代码文件编辑窗中添加恰当的语句，实现上述编程任务。 2.3 修改字段 数据库MyDb中有表addressBook(通信录)，结构如下： 字段名称 数据类型 备注 serialNo int 自动编号，主码 name char(32) 姓名 company char(32) 工作单位 position char(10) 职位 workPhone char(16) 办公电话 mobile char(11) 手机 QQ int QQ号 weixin char(12) 微信号 当初创建表的语句如下： 你的编程任务是对表addressBook作以下修改： 将QQ号的数据类型改为char(12); 将列名weixin改为wechat。 请根据提示，在右侧代码文件test3.sql编辑窗中添加恰当的语句，实现上述编程任务。 2.4 添加、删除与修改约束 数据库MyDb中有以下两表： Dept(部门) 字段名称 数据类型 备注 deptNo INT 部门号，主键 deptName VARCHAR(32) 部门名称，不同部门不允许重名 tel char(11) 部门电话 mgrStaffNo int 部门经理的工号，外码 Staff(职工) 字段名称 数据类型 备注 staffNo INT 工号，主键 staffName VARCHAR(32) 职工姓名 gender CHAR(1) 性别，取值范围：F-女，M-男 dob date 出生日期 Salary numeric(8,2) 工资 dept INT 部门号,外键 现通过以下语句，完成了两表的基础创建工作(部分约束没有实现): 请在右侧代码编辑窗对应位置写出适当的语句，完成以下工作： (1) 为表Staff添加主码; (2) Dept.mgrStaffNo是外码，对应的主码是Staff.staffNo,请添加这个外码，名字为FK_Dept_mgrStaffNo; (3) Staff.dept是外码，对应的主码是Dept.deptNo. 请添加这个外码，名字为FK_Staff_dept; (4) 为表Staff添加check约束，规则为：gender的值只能为F或M；约束名为CK_Staff_gender; (5) 为表Dept添加unique约束：deptName不允许重复。约束名为UN_Dept_deptName. 三、实训3 数据查询(Select)之一 3.1 查询客户主要信息 本实训采用的是某银行的一个金融场景应用的模拟数据库，数据库中表，表结构以及所有字段的说明如下： 表1 client(客户表) 字段名称 数据类型 约束 说明 c_id INTEGER PRIMARY KEY 客户编号 c_name VARCHAR(100) NOT NULL 客户名称 c_mail CHAR(30) UNIQUE 客户邮箱 c_id_card CHAR(20) UNIQUE NOT NULL 客户身份证 c_phone CHAR(20) UNIQUE NOT NULL 客户手机号 c_password CHAR(20) NOT NULL 客户登录密码 表2 bank_card(银行卡) 字段名称 数据类型 约束 说明 b_number CHAR(30) PRIMARY KEY 银行卡号 b_type CHAR(20) 无 银行卡类型(储蓄卡/信用卡) b_c_id INTEGER NOT NULL FOREIGN KEY 所属客户编号,引用自client表的c_id字段。 b_balance NUMERIC(10,2) NOT NULL 余额,信用卡余额系指已透支的金额 表3 finances_product(理财产品表) 字段名称 数据类型 约束 说明 p_name VARCHAR(100) NOT NULL 产品名称 p_id INTEGER PRIMARY KEY 产品编号 p_description VARCHAR(4000) 无 产品描述 p_amount INTEGER 无 购买金额 p_year INTEGER 无 理财年限 表4 insurance(保险表) 字段名称 数据类型 约束 说明 i_name VARCHAR(100) NOT NULL 保险名称 i_id INTEGER PRIMARY KEY 保险编号 i_amount INTEGER 无 保险金额 i_person CHAR(20) 无 适用人群 i_year INTEGER 无 保险年限 i_project VARCHAR(200) 无 保障项目 表5 fund(基金表) 字段名称 数据类型 约束 说明 f_name VARCHAR(100) NOT NULL 基金名称 f_id INTEGER PRIMARY KEY 基金编号 f_type CHAR(20) 无 基金类型 f_amount INTEGER 无 基金金额 risk_level CHAR(20) NOT NULL 风险等级 f_manager INTEGER NOT NULL 基金管理者 表6 property(资产表) 字段名称 数据类型 约束 说明 pro_id INTEGER PRIMARY KEY 资产编号 pro_c_id INTEGER NOT NULL FOREIGN KEY pro_pif_id INTEGER NOT NULL 业务约束 pro_type INTEGER NOT NULL 商品类型:1表示理财产品;2表示保险;3表示基金 pro_status CHAR(20) 无 商品状态:'可用','冻结' pro_quantity INTEGER 无 商品数量 pro_income INTEGER 无 商品收益 pro_purchase_time DATE 无 购买时间 请用一条SQL语句完成以下查询任务： 查询所有客户的名称、手机号和邮箱信息。查询结果按照客户编号排序。 3.2 邮箱为null的客户 请用一条SQL语句完成以下查询任务： 查询客户表(client)中没有填写邮箱信息(邮箱字段值为null)的客户的编号、名称、身份证号、手机号。 3.3 既买了保险又买了基金的客户 请用一条SQL语句完成以下查询任务： 查询既买了保险又买了基金的客户的名称、邮箱和电话,结果依客户编号排序。 3.4 办理了储蓄卡的客户信息 请用一条SQL语句完成以下查询任务： 查询办理了储蓄卡的客户名称、手机号和银行卡号。注意一个客户在本行可能不止一张储蓄卡，应全列出。查询结果结果依客户编号排序。 3.5 每份金额在30000～50000之间的理财产品 请用一条SQL语句完成以下查询任务： 查询理财产品中每份金额在30000～50000之间的理财产品的编号,每份金额，理财年限，并按金额升序排序，金额相同的按照理财年限降序排序。 3.6 商品收益的众数 请用一条SQL语句完成以下查询任务： 众数是一组数据中出现次数最多的数值，有时众数在一组数中会有好几个。查询资产表中所有资产记录里商品收益的众数和它出现的次数，出现的次数命名为presence。 3.7 未购买任何理财产品的武汉居民 请用一条SQL语句完成以下查询任务： 已知身份证前6位表示居民地区，其中4201开头表示湖北省武汉市。查询身份证隶属武汉市没有买过任何理财产品的客户的名称、电话号、邮箱。依客户编号排序 3.8 持有两张信用卡的用户 请用一条SQL语句完成以下查询任务： 查询持有两张(含）以上信用卡的用户的名称、身份证号、手机号。查询结果依客户编号排序 3.9 购买了货币型基金的客户信息 请用一条SQL语句完成以下查询任务： 查询购买了货币型(f_type='货币型')基金的用户的名称、电话号、邮箱。依客户编号排序 3.10 投资总收益前三名的客户 请用一条SQL语句完成以下查询任务： 查询当前总的可用资产收益(被冻结的资产除外)前三名的客户的名称、身份证号及其总收益，按收益降序输出，总收益命名为total_income。不考虑并列排名情形。 3.11 黄姓客户持卡数量 请用一条SQL语句完成以下查询任务： 给出黄姓用户的编号、名称、办理的银行卡的数量(没有办卡的卡数量计为0). 按办理银行卡数量降序输出,持卡数量相同的,依客户编号排序。 3.12 客户理财、保险与基金投资总额 请用一条SQL语句完成以下查询任务： 综合客户表(client)、资产表(property)、理财产品表(finances_product)、保险表(insurance)和基金表(fund)，列出客户的名称、身份证号以及投资总金额（即投资本金，每笔投资金额=商品数量*该产品每份金额)，注意投资金额按类型需查询不同的表， 投资总金额是客户购买的各类(理财,保险,基金)资产投资金额的总和，总金额命名为total_amount。查询结果按总金额降序排序。 3.13 客户总资产 请用一条SQL语句完成以下查询任务： 综合客户表(client)、资产表(property)、理财产品表(finances_product)、保险表(insurance)、基金表(fund)，列出所有客户的编号、名称和总资产，总资产命名为total_property。总资产为储蓄卡总余额，投资总额，投资总收益的和，再扣除信用卡透支的总金额(信用卡余额即为透支金额)。客户总资产包括被冻结的资产。 3.14 第N高问题 请用一条SQL语句实现本询要求： 查询每份保险金额第4高保险产品的编号和保险金额。在数字序列8000,8000,7000,7000,6000中，两个8000均为第1高，两个7000均为第2高,6000为第3高。 3.15 基金收益两种方式排名 排名问题： 排名是数据库应用中的一类经典问题，实际又根据具体需求细分为3种场景： 连续排名，同数不同名次，无并列名次。例如300、200、200、150、150、100的排名结果为1, 2, 3, 4, 5, 6。这种排名类似于编号； 同数同名次，总排名不连续。例如300、200、200、150、150、100的排名结果为1, 2, 2, 4, 4, 6； 同数同名次，总排名连续。例如300、200、200、150、150、100的排名结果为1, 2, 2, 3, 3, 4。 不同的需求对应着不同的查询策略。本关的任务是实现后两种方式的排名。 编程要求： 请分别用两条SQL语句实现下述两个任务： 查询资产表中客户编号，客户基金投资总收益,基金投资总收益的排名(从高到低排名)。总收益相同时名次亦相同(即并列名次)。总收益命名为total_revenue, 名次命名为rank。第一条SQL语句实现全局名次不连续的排名，第二条SQL语句实现全局名次连续的排名。不管哪种方式排名，收益相同时,客户编号小的排在前 3.16 持有完全相同基金组合的客户 请用一条SQL语句实现查询任务： 查询持有相同基金组合的客户对，如编号为A的客户持有的基金，编号为B的客户也持有，反过来，编号为B的客户持有的基金，编号为A的客户也持有，则(A,B)即为持有相同基金组合的二元组，请列出这样的客户对。为避免过多的重复，如果(1,2)为满足条件的元组，则不必显示(2,1)，即只显示 编号小者在前的那一对，这一组客户编号分别命名为c_id1,c_id2 3.17 购买基金的高峰期 请用一条SQL语句实现查询任务： 查询2022年2月购买基金的高峰期。至少连续三个交易日，所有投资者购买基金的总金额超过100万(含)，则称这段连续交易日为投资者购买基金的高峰期。只有交易日才能购买基金,但不能保证每个交易日都有投资者购买基金。2022年春节假期之后的第1个交易日为2月7日,周六和周日是非交易日，其余均为交易日。请列出高峰时段的日期和当日基金的总购买金额，按日期顺序排序。总购买金额命名为total_amount。 （破防！切身感受到了课上讲的和实际要你做的之间的天壤之别） 3.18 至少有一张信用卡余额超过5000元的客户信息 请用一条SQL语句实现以下查询要求： 查询至少有一张信用卡余额超过5000元的客户编号，以及该客户持有的信用卡总余额，总余额命名为credit_card_amount。查询结果依客户编号排序。 （第17关快把人整死，第18关就这？） 3.19 以日历表格式显示每日基金购买总金额 请用一条SQL语句完成以下查询任务： 以日历表格式列出2022年2月每周每个交易日基金购买总金额，输出格式如下： week_of_trading Monday Tuesday Wednesday Thursday Friday 1 2 3 4 列表中第1列为周次，2022年2月7日(星期一)为当月的第1个交易日，这一周记为第1周次。注意显示结果并不需要画表格线，只需按这个格式输出结果即可了。 四、实训4 数据查询(Select)之二 4.1 查询销售总额前三的理财产品 请用一条SQL语句完成以下查询任务： 查询2010年和2011年这两年每年销售总额前3名（如果有并列排名，则后续排名号跳过之前的并列排名个数，例如1、1、3）的统计年份（pyear）、销售总额排名值(rk)、理财产品编号(p_id)、销售总额(sumamount)。 注意结果输出要求：(1)按照年份升序排列，同一年份按照销售总额的排名值升序排列，如遇到并列排名则按照理财产品编号升序排列;(2)属性显示：统计年份（pyear）、销售总额排名值(rk)、理财产品编号(p_id)、销售总额(sumamount)（3）结果显示顺序：先按照统计年份（pyear）升序排,同一年份按照销售总额排名值（rk）升序排,同一排名值的按照理财产品编号（p_id ）升序排。 4.2 投资积极且偏好理财类产品的客户 请用一条SQL语句完成以下查询任务： 购买了3种（同一编号的理财产品记为一种）以上理财产品的客户被认为投资积极的客户，若该客户持有基金产品种类数（同一基金编号记为相同的基金产品种类）小于其持有的理财产品种类数，则认为该客户为投资积极且偏好理财产品的客户。查询所有此类客户的编号(pro_c_id)。 注意结果输出要求：按照客户编号的升序排列，且去除重复结果 4.3 查询购买了所有畅销理财产品的客户 请用一条SQL语句完成以下查询任务： 若定义持有人数超过2的理财产品称为畅销理财产品。查询购买了所有畅销理财产品的客户编号(pro_c_id)。 注意结果输出要求：按照客户编号的升序排列，且去除重复结果。 （实训3做出阴影了，感觉实训4都还算简单） 4.4 查找相似的理财产品 请用一条SQL语句完成以下查询任务： 在某些推荐方法中，需要查找某款理财产品相似的其他理财产品，不妨设其定义为：对于某款理财产品A，可找到持有A数量最多的“3”个（包括所有持有相同数量的客户，因此如有3个并列第一、1个第二、一个第三，则排列结果是1,1,1,2,3）客户，然后对于这“3”个客户持有的所有理财产品（不包含产品A自身），每款产品被全体客户持有总人数被认为是和产品A的相似度，若有相似度相同的理财产品，则为了便于后续处理的确定性，则这些相似度相同的理财产品间按照产品编号的升序排列。按照和产品A的相似度，最多的“3”款（同上理，前3名允许并列的情况，例如排列结果是1,2,2,2,3）理财产品，就是产品A的相似的理财产品。 请查找产品14的相似理财产品编号（不包含14自身）（pro_pif_id）、该编号的理财产品的客购买客户总人数（cc）以及该理财产品对于14 号理财产品的相似度排名值（prank）。 注意结果输出要求：按照相似度值降序排列，相同相似度的理财产品之间则按照产品编号的升序排列。 4.5 查询任意两个客户的相同理财产品数 请用一条SQL语句完成以下查询任务： 查询任意两个客户之间持有的相同理财产品种数，并且结果仅保留相同理财产品数至少2种的用户对。 注意结果输出要求：第一列和第二列输出客户编号(pro_c_id,pro_c_id)，第三列输出他们持有的相同理财产品数(total_count)，按照第一列的客户编号的升序排列。 4.6 查找相似的理财客户 请用一条SQL语句完成以下查询任务： 在某些推荐方法中，需要查找某位客户在理财行为上相似的其他客户，不妨设其定义为：对于A客户，其购买的理财产品集合为{P}，另所有买过{P}中至少一款产品的其他客户集合为{B}，则{B}中每位用户购买的{P}中产品的数量为他与A客户的相似度值。将{B}中客户按照相似度值降序排列，得到A客户的相同相似度值则按照客户编号升序排列，这样取前两位客户即为A客户的相似理财客户列表。 查询每位客户(列名：pac)的相似度排名值小于3的相似客户(列名：pbc)列表，以及该每位客户和他的每位相似客户的共同持有的理财产品数(列名：common)、相似度排名值(列名：crank)。 注意结果输出要求：要求结果先按照左边客户编号(pac)升序排列，同一个客户的相似客户则按照客户相似度排名值（crank）顺序排列。 五、实训5 数据的插入、修改与删除(Insert,Update,Delete) 5.1 插入多条完整的客户信息 重温上一实训项目中金融应用场景数据库中的客户表结构： 表1 client(客户表) 字段名称 数据类型 约束 说明 c_id INTEGER PRIMARY KEY 客户编号 c_name VARCHAR(100) NOT NULL 客户名称 c_mail CHAR(30) UNIQUE 客户邮箱 c_id_card CHAR(20) UNIQUE NOT NULL 客户身份证 c_phone CHAR(20) UNIQUE NOT NULL 客户手机号 c_password CHAR(20) NOT NULL 客户登录密码 向客户表插入以下3条数据: c_id c_name c_mail c_id_card c_phone c_password 1 林惠雯 960323053@qq.com 411014196712130323 15609032348 Mop5UPkl 2 吴婉瑜 1613230826@gmail.com 420152196802131323 17605132307 QUTPhxgVNlXtMxN 3 蔡贞仪 252323341@foxmail.com 160347199005222323 17763232321 Bwe3gyhEErJ7 用一条insert语句，或用三条insert语句完成，都可以。 5.2 插入不完整的客户信息 已知33号客户部分信息如下: c_id(编号):33 c_name(名称):蔡依婷 c_phone(电话):18820762130 c_id_card(身份证号):350972199204227621 c_password(密码):MKwEuc1sc6 请用一条SQL语句将这名客户的信息插入到客户表(client) 5.3 批量插入数据 已知表new_client保存了一批新客户信息，该表与client表结构完全相同。 请用一条SQL语句将new_client表的全部客户信息插入到客户表(client) 5.4 删除没有银行卡的客户信息 请用一条SQL语句删除client表中没有银行卡的客户信息。 注意：MySQL的delete语句中from关键词不能省略。 5.5 冻结客户资产 请用一条update语句将手机号码为“13686431238”这位客户的投资资产(理财、保险与基金)的状态置为“冻结”。 5.6 连接更新 在金融应用场景数据库中，已在表property(资产表)中添加了客户身份证列，列名为pro_id_card，类型为char(18)，该列目前全部留空(null)。请用一条update语句，根据client表中提供的身份证号(c_id_card)，填写property表中对应的身份证号信息(pro_id_card) 六、视图 6.1 创建所有保险资产的详细记录视图 根据提示，在右侧代码编辑窗口填写1条SQL语句，完成以下任务： 创建包含所有保险资产记录的详细信息的视图v_insurance_detail，包括购买客户的名称、客户的身份证号、保险名称、保障项目、商品状态、商品数量、保险金额、保险年限、商品收益和购买时间 6.2 基于视图的查询 基于上一关创建的视图v_insurance_detail进行分组统计查询，列出每位客户的姓名，身份证号，保险投资总额（insurance_total_amount）和保险投资总收益（insurance_total_revenue）,结果依保险投资总额降序排列 七、存储过程与事务 7.1 使用流程控制语句的存储过程 数据库中有表fibonacci，用来储存斐波拉契数列的前n项： 列名 类型 说明 n int 斐波拉契数列的第n项,主码 fib(n) bigint 第n项的值 斐波拉契数列的前5项为: n fib(n) 0 0 1 1 2 1 3 2 4 3 推导公式为：fib(n) = fib(n-1) + fib(n-2)。 请根据提示，在右侧代码文件编辑器补充代码，创建存储过程sp_fibonacci(in m int)，向表fibonacci插入斐波拉契数列的前m项，及其对应的斐波拉契数。fibonacci表初始值为一张空表。请保证你的存储过程可以多次运行而不出错。 7.2 使用游标的存储过程 医院的某科室有科室主任1名(亦为医生)，医生若干(至少2名，不含主任)，护士若干(至少4人)，现在需要编写一存储过程，自动安排某个连续期间的大夜班(即每天00:00-8:00时间段)的值班表，排班规则为： 1.每个夜班安排1名医生，2名护士； 2.值班顺序依工号顺序循环轮流安排(即排至最后1名后再从第1名接着排)； 3.科室主任参与轮值夜班，但不安排周末(星期六和星期天)的夜班，当周末轮至科主任时，主任的夜班调至周一，由排在主任后面的医生依次递补值周末的夜班。 存储过程的名字为sp_night_shift_arrange,它带两个输入参数：start_date, end_date，分别指排班的起始时间和结束时间。排班结果直接写入表night_shift_schedule，其结构如下： 表night_shift_schedule(夜班值班安排表) 列 类型 说明 n_date date 日期, primary key n_doctor_name char(30) 医生姓名 n_nurse1_name char(30) 护士1姓名 n_nurse2_name char(30) 护士2姓名 假定该科室没有同名的医生和同名的护士。 科室参与值班的医护人员存储在表employee中，其结构为： 表employee(医护人员表) 列 类型 说明 e_id int 编号, primary key e_name char(30) 姓名 e_type int 类别：1-主任,医生;2-医生;3-护士 不用考虑其它信息(比如科室之类的)，在生产环境中，只需在where短语中施加条件限制即可明确选出所需科室的医护人员。这里，且把表中全部人员视为该科室人员。 请根据提示，在右侧代码文件编辑器补充代码，完成该存储过程。（这部分太长了，感觉有点复杂，没有自己完成，以下部分为学长的代码）[HUST-CS-Database-system-principle/第2关 使用游标的存储过程.txt at main · fly-lovest/HUST-CS-Database-system-principle (github.com)](https://github.com/fly-lovest/HUST-CS-Database-system-principle/blob/main/数据库系统原理实验/实验code及报告/源代码/实训6 存储过程与事务/第2关 使用游标的存储过程.txt) 7.3 使用事务的存储过程 在金融应用场景数据库中，编程实现一个转账操作的存储过程sp_transfer，实现从一个帐户向另一个帐户转账。该过程有5个输入参数： applicant_id 付款人编号 source_card_id 付款卡号 receiver_card_id 收款人编号 dest_card_id 收款卡号 amount 转账金额 return_code 1：正常转账；0:转账不成功 转账操作涉及对表bank_card的操作(在生产环境中，至少还要记录转账操作本身相关的信息至转账表，在实验环境中没有设计这样的表，从略；另外，生产环境中，当银行卡被冻结，或被卡主挂失后，都不能进行转账，在实验环境中，没有设计相应的字段 ，故也从略)。 注意事项： 仅当转款人是转出卡的持有人时，才可转出； 仅当收款人是收款卡的持有人时，才可转入； 储蓄卡之间可以相互转账； 允许储蓄卡向信用卡转账，称为信用卡还款(允许替它人还款)，还款可以超过信用卡余额，此时，信用卡余额为负数； 信用卡不能向储蓄卡转账； 转账金额不能超过储蓄卡余额； 附上 bank_card(银行卡)表结构： 字段名称 数据类型 约束 说明 b_number CHAR(30) PRIMARY KEY 银行卡号 b_type CHAR(20) 无 银行卡类型(储蓄卡/信用卡) b_c_id INTEGER NOT NULL FOREIGN KEY 所属客户编号,引用自client表的c_id字段。 b_balance NUMERIC(10,2) NOT NULL 余额,信用卡余额系指已透支的金额 请根据上述要求，在右侧代码文件编辑器补充代码，完成存储过程sp_transfer的编程。 八、触发器 8.1 为投资表property实现业务约束规则-根据投资类别分别引用不同表的主码 在右侧代码文件编辑器里补充代码，实现本任务所要求的完整性业务规则。当插入的数据不符合要求时，拒绝数据的插入，并反馈出错信息： (1) pro_type数据不合法时，显示: type x is illegal! 这里，x系指试图插入的pro_type值。 (2) pro_type = 1,但pro_pif_id不是finances_product表中的某个主码值，显示: finances product #x not found! 这里,x系指试图插入的pro_pif_di的值。 (3) pro_type = 2,但pro_pif_id不是insurance表中的某个主码值，显示: insurance #x not found! 这里,x系指试图插入的pro_pif_id的值。 (3) pro_type = 3,但pro_pif_id不是fund表中的某个主码值，显示: fund #x not found! 这里,x系指试图插入的pro_pif_id的值。 提示： (1) 查阅MySQL的字符串函数，构造出错信息； (2) 当数据不合法时，用signal sqlstate 语句抛出异常，并设置出错信息： SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = msg; 其中，通用SQLSTATE '45000'意指用户定义的待处理异常，msg需替换成你想要显示的提示信息(不超过128个字符)。 九、用户自定义函数 9.1 创建函数并在语句中使用它 主在右侧代文件编辑器补充代码，完成以下编程任务： (1) 用create function语句创建符合以下要求的函数： 依据客户编号计算其所有储蓄卡余额的总和 函数名为：get_deposit (2) 利用创建的函数，仅用一条SQL语句查询存款总额在100万(含)以上的客户身份证号，姓名和存款总额(total_deposit)，结果依存储总额从高到低排序 十、数据库设计与实现 10.1 从概念模型到MySQL实现 应用背景介绍 这是一个机票订票系统，系统需要考虑以下实体： （1）用户(user)： 用户分两类，普通用户可以订票，管理用户有权限维护和管理整个系统的运营。为简单起见，两类用户合并，用admin_tag标记区分。用户的属性(包括业务约束)有： ● 用户编号: user_id int 主码,自动增加 ● 名字: firstname varchar(50) 不可为空 ● 姓氏: lastname varchar(50) 不可为空 ● 生日: dob date 不可为空 ● 性别: sex char(1) 不可为空 ● 邮箱: email varchar(50) ● 联系电话: phone varchar(30) ● 用户名: username varchar(20) 不可空,不可有重 ● 密码: password char(32) 不可空 ● 管理员标志: admin_tag tinyint 缺省值0(非管理员),不能空 （2）旅客(passenger)： 用户登录系统不一定是替自己买票，所以用户和旅客信息是分开存储的。属性有： ● 旅客编号: passenger_id int 自增，主码 ● 证件号码: id char(18) 不可空 不可重 ● 名字：firstname varchar(50) 不可空 ● 姓氏: lastname varchar(50) 不可空 ● 邮箱：mail varchar(50) ● 电话: phone varchar(20) 不可空 ● 性别: sex char(1) 不可空 ● 生日: dob date （3）机场(airport)： 有以下属性： ● 编号: airport_id int 自增，主码 ● 国际民航组织编码：iata char(3) 不可空，全球唯一 ● 国际航运协会编码: icao char(4) 不可空，全球唯一 ● 机场名称: name varchar(50) 不可空，普通索引 ● 所在城市: city varchar(50) ● 所在国家: country varchar(50) ● 纬度: latitude decimal(11,8) ● 经度: longitude decimal(11,8) ● 全球每个机场都有唯一IATA编码和ICAO编码，IATA为3个字符，ICAO为4个字符。例如首都机场的(IATA,ICAO)分别为(PEK,ZBAA)，大兴机场为(PKX,ZBAD),天河机机场为(WUH,ZHHH)。在飞机登记牌上出发地和到达地均用IATA表示。为能在地图上显示机场位置，需要记录经纬度信息。 （4）航空公司(airline)： 有以下属性: ● 编号：airline_id int 自增，主码 ● 名称：name varchar(30) 不可空 ● 国际民航组织编码: iata char(2) 不可空，具全球唯一性 ● 航空公司的IATA编码为2位，如东航为MU，国航为CA,南航为CZ等，航班号一般以所属航空公司的IATA码为前缀。 （5）民航飞机(airplane)： 有属性： ● 编号：airplane_id int 自增，主码 ● 机型：type varchar(50) 不可空，如B737-300,A320-500等 ● 座位: capacity smallint 不可空 ● 标识: identifier varchar(50) 不可空 （6）航班常规调度表(flightschedule)： 舤班一般以周次安排，例如：每周两个班次，周一和周五。有属性： ● 航班号: flight_no char(8) 主码 ● 起飞时间: departure time 非空 ● 到达时间: arrival time 非空 ● 飞行时长: duration smalint 非空，飞行时长一般以分种计。 ● 周一：monday tinyint 缺省0 ● 周二：tuesday tinyint 缺省0 ● 周三：wednesday tinyint 缺省0 ● 周四：thursday tinyint 缺省0 ● 周五：friday tinyint 缺省0 ● 周六：saturday tinyint 缺省0 ● 周日：sunday tinyint 缺省0 （7）航班表(flight)： 航班依航班常规调度表为基准安排。但调度表不是一成不变，也不是每个既定的航都实际起飞，也不总是按既定的时间起飞，所以实飞航班必须单独安排并记录。要记录的信息有： ● 飞行编号：flight_id int 自增，主码 ● 起飞时间: departure datetime 非空 ● 到达时间: arrival datetime 非空 ● 飞行时长：duration smallint 非空 有的系统还会实时显示航班经纬度和高度位置，这里我们作了简化，去掉了实时飞行信息。 （8）机票(ticket)： 用户可替自己或其亲友购买某个航班的机票。机票的属性有： ● 票号：ticket_id int 自增，主码 ● 坐位号：seat char(4) ● 价格：price decimal(10,2) 不能空 （9）实体间的联系： 实体间的联系都清楚地标注在ER图中： 每个航空公司都有一个母港(机场)，又叫基地。大的机场可能会是多家公司的基地，小型机场可能不是任何航空公司的基地。 每个航班属于一家航空公司，航空公司可以很多航班。 任何一驾民航飞机属于一家航空公司，航空公司可以有多驾飞机。 每驾飞机可以执飞多个航班，一个飞行航班由一架飞机执飞。 一个航班根据执飞机型可以售出若干机票。一张机票是某个特定航班的机票。 用户可以多次订票，旅客可以多次乘坐飞机。一张机票肯定是某个用户为某个特定的旅客购买的特定航班的机票。即机票信息不仅跟乘坐人有关，同时记录购买人信息(虽然两者有时是同一人)。为简单起见，订购时间没有考虑。 无论常规计划的航班，还是实际飞行航班，都是从某个机场出发，到达另一个机场。但一个机场可以是很多个航班的出发地，也是很多航班的到达地。 请根据上述信息和所给ER图，给出在MySQL实现flight_booking的语句，包括建库，建表，创建主码，外码，索引，指定缺省，不能为空等约束。所有索引采用BTREE。所有约束的名字不作要求。所有的外码与主码同名。但有两处例外: 计划航班和飞行航班都涉及出发机场和到达机场，外码与主码同名会导致同一表有两个同名列。故这两处外码例外处理：出发机场命名为from，到达机场命名为to。 注意：所有的表名字和列名，都没有大写字母。列名存在与关键字同名的情形，请妥善处理。你有可能不能一次通过评测，请考虑你的代码要反复修改再运行的情形。 十一、数据库应用开发(JAVA篇) 11.1 JDBC体系结构和简单的查询 正确使用JDBC，查询金融应用场景数据库finance的client表(客户表)中邮箱不为空的客户信息，列出客户姓名，邮箱和电话.一个展示结果的示例如下(字体颜色是平台自动加的，不是编程要求)： 注：标题以及字段值之间用制表符隔开。第1列和第2列间用一个制表符，第2列和第3列间，标题用4个制表符，字段值用两个制表符隔开。 JDBC驱动程序(mysql-connector-java-8.0.23.jar)由平台提供，直接使用即可，不用特别设置。 请在右侧代码文件编辑器适当的位置补充代码，实现上述编程要求。 附client表的结构: 表1 client(客户表) 字段名称 数据类型 约束 说明 c_id INTEGER PRIMARY KEY 客户编号 c_name VARCHAR(100) NOT NULL 客户名称 c_mail CHAR(30) UNIQUE 客户邮箱 c_id_card CHAR(20) UNIQUE NOT NULL 客户身份证 c_phone CHAR(20) UNIQUE NOT NULL 客户手机号 c_password CHAR(20) NOT NULL 客户登录密码 11.2 用户登录 编程体验客户登录功能.程序先后提示客户输用户名和密码: 请输入用户名： 请输入密码： 客户的邮箱(c_mail)充当用户名,而不是编号(c_id).通常邮箱更容易记住. 根据客户的输入,输出以下两类信息之一: 登录成功。 用户名或密码错误！ 如果用户名和密码匹配成功,输出前者,其它情况输出后者(包括该用户不存在).通常被恶意用户试探出用户名,也会带来不良后果. 请在右侧代码文件编辑窗补充代码，完成编程任务。 11.3 添加新客户 根据提示，在右侧代码文件编辑器补充代码，实现向client表插入客户信息的方法： insertClient()方法的每个参数都在注释中进行了详细的说明。 insertClient()方法返回插入的行数，尽管main()方法在调时并没有检查该返回值(评测程序能检查出是否插入是否成功)。 假定输入的客户信息没有错误需要处理(比如客户编号不是整数，信息项缺失等异常情况)。 请不要修改main()方法。 11.4 银行卡销户 根据提示，在右侧代码文件编辑器补充代码，实现向银行卡销号的方法，只要客户编号和银行卡号匹配，即从bank_card表中删除该银行卡。至于卡内余额或信用卡的欠款，由客户在柜台当面结清，程序不用考虑其它事项。 removeBankCard()方法的每个参数都在注释中进行了详细的说明。 removeBankCard()方法返回被删除的行数。 假定输入的客户信息没有错误需要处理(比如客户编号不是整数，信息项缺失等异常情况)。 请不要修改main()方法。 11.5 客户修改密码 根据提示，在右侧代码文件编辑器补充代码，实现修改密码的方法passwd()。客户修改密码通常需要确认客户身份，即客户需提供用户名(以邮箱为用户名)和密码，同时还需要输两次新密码，以免客户实际输入的密码与心中想的不一致，只有当所有条件(合法的客户，两次密码输入一致)时才修改密码。main()方法在调用passwd()之前，会先检查新设密码的两次输入是否一致，只有在两次输入一致的情形下才会调用passwd(); passwd()方法的每个参数都在注释中进行了详细的说明。 passwd()方法返回一个整数： 1 - 密码修改成功 2 - 用户不存在 3 - 密码不正确 -1 - 程序异常(如没能连接到数据库等） 请不要修改main()方法。 11.6 事务与转账操作 编写一个银行卡转账的方法transferBalance()。 transferBalance()在被调用前，柜台已经确认过转出帐号持有者身份，所以转帐方法只接受转出卡号，转入卡号和转账金额三个参数。由调用者保证转账金额为正数。 transferBalance()方法的每个参数都在注释中进行了详细的说明。 transferBalance()返回boolean值，true表示转帐成功，false表示转账失败，并不需要细分或解释失败的原因。 下列任一情形都不可转账(转账失败的原因)： 转出或转入帐号不存在 转出账号是信用卡 转出帐号余额不足 提示： 1.本方法需开启手工事务,并正确使用commit和rollback； 2.当转入卡是信用卡时，意指信用卡还款，还款可以超过透支款项； 3.对事务的隔离级别不作要求。 请不要修改main()方法。 附:bank_card(银行卡)的表结构： 字段名称 数据类型 约束 说明 b_number CHAR(30) PRIMARY KEY 银行卡号 b_type CHAR(20) 无 银行卡类型(储蓄卡/信用卡) b_c_id INTEGER NOT NULL FOREIGN KEY 所属客户编号,引用自client表的c_id字段。 b_balance NUMERIC(10,2) NOT NULL 余额,信用卡余额系指已透支 ","link":"https://2006wzt.github.io/post/数据库系统概论实验/"},{"title":"Socket编程实验","content":"Socket编程实验 一、实验要求 1.1 实验目标和内容 1.1.1 实验目的 ● 了解应用层和运输层的作用及相关协议的工作原理和机制。 ● 掌握 SOCKET 编程的基本方法。 1.1.2 实验环境 ● 操作系统：Windows ● 语言：C++ ● 编程开发环境：Visual Studio 2008-2019 皆可 1.1.3 实验内容 **题目：**编写一个 Web 服务器软件，要求如下： 基本要求： ● 可配置 Web 服务器的监听地址、监听端口和主目录（不得写在代码里面，不能每配置一次都要重编译代码）； ● 能够单线程处理一个请求。当一个客户（浏览器,输入 URL：http://202.103.2.3/index.html）连接时创建一个连接套接字； ● 从连接套接字接收 http 请求报文，并根据请求报文的确定用户请求的网页文件； ● 从服务器的文件系统获得请求的文件。 创建一个由请求的文件组成的 http 响应报文。； ● 经 TCP 连接向请求的浏览器发送响应，浏览器可以正确显示网页的内容； 高级要求： ● 能够传输包含多媒体（如图片）的网页给客户端，并能在客户端正确显示； ● 在服务器端的屏幕上输出请求的来源（IP 地址、端口号和 HTTP 请求命令行）； ● 在服务器端的屏幕上能够输出对每一个请求处理的结果； ● 对于无法成功定位文件的请求，根据错误原因，作相应错误提示，并具备一定的异常情况处理能力。 1.2 检查表 二、功能实现 2.1 Server和Client简单Demo 2.1.1 Server 上述搭建的服务器支持网页的访问请求，访问网址 http://127.0.0.1，会向该服务器请求网页资源，然后该服务器会发送响应报文，向浏览器推送网页，使用IP地址127.0.0.1是因为该IP地址指向的是本主机，而不会参与到公共网络的访问中去。 2.1.2 Client 该客户端模拟一个向服务器发送消息的进程。 2.2 配置监听地址、端口和主目录 配置监听IP地址、端口和主目录，改变上述配置之后不允许重新编译，那么我们可以通过添加一个配置文件，将Server的监听地址、端口和主目录存储在配置文件中，通过读文件的方式来获取设置的监听地址、端口和主目录。 2.2.1 Server 配置文件的存储格式如下： 读取配置文件并解析的代码如下： 在配置监听地址与端口处，将我们解析得到的IP与port设置为服务器的IP与port 经过测试，edge浏览器可以正确得获得网页。 我们修改配置文件 config.txt 中的IP地址和端口号，同样可以实现网页的请求，因此也实现了在监听端口上进行监听的功能，同时网页正常得显式也说明了我们当前的服务器可以在收到客户端请求时创建连接套接字 2.3 响应客户端请求，定位相应的html文件 我们之前反馈给网页的是一个字符串，因此网页只能显示这一句话，注意到网页内容的请求报文为（以edge浏览器为例）： 网页请求网页图标的请求报文为： 浏览器并不会明确得告诉我们它所要请求的html网页是什么，根据 Accept 语句，它希望得到的是text文本或html文件，因此我们希望将一个存储在主目录下的html文件反馈给网页，首先设计好主目录下的文件结构，如下图所示： 我们将html文件存储在主目录下的 file 文件夹中，将网页中用到的图片存储在 image 文件夹中，实现一个简单的网页： 2.3.1 Server 为了解析请求报文，我们将请求报文按照 \\r\\n 分割成若干行，存储在全局 vector 中，分割函数如下： 初步编写构造响应报文的函数如下，可以构造相关的html报文： 当我们向浏览器发送构造的响应报文时，我们收到的请求报文中多了这样一条： 显然，这正是请求我们html网页中图片的请求报文，说明我们发送html文件的响应报文被浏览器正确接收并解析了，于是我们进一步完善构造响应报文的函数，使其能够发送图片给浏览器，如下： 可见我们上述的函数大改，从创建报文变为了创建并发送报文，主要是因为image发送一直失败，最后在这个函数中将首部和content分开发送才成功传输了图片，可能是因为缺少了 2.4 支持多种类型的文件传输 在上述html网页中已经实现了html文本和图片的传输，我们首先完善一下浏览器请求访问特定网页的请求，网页内容如下： 👆网上白嫖的简单炫酷的代码，并不重要，重要的是能否进行传输，完善的代码如下：去掉了target的标识（当时搞的时候没想到多此一举），由此我们基本实现了传输网页与图片的功能。 我们访问以下网址即可得到特定的html文件：烟花特效 为了完善多类型文件的传输，我希望能正确得传输流媒体文件，因此参考一下网络上的视频传输报文： 2.4.1 Server 参考网络上的报文请求格式，我们也可以实现传输mp4文件的响应报文： 发现传输文件的过程存在很大部分的共性，不妨整合一下3个部分的传输代码，并添加以下功能： ● 对于一些私人文件（文件名中有private字样），它们可以被路径找到，但是不允许发送，发送403响应报文 ● 打开文件失败时应该发送404响应报文 ● HTTP版本不为HTTP1.1时应该发送505响应报文 ● 服务器未从请求行中解析出有效路径时，说明读不懂报文，发送400响应报文 2.5 服务器屏幕上显示请求来源 我们只需要输出会话Socket的IP地址与端口号即可，请求报文已经输出过了，因此不再单独显示http的请求行 2.5.1 Server 2.6 请求处理结果与错误提示 我们在上述的代码中已经基本实现了请求处理结果和错误提示的显示，希望进一步得完善此部分： ● 底层初始化错误时，也进行报错，并展示错误代码： ● 初始化Server Socket时，也进行报错，并展示错误代码： ● Socket与地址和端口绑定时，也进行报错，并展示错误代码： ● 与客户端连接失败时，也进行报错，并展示错误代码： ● 针对可复现的404（NOT FOUND）、403（FORBIDDEN）、400（Bad Request）三种错误情况，在发送请求响应报文之后再向客户端反馈一张提示当前遇到的错误的图片，进行错误提示：（没错，我又对函数大改了一下，将处理这三个错误的部分的报文构造封装成了一个函数 error_tips() ） 最终优化效果如下： 404：路径无效，找不到文件 403：路径有效，但请求的是非法文件 400：请求的文件不是html、image、video中的一个（此处设定的是400优先级高于404） 2.7 异常处理 我们还希望服务器有一定的异常处理能力，我此处关注到的是当前代码中的warning部分： 因此添加一个分支以避免溢出： 2.8 服务器完善 最终对我们的服务器进行一定的完善，为各个提示添加颜色，使其更加直观，最终得到的服务器代码如下： ","link":"https://2006wzt.github.io/post/socket编程实验/"},{"title":"CSP-精炼","content":"CSP-精练 目前考过了2次CSP，分别拿到了290和320分，感觉需要着重练习一下4、5题，在记录重点题目的练习前先谈谈我的感受。 （1）首先对于第一题，毫无疑问这100分一定是送的，就算你读完一遍题目没有头绪，也可以在提示中找到解题方法，不再过多阐述。 （2）对于第二题，这是一道比较吃基本算法的题，26次CSP认证考的是如何处理稀疏大数组，27次考的是类似于0-1背包的动态规划，对于有算法基础的人来说还是可以通过思考快速解决的，最好需要熟悉一些基本的诸如：归并、动态规划、贪心等算法。 （3）对于第三题，是一道不折不扣的大模拟，不太吃算法，基本上暴力模拟出来就能解决，如果你用cin和cout比较多的话，尽量使用下面这行代码进行优化： 第三题一定要设计好数据结构，可以通过定义结构体或充分利用 STL ~STL~ STL 中的容器存储数据，理清逻辑还是可以暴力求解的。 （4）对于第四、五题：感觉这就很吃算法能力了，即需要利用合适的算法进行求解，还需要充分得进行优化，这也是我在这篇文章中所要着重练习的地方，如果没有多加练习很难进行突破，毕竟oi爷也都是经过大量练习才能随随便便CSP500的。 仅考过两次CSP，经验还不多，那么我们直接就从第27次CSP认证的4、5题上手吧！ [27-4] 吉祥物投票（vote） 【题目描述】 为了促进西西艾弗岛上的旅游业发展，当地决定设计一个吉祥物形象。活动吸引了众多设计领域的大师和爱好者参加，经过初步筛选，共选出了 m 个作品，编号为 1 ∼ m，进行最终的投票角逐。 活动还吸引了西西艾弗岛上的 n 名投票者参与，编号为 1 ∼ n，每人都在最终的投票环节拥有投一票的权利，也可以放弃投票。我们定义每个人的投票意愿 ai(1≤i≤n) ~a_i(1 ≤ i ≤ n)~ ai​(1≤i≤n) 为一个 0 ∼ m 的整数，若 ai=0 ~a_i = 0~ ai​=0 表示这个人目前没有支持的作品，打算放弃投票，否则表示这个人支持第 ai ~a_i~ ai​ 号作品并有意愿将票投给它。 最初，由于所有人对参与竞选的作品都不了解，因此投票意愿 ai ~a_i~ ai​ 均为 0。接下来是紧张刺激的拉票环节，作品的设计者们要想方设法给自己的作品拉票，这一过程中可能出现如下若干种事件： 1 l r x：编号为 x 的作品开展了一场拉票活动，成功地吸引了编号为 l ∼ r 的投票者的兴趣，使得他们的投票意愿全部改为 x。 2 x w：编号为 x 的作品需要经历一次大规模修改，所以需要暂时退出竞选。由于x 与 w 两个作品的风格较为相近，因此原先投票意愿为 x 的投票者的投票意愿变为了w。特别地，若 w = 0，表示这些投票者暂时找不到新的支持的作品。需要注意的是，作品 x 退出竞选只是暂时的，因此后续的事件中作品 x 仍可能出现。 3 x y：主办方发现自己的统计出现了失误，将编号为 x 和 y 的作品弄颠倒了。发布勘误后，所有原先投票意愿为 x 的投票者的投票意愿变为了 y，所有原先投票意愿为y 的投票者的投票意愿变为了 x。 4 w：主办方决定进行一次调查：希望知道所有投票者中，当前投票意愿为 w 的有多少人。若 w=0 ~w\\not=0~ w​=0 ，相当于调查有多少投票者目前支持作品 w ，否则相当于调查有多少投票者目前没有支持的作品。 5：主办方决定进行一次调查：若以现在的投票意愿进行最终的投票，获胜的作品是哪一个。规定得票数至少为 1 且最多的作品获胜，得票数相同则编号较小的作品获胜。特别地，若所有作品均无得票，认为不存在获胜作品。 从拉票开始到结束，共出现了 q 次如上的事件。由于参选的作品数和投票人数实在太多，单凭活动主办方的能力难以全面统计，现在请你编写一个程序来处理这些事件，并求出每次调查的结果。 【输入格式】 从标准输入读入数据。 第 1 行，3 个正整数 n, m, q。 接下来 q 行，每行 1 ∼ 4 个非负整数，描述一个事件。 【输出格式】 输出到标准输出。 对于每个 4 或 5 事件输出一行，一个非负整数表示此次调查的结果。 其中事件 5 若 不存在获胜作品则输出0。 【样例 1 输入】 【样例 1 输出】 【数据范围】 对于所有的数据，满足 1≤n≤109，1≤m,q≤105，1≤l≤r≤n，1≤x,y≤m，0≤w≤m ~1 ≤ n ≤ 10^9，1 ≤ m, q ≤ 10^5，1 ≤ l ≤ r ≤ n，1 ≤ x, y ≤ m，0 ≤ w ≤ m~ 1≤n≤109，1≤m,q≤105，1≤l≤r≤n，1≤x,y≤m，0≤w≤m 。 保证事件 2 中 x≠w，事件 3 中 x≠y。 【正解】 我们先来整理以下题目大意： 有 m ~m~ m 个作品（编号1-m）， n ~n~ n 个投票者（编号1-n），投票者就是一个长度为 n ~n~ n 的序列，初始值为0，需要支持下列操作： ① 1 l r x ~1~l~r~x~ 1 l r x ：把区间 [l,r] ~[l,r]~ [l,r] 中的数据变为 x ~x~ x ② 2 x w ~2~x~w~ 2 x w ：把区间 [1,n] ~[1,n]~ [1,n] 值为 x ~x~ x 的数据替换为 w ~w~ w ③ 3 x y ~3~x~y~ 3 x y ：交换区间 [1,n] ~[1,n]~ [1,n] 中值分别为 x ~x~ x 和 y ~y~ y 的数据 ④ 4 w ~4~w~ 4 w ：统计区间 [1,n] ~[1,n]~ [1,n] 中值为 w ~w~ w 的数据个数 ⑤ 5 ~5~ 5 ：统计区间 [1,n] ~[1,n]~ [1,n] 中出现次数最多的数据 序列长度 n≤109 ~n\\le10^9~ n≤109 ，参赛作品数 m≤105 ~m\\le10^5~ m≤105 ，操作个数 q≤105 ~q\\le10^5~ q≤105 显然，如果想要拿满分，开大数组是不可能的；但是如果想要拿部分分，直接开数组是最好的选择，按照题目要求暴力循环各个操作就可以拿到20分（我不会告诉你这个人就是我）。 既然数组开不到 109 ~10^9~ 109 ，那么我们该怎么办呢？因为注意到测试集中有特殊的只有1、4、5事件的，所以首先从1操作入手，注意到1操作是分段的操作，因此我们可以利用一个比较玄学的东西：颜色段均摊 颜色段均摊 颜色段均摊算法也叫做珂朵莉树，亦可简称ODT（Old Driver Tree），至于为什么叫这个名字，可以看看它的来源CF896C Willem, Chtholly and Seniorious，我们在下文将其简称为ODT，这是一个暴力玄学的基于 std::set 数据结构，对于随机的数据非常有效。 有一类问题，对一个序列，进行一个区间推平操作。就是把一个范围内，比如 [ l , r ] 范围内的数字变成同一个。除了推平之外，也可能夹杂着其它操作。如果数据是随机的，就可以用ODT，我们可以将具有相同值的一段数据区间看作一个结点，结点定义如下： ODT的核心操作是 split，这是因为后续操作中有的node一部分需要修改，而另一部分不需要修改，为此我们需要进行切分。 比如对于上述图例，我们要执行一个操作：将 [ 2 , 8 ] 中的值替换为666，则需要先执行split操作以便替换方便： 值得注意的是：修改 [ l , r ] 区间中的值之前，对于set进行split操作时，需要先执行 split(r+1)，再执行 split(l) 实现完核心功能后，我们就可以定义一些相关操作了： 推平 assign：将 [l,r] ~[l,r]~ [l,r] 中的值改为x 看下图可以更直观得理解操作的原理： 增加 add：给 [l,r] ~[l,r]~ [l,r] 中的值加上x 还有其他的操作，因为本题用不上，就不再过多赘述了，利用ODT我们解决该题的操作1应该是没问题了，操作2、3不妨先暴力解决，对于操作4、5，每当执行该操作时遍历一遍ODT，更新记录得票数的 vector，1.0版本代码如下： 利用上述代码我们可以得到45分，比暴力开数组多了25分，说明我们的ODT较好得解决了操作1，但是操作2、3是暴力实现的，还需要进一步得对算法进行优化。 ","link":"https://2006wzt.github.io/post/CSP-精练/"},{"title":"深度学习实战（二）：知识回顾","content":"数学基础 2.1 二元分类 我们首先回顾一下机器学习中的二元分类，我们假设输入是一个 64×64 ~64\\times64~ 64×64 的图片，我们需要判断图片中的是不是猫，这就是一个典型的二分类问题，在计算机中图片的显示通过RGB通道，也就是说这个图片在计算机存储为3个 64×64 ~64\\times64~ 64×64 的矩阵，我们不妨将这三个矩阵用一个向量表示，那么我们的输入 x ~x~ x 将会是一个大小为 64×64×3=12288 ~64\\times64\\times3=12288~ 64×64×3=12288 的一维向量。 在此我们统一本次学习中用到的符号表示规范： ① 我们用 (x,y) ~(x,y)~ (x,y) 表示一个输入的样本，用 nx ~n_x~ nx​ 表示输入的特征向量的大小， x∈Rnx ~x\\in R^{n_x}~ x∈Rnx​ ，二分类中 y∈{0,1} ~y\\in\\{0,1\\}~ y∈{0,1} ② 我们用 m ~m~ m 表示训练集的大小，用 (x(1),y(1)),(x(2),y(2)),...,(x(m),y(m)) ~(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})~ (x(1),y(1)),(x(2),y(2)),...,(x(m),y(m)) 表示第 1−m ~1-m~ 1−m 个样本 ③ 有时会用 M=Mtrain ~M=M_{train}~ M=Mtrain​ 表示训练集样本大小， m=mtest ~m=m_{test}~ m=mtest​ 表示测试集样本大小 ④ 默认 x ~x~ x 为列向量，则用 nx×m ~n_x\\times m~ nx​×m 矩阵 X ~X~ X 表示整个输入的特征向量， X=[x(1),x(2),...,x(m)] ~X=[x^{(1)},x^{(2)},...,x^{(m)}]~ X=[x(1),x(2),...,x(m)] ，神经网络中通常会这样表示 ⑤ 同理标签集表示为 Y=[y(1),y(2),...,y(m)] ~Y=[y^{(1)},y^{(2)},...,y^{(m)}]~ Y=[y(1),y(2),...,y(m)] ，这是一个 1×m ~1\\times m~ 1×m 矩阵 2.2 逻辑回归 我们再来回顾一下逻辑回归，对于一个输入的 x ~x~ x ，逻辑回归模型为： h(x)=y^=σ(wTx+b)=1e−(wTx+b)+1=P(y=1∣x)h(x)=\\hat y=\\sigma(w^Tx+b)=\\frac{1}{e^{-(w^Tx+b)}+1}=P(y=1|x) h(x)=y^​=σ(wTx+b)=e−(wTx+b)+11​=P(y=1∣x) 逻辑回归模型的两个参数为 w,b ~w,b~ w,b ，我们可以通过梯度下降的方式训练出这两个参数，首先定义逻辑回归模型的成本函数： J(w,b)=−1m∑i=1m(y(i)log⁡y^(i)+(1−y(i))log⁡(1−y^(i)))J(w,b)=-\\frac1m\\sum_{i=1}^m\\big(y^{(i)}\\log \\hat{y}^{(i)}+(1-y^{(i)})\\log (1-\\hat{y}^{(i)})\\big) J(w,b)=−m1​i=1∑m​(y(i)logy^​(i)+(1−y(i))log(1−y^​(i))) 成本函数是极大似然估计推出的，推导过程可以参考之前的博客逻辑回归 (2006wzt.github.io)，进行梯度下降： := ~:=~ := 表示迭代 w:=w−α∂J(w,b)∂wb:=b−α∂J(w,b)∂bw:=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}\\\\ b:=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b} w:=w−α∂w∂J(w,b)​b:=b−α∂b∂J(w,b)​ 吴恩达老师在这部分内容中讲解了很多与导数相关的知识，大家可以自行学习，因为较为基础，博客中将不再阐述。 成本函数的求导过程如下，这与吴恩达老师的计算图思想是一样的：记 z(i)=wTx(i)+b ~z^{(i)}=w^Tx^{(i)}+b~ z(i)=wTx(i)+b J(w,b)=−1m∑i=1m(y(i)log⁡σ(z(i))+(1−y(i))log⁡(1−σ(z(i))))∂J(w,b)∂w=∂J(w,b)∂σ(z)⋅∂σ(z)∂z⋅∂z∂w=∑i=1mσ(z(i))−y(i)σ(z(i))(1−σ(z(i)))⋅σ(z(i))(1−σ(z(i)))⋅x(i)=∑i=1m(σ(z(i))−y(i))x(i)∂J(w,b)∂b=∂J(w,b)∂σ(z)⋅∂σ(z)∂z⋅∂z∂b=∑i=1mσ(z(i))−y(i)σ(z(i))(1−σ(z(i)))⋅σ(z(i))(1−σ(z(i)))⋅1=∑i=1m(σ(z(i))−y(i))\\begin{aligned} &amp;J(w,b)=-\\frac1m\\sum_{i=1}^m\\big(y^{(i)}\\log \\sigma(z^{(i)})+(1-y^{(i)})\\log (1-\\sigma(z^{(i)}))\\big)\\\\ &amp;\\frac{\\partial J(w,b)}{\\partial w}=\\frac{\\partial J(w,b)}{\\partial \\sigma(z)}\\cdot\\frac{\\partial \\sigma(z)}{\\partial z}\\cdot\\frac{\\partial z}{\\partial w}=\\sum_{i=1}^m\\frac{\\sigma(z^{(i)})-y^{(i)}}{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}\\cdot \\sigma(z^{(i)})(1-\\sigma(z^{(i)}))\\cdot x^{(i)}=\\sum_{i=1}^m(\\sigma(z^{(i)})-y^{(i)})x^{(i)}\\\\ &amp;\\frac{\\partial J(w,b)}{\\partial b}=\\frac{\\partial J(w,b)}{\\partial \\sigma(z)}\\cdot\\frac{\\partial \\sigma(z)}{\\partial z}\\cdot\\frac{\\partial z}{\\partial b}=\\sum_{i=1}^m\\frac{\\sigma(z^{(i)})-y^{(i)}}{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}\\cdot \\sigma(z^{(i)})(1-\\sigma(z^{(i)}))\\cdot 1=\\sum_{i=1}^m(\\sigma(z^{(i)})-y^{(i)}) \\end{aligned} ​J(w,b)=−m1​i=1∑m​(y(i)logσ(z(i))+(1−y(i))log(1−σ(z(i))))∂w∂J(w,b)​=∂σ(z)∂J(w,b)​⋅∂z∂σ(z)​⋅∂w∂z​=i=1∑m​σ(z(i))(1−σ(z(i)))σ(z(i))−y(i)​⋅σ(z(i))(1−σ(z(i)))⋅x(i)=i=1∑m​(σ(z(i))−y(i))x(i)∂b∂J(w,b)​=∂σ(z)∂J(w,b)​⋅∂z∂σ(z)​⋅∂b∂z​=i=1∑m​σ(z(i))(1−σ(z(i)))σ(z(i))−y(i)​⋅σ(z(i))(1−σ(z(i)))⋅1=i=1∑m​(σ(z(i))−y(i))​ 此处我们引入一个概念：向量化，这是一个用于提高代码计算效率的方法，简单来说，计算z=wTx+bz=w^Tx+bz=wTx+b有两种方法： 一些小技巧是不要吝啬去使用reshape去确保向量的维数与期望的是相同的。 这一节貌似没什么有新东西，大家可以仅当成一个机器学习知识的回顾，因为吴恩达老师在这部分讲的都是一些很基础的东西，可以自行去观看视频学习，那么下一节将会正式得进入深度学习部分。 ","link":"https://2006wzt.github.io/post/深度学习实战（二）：知识回顾/"},{"title":"深度学习实战（一）：深度学习导论","content":"深度学习导论 在我们了解了机器学习的基本知识之后，就进入到了深度学习的模块，因为一些算法竞赛中的算法实战涉及到深度学习的部分，所以我会先介绍完深度学习的基本知识，再进入实际的问题求解。 本系列深度学习的知识是我对吴恩达老师课程的总结，希望对你有所帮助。课程链接 1.1 什么是神经网络 深度学习指的是训练神经网络，我们以房价预测为例，假设我们知道关于房子的某些特征，我们希望根据这些特征得到房子的价格，简单的线性回归可以通过拟合得到一个趋势，如下图所示。 而在神经网络中，我们可以将这个线性拟合的过程看作一个神经元（neuron），而这个神经元就是负责上述拟合过程的，这个用于拟合的函数称为ReLU，即修正线性单元（rectified linear unit），修正即取不小于0的值。由此我们得到了一个简单的单神经元网络，而复杂的神经网络就是通过堆叠神经元实现的。 我们不妨想象一个更为复杂的神经元：我们可以通过房屋面积（size）、卧室数量（bedrooms）估算家庭人口（family size），通过邮编（zip code）估算步行化程度（walkability），通过邮编（zip code）和富裕程度（wealth）估算学校质量（school quality），最后我们通过家庭人口容纳数、步行化程度、学校质量进行房价预测，构建的神经网络如下，用到了4个神经元，它们每个神经元都可能是一个ReLU，在这个例子中： X={size,bedrooms,zip_code,wealth} ~X=\\{size,bedrooms,zip\\_code,wealth\\}~ X={size,bedrooms,zip_code,wealth} ， Y={price} ~Y=\\{price\\}~ Y={price} 在我们构建好神经网络之后，输入X就可以得到预测结果y，X所在的层为输入层，中间的3个神经元称为隐藏单元，我们将每个特征输入到这些神经元中，由神经网络决定他该如何处理这些特征，而不需要我们去指定固定的处理规则，神经网络往往能得出较为精确的映射函数 1.2 神经网络与监督学习 几乎所有神经网络都是基于监督学习的，我们已经了解了基本的神经网络（neural network），同时还有处理图像数据的卷积神经网络，处理一维序列数据的循环神经网络。 在监督学习中我们用到的数据集包括结构化数据和非结构化数据： ① 结构化数据来自数据库，每个特征都有着清晰的定义，例如房价预测中的房间大小、卧室数量等等 ② 非结构化数据即音频、图片、文本等计算机不太擅长处理的数据，但是人类却更善于理解这类数据 深度学习用到的很多理论都是很早就提出来得，可是为什么近些年深度学习才火起来呢？ 我们可以看下面这张图，横坐标代表数据量，纵坐标代表算法表现，随着现在人类行为的电子化和信息化，数据的规模逐渐增大，可是诸如支持向量、逻辑回归等算法的表现在随着数据量增大到一定程度后提升不再明显，如红线所示；但是如果我们构建一个小型的神经网络，它的表现将比这些经典算法更好，如黄线所示；而中型（蓝线所示）或者大型（绿线所示）显然会有更好的表现。 需要注意的是在数据集很小时，算法的能力是有提取特征的能力和算法的细节所决定的，所以SVM表现比NN好是完全有可能的，但是当数据集很大时，神经网络将会有更好的性能。 ","link":"https://2006wzt.github.io/post/深度学习导论/"},{"title":"算法刷题小记","content":"算法刷题小记 [NOIP1998 普及组] 幂次方 P1010 NOIP1998 普及组] 幂次方 - 洛谷 题目描述 任何一个正整数都可以用 222 的幂次方表示。例如 $137=27+23+2^0 $。 同时约定方次用括号来表示，即 aba^bab 可表示为 a(b)a(b)a(b)。 由此可知，137137137 可表示为 2(7)+2(3)+2(0)2(7)+2(3)+2(0)2(7)+2(3)+2(0) 进一步： 7=22+2+207= 2^2+2+2^07=22+2+20 ( 212^121 用 222 表示)，并且 3=2+203=2+2^03=2+20。 所以最后 137137137 可表示为 2(2(2)+2+2(0))+2(2+2(0))+2(0)2(2(2)+2+2(0))+2(2+2(0))+2(0)2(2(2)+2+2(0))+2(2+2(0))+2(0)。 又如 1315=210+28+25+2+11315=2^{10} +2^8 +2^5 +2+11315=210+28+25+2+1 所以 131513151315 最后可表示为 2(2(2+2(0))+2)+2(2(2+2(0)))+2(2(2)+2(0))+2+2(0)2(2(2+2(0))+2)+2(2(2+2(0)))+2(2(2)+2(0))+2+2(0)2(2(2+2(0))+2)+2(2(2+2(0)))+2(2(2)+2(0))+2+2(0)。 输入格式 一行一个正整数 nnn。 输出格式 符合约定的 nnn 的 0,20, 20,2 表示（在表示中不能有空格）。 样例 #1 样例输入 #1 样例输出 #1 提示 【数据范围】 对于 100%100\\%100% 的数据，1≤n≤2×1041 \\le n \\le 2 \\times {10}^41≤n≤2×104。 解题过程 看完题之后第一个想到的方法就是递归，先将输入的数字表示为相应的二进制，然后递归调用函数即可，具体思路如下方代码所示： [USACO1.5] [IOI1994]数字三角形 Number Triangles [P1216 USACO1.5][IOI1994]数字三角形 Number Triangles - 洛谷 题目描述 观察下面的数字金字塔。 写一个程序来查找从最高点到底部任意处结束的路径，使路径经过数字的和最大。每一步可以走到左下方的点也可以到达右下方的点。 在上面的样例中,从 7→3→8→7→57 \\to 3 \\to 8 \\to 7 \\to 57→3→8→7→5 的路径产生了最大 输入格式 第一个行一个正整数 rrr ,表示行的数目。 后面每行为这个数字金字塔特定行包含的整数。 输出格式 单独的一行,包含那个可能得到的最大的和。 样例 #1 样例输入 #1 样例输出 #1 提示 【数据范围】 对于 100%100\\%100% 的数据，1≤r≤10001\\le r \\le 10001≤r≤1000，所有输入在 [0,100][0,100][0,100] 范围内。 解题过程 我第一个想到的方法是记忆化递归，自顶向下即可实现求解最大值。 但是发现竟然卡了一个测试集，看来还得将递归转成动态规划： 递归转动态规划只需要将递归函数替换为循环即可，同时要自底向上得去寻找路径： [NOIP2005 普及组] 采药 [P1048 NOIP2005 普及组] 采药 - 洛谷 题目描述 辰辰是个天资聪颖的孩子，他的梦想是成为世界上最伟大的医师。为此，他想拜附近最有威望的医师为师。医师为了判断他的资质，给他出了一个难题。医师把他带到一个到处都是草药的山洞里对他说：“孩子，这个山洞里有一些不同的草药，采每一株都需要一些时间，每一株也有它自身的价值。我会给你一段时间，在这段时间里，你可以采到一些草药。如果你是一个聪明的孩子，你应该可以让采到的草药的总价值最大。” 如果你是辰辰，你能完成这个任务吗？ 输入格式 第一行有 222 个整数 TTT（1≤T≤10001 \\le T \\le 10001≤T≤1000）和 MMM（1≤M≤1001 \\le M \\le 1001≤M≤100），用一个空格隔开，TTT 代表总共能够用来采药的时间，MMM 代表山洞里的草药的数目。 接下来的 MMM 行每行包括两个在 111 到 100100100 之间（包括 111 和 100100100）的整数，分别表示采摘某株草药的时间和这株草药的价值。 输出格式 输出在规定的时间内可以采到的草药的最大总价值。 样例 #1 样例输入 #1 样例输出 #1 提示 【数据范围】 对于 30%30\\%30% 的数据，M≤10M \\le 10M≤10； 对于全部的数据，M≤100M \\le 100M≤100。 【题目来源】 NOIP 2005 普及组第三题 解题过程 可以说这又是一道非常标准的模板题，即0-1背包，其实现思想如下：（过于基础，就不细说了） [NOIP2006 普及组] 开心的金明 P1060 NOIP2006 普及组] 开心的金明 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 题目描述 金明今天很开心，家里购置的新房就要领钥匙了，新房里有一间他自己专用的很宽敞的房间。更让他高兴的是，妈妈昨天对他说：“你的房间需要购买哪些物品，怎么布置，你说了算，只要不超过NNN元钱就行”。今天一早金明就开始做预算，但是他想买的东西太多了，肯定会超过妈妈限定的NNN元。于是，他把每件物品规定了一个重要度，分为555等：用整数1−51-51−5表示，第555等最重要。他还从因特网上查到了每件物品的价格（都是整数元）。他希望在不超过NNN元（可以等于NNN元）的前提下，使每件物品的价格与重要度的乘积的总和最大。 设第jjj件物品的价格为v[j]v[j]v[j]，重要度为w[j]w[j]w[j]，共选中了kkk件物品，编号依次为j1,j2,…,jkj_1,j_2,…,j_kj1​,j2​,…,jk​，则所求的总和为： v[j1]×w[j1]+v[j2]×w[j2]+…+v[jk]×w[jk]v[j_1] \\times w[j_1]+v[j_2] \\times w[j_2]+ …+v[j_k] \\times w[j_k]v[j1​]×w[j1​]+v[j2​]×w[j2​]+…+v[jk​]×w[jk​]。 请你帮助金明设计一个满足要求的购物单。 输入格式 第一行，为222个正整数，用一个空格隔开：n,mn,mn,m（其中N(&lt;30000)N(&lt;30000)N(&lt;30000)表示总钱数，m(&lt;25)m(&lt;25)m(&lt;25)为希望购买物品的个数。） 从第222行到第m+1m+1m+1行，第jjj行给出了编号为j−1j-1j−1的物品的基本数据，每行有222个非负整数$ v p（其中（其中（其中v表示该物品的价格表示该物品的价格表示该物品的价格(v \\le 10000)，，，p$表示该物品的重要度(1−51-51−5) 输出格式 111个正整数，为不超过总钱数的物品的价格与重要度乘积的总和的最大值(&lt;100000000)(&lt;100000000)(&lt;100000000)。 样例 #1 样例输入 #1 样例输出 #1 提示 NOIP 2006 普及组 第二题 解题过程 我们不妨换个思想理解该题，用N元钱可以买来最多的重要度，那么这就是一道典型的贪心的题目，但是显然重要度是一个整体，是不可分的，那么这就变成了一道动态规划的问题，解法如下： 可以发现此题解法与上一道题基本相同，我们需要辨别题目所对应的算法类型。 ","link":"https://2006wzt.github.io/post/算法刷题小记/"},{"title":"机器学习实战（十四）：树模型","content":"树模型 树模型在机器学习中至关重要，它不仅本身具有较好的性能，也可以用于优化其他的算法。 我们在本节将要介绍优化 KNN ~KNN~ KNN 算法的树模型以及决策树。 一、 KNN ~KNN~ KNN 的数据结构 在KNN算法中我们要找到测试点的最近的K个邻居，但是这需要我们求解所有点与测试点之间的距离（我们称这个过程为线性扫描），在数据集很大时这显然是不合理的，为此我们需要在此讨论以下KNN算法的数据结构。 1.1 时间复杂度 我们首先回顾一下 KNN ~KNN~ KNN 算法的时间复杂度，设数据集大小为 n ~n~ n ，特征向量维度为 d ~d~ d ，则对一个点进行分类的时间复杂度为： O(nd) ~O(nd)~ O(nd) 显然，随着数据集的增大，计算量将变得巨大，导致算法运行速度很慢，这并不是我们想看到的 我们希望找到一个较好的数据结构，使得对测试点进行分类时不再需要遍历每一个点。 1.2 KD ~KD~ KD 树 KD ~KD~ KD （K-Dimensional）树是一种对 k ~k~ k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构，它是一种二叉树，表示对 k ~k~ k 维空间的一个划分。 构造 KD ~KD~ KD 树相当于不断地用超平面将 k ~k~ k 维空间划分，构成一系列的 k ~k~ k 维超矩形区域， KD ~KD~ KD 树的每一个结点对应一个超矩形。 1.2.1 构造KD树 构造 KD ~KD~ KD 树的方法如下： ① 构造根节点，根节点对应特征空间中包含所有实例点的超矩形区域。 ② 在超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，由此确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前的超矩形区域分为左右两个子区域，这时该超矩形内的实例被分到了两个子区域，生成两个子节点。 ③ 对每个结点重复执行②操作直到子区域内不再存在实例，由此得到的结点为叶子结点。 构造 KD ~KD~ KD 树的算法的形式化定义如下： 输入： k~k k维空间数据集为： D={(x1,y1),(x2,y2),...,(xn,yn)}D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\} D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)} 其中我们将特征向量对应的数据集记为： T={x1,x2,...,xn}T=\\{x_1,x_2,...,x_n\\} T={x1​,x2​,...,xn​} 其中的特征向量为 k ~k~ k 维向量： xi=[ xi(1),xi(2),...,xi(k) ]Tx_i=[~x_i^{(1)},x_i^{(2)},...,x_i^{(k)}~]^T xi​=[ xi(1)​,xi(2)​,...,xi(k)​ ]T （1）开始：构造根节点：根节点对应包含 T ~T~ T 的 k ~k~ k 维空间的超矩形区域。 选择 x(1) ~x^{(1)}~ x(1) 为坐标轴，以 T ~T~ T 中所有实例的 x(1) ~x^{(1)}~ x(1) 坐标的中位数为切分点，将根节点对应的超矩形区域分为两个子区域。 切分由通过切分点并与坐标轴 x(1) ~x^{(1)}~ x(1) 垂直的超平面实现，由根节点生成深度为1的左、右子节点： 切分点：xp=median(x(1))左子节点中的实例：x(1)&lt;xp右子节点中的实例：x(1)&gt;xp\\begin{aligned} &amp;切分点：x_p= {median}(x^{(1)})\\\\ &amp;左子节点中的实例：x^{(1)}&lt;x_p\\\\ &amp;右子节点中的实例：x^{(1)}&gt;x_p \\end{aligned} ​切分点：xp​=median(x(1))左子节点中的实例：x(1)&lt;xp​右子节点中的实例：x(1)&gt;xp​​ 将落在切分超平面上的实例点保存为根节点。 （2）重复：对深度为 j ~j~ j 的结点，选择 x(l) ~x^{(l)}~ x(l) 为切分的坐标轴， l=(jmod k)+1 ~l=(j\\mod k)+1~ l=(jmodk)+1 ，以该节点区域中所有实例的 x(l) ~x^{(l)}~ x(l) 的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴 x(l) ~x^{(l)}~ x(l) 垂直的超平面实现。 由该节点生成深度为 j+1 ~j+1~ j+1 的左、右子节点： 切分点：xp=median(x(l))左子节点中的实例：x(l)&lt;xp右子节点中的实例：x(l)&gt;xp\\begin{aligned} &amp;切分点：x_p= {median}(x^{(l)})\\\\ &amp;左子节点中的实例：x^{(l)}&lt;x_p\\\\ &amp;右子节点中的实例：x^{(l)}&gt;x_p \\end{aligned} ​切分点：xp​=median(x(l))左子节点中的实例：x(l)&lt;xp​右子节点中的实例：x(l)&gt;xp​​ 将落在切分超平面上的实例点保存在该节点。 对于维度的选择还有另一个方法，即选择方差最大的维度去进行划分，这样可能会划分得更好。 （3）直到两个子区域没有实例存在时停止，从而形成 KD ~KD~ KD 树的区域划分。 我们不妨在 2 ~2~ 2 维空间模拟一下 KD ~KD~ KD 树构造的过程： T={(2,3)T,(5,4)T,(9,6)T,(4,7)T,(8,1)T,(7,2)T}T=\\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\\} T={(2,3)T,(5,4)T,(9,6)T,(4,7)T,(8,1)T,(7,2)T} 其构造过程如下： ①选择根节点： x(1) ~x^{(1)}~ x(1) 所对应的切分点： (5,4) ~(5,4)~ (5,4) 或 (7,2) ~(7,2)~ (7,2) ，我们不妨选择 (7,2) ~(7,2)~ (7,2) 左子区域包含： (2,3),(4,7),(5,4) ~(2,3),(4,7),(5,4)~ (2,3),(4,7),(5,4) ，右子区域包含： (8,1),(9,6) ~(8,1),(9,6)~ (8,1),(9,6) ②对深度为 1 ~1~ 1 的结点继续划分： l=(1mod 2)+1=2 ~l=(1\\mod 2)+1=2~ l=(1mod2)+1=2 ，以 x(2) ~x^{(2)}~ x(2) 为基准进行划分 左子区域切分点： (5,4) ~(5,4)~ (5,4) ，右子区域切分点： (9,6) ~(9,6)~ (9,6) ③对深度为 2 ~2~ 2 的结点继续划分： l=(2mod 2)+1=1 ~l=(2\\mod 2)+1=1~ l=(2mod2)+1=1 ，以 x(1) ~x^{(1)}~ x(1) 为基准进行划分 由此得到三个新的切分点： (2,3),(4,7),(8,1) ~(2,3),(4,7),(8,1)~ (2,3),(4,7),(8,1) 由此得到的划分如下图所示： 得到的 KD ~KD~ KD 树如下： 注意：我们在实际的算法中往往并不会使用全部的实例点去构造 KD ~KD~ KD 树，因为这样的时间复杂度很高，往往选取部分点对区域进行划分 1.2.2 搜索KD树 我们构造KD树的目的还是用于进行分类，因此我们需要思考如何搜索KD树来进行分类，k-近邻的搜索方式如下： ①对于给定的测试点 xt ~x_t~ xt​ ，我们首先在 KD ~KD~ KD 树中找到包含该测试点的叶子结点 ②从该结点出发，依次退回到父节点，不断查找与目标点最邻近的结点 ③当确定不可能存在更近的结点时中止，这样搜索区域便被限制在空间的局部区域上了 为了更加直观得理解该算法，我们进行详细的分析：以 1−NN ~1-NN~ 1−NN 为例 输入：已构造的 KD ~KD~ KD 树，目标点 xt ~x_t~ xt​ 输出： xt ~x_t~ xt​ 的最近邻 （1）在 KD ~KD~ KD 树中找到包含目标点的叶子节点：寻找方法很容易，只需要从根节点开始递归得访问 KD ~KD~ KD 树，如果 xt(l)&lt;xp ~x_t^{(l)}&lt;x_p~ xt(l)​&lt;xp​ 则转到左子结点，反之则转到右子结点，直到子节点为叶子结点为止。 （2）此叶子结点为“当前最近结点”，递归得向上回退，对每个结点进行如下操作： ① 如果该结点保存的实例点比“当前最近结点”离目标点 xt ~x_t~ xt​ 距离更近，则以当前结点为“当前最近结点” ② 当前最近点一定存在于该结点的一个子结点对应的区域，检查该子结点对应的父结点的另一子结点对应的区域中是否存在更近的点。 具体地，检查另一子结点对应地区域是否与以目标点为球心、以目标点与当前最近结点的距离为半径的超球体相交，如果相交则可能存在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着递归得进行搜索。 如果不相交则向上回退。 （3）当回退到根节点时，搜索结束。最后的“当前最近结点”记为 xt ~x_t~ xt​ 的最近邻点。 如果实例点是随机分布的，则 KD ~KD~ KD 树搜索的平均计算时间复杂度为 O(log⁡n) ~O(\\log n)~ O(logn) ， KD ~KD~ KD 树更加适合训练实例数 n ~n~ n 远大于空间维数 k ~k~ k 时的 k ~k~ k 近邻搜索，当训练实例数接近特征空间维度数时它则接近于线性扫描。 我们以下图为例： A ~A~ A 为根节点，子结点为 B、C ~B、C~ B、C ，目标点为 S ~S~ S 得到的 KD ~KD~ KD 树如下： （1） 首先，我们找到了 S ~S~ S 位于区域②，因此得到“当前最近结点”为 D ~D~ D （2） 然后，检查叶子节点②的父节点 B ~B~ B 的另一子结点①，发现①没有与超球体相交，则不需要检查 （3） 继续，返回父节点 A ~A~ A ，发现结点 C ~C~ C 对应的区域中④与超球体相交，对④进行搜索找到了更近的点 E ~E~ E （4） 最终，我们得到了 S ~S~ S 的最近邻为 E ~E~ E 1.2.3 代码实现 手动实现的代码更加灵活，但鉴于笔者才疏学浅，我的选择往往是调库，手动实现是为了加强理解。 首先我们定义树节点类：代码实现参考了文章 k近邻算法之kd树优化 接着我们定义用于构造和搜索Kd树的类： 由此我们可以定义以 Kd ~Kd~ Kd 树为数据结构的优化后的 KNN ~KNN~ KNN 模型： 注意，我们上述实现的代码与我们的举例存在不同，一方面我们选择切分维度的方法是选择方差最大的那个维度，另外我们搜索 KD ~KD~ KD 树时不再是寻找最近邻，而是寻找 k ~k~ k 近邻，区别在于我们添加了一个数组存储当前寻找到的 k ~k~ k 个近邻，超球体半径是第 k ~k~ k 小的数据点与目标点之间的距离，这样实现的代码更具有普适性。 我们可以用鸢尾花数据集对上述模型进行验证： 可以发现正确率为0.967，模型效果良好，上述手动实现过程还是比较复杂的，在算法竞赛过程中为了提高编码效率，还是调库效率较高，不过如果涉及到算法的优化的话，面向手动实现的代码进行分析更有优势。 1.2.4 总结 ① KD树是一种二叉树，其中每个节点都是一个k维点。 ② 可以将每个非叶节点视为隐式生成一个拆分超平面，该超平面将空间拆分为两部分，称为半空间。 ③ 此超平面左侧的点由该节点的左子树表示，而超平面右侧的点由右子树表示。 ④ 超平面方向的选择方式如下：树中的每个节点都与k维度中的一个维度相关联，超平面垂直于该维度的轴。 1.3 球树 球树类似于KD树，但是不用超平面对特征空间进行分割，而是用超球面进行分割 ball结构允许我们沿着点所在的底层流形对数据进行分区，而不是重复剖析整个特征空间（如KD树） 1.3.1 伪码实现 球树的构造伪码如下图所示，因为其构造过程类似于 KD ~KD~ KD 树，所以在此不再手动实现（笔者此刻不愿意coding了 TAT） 1.3.2 球树应用 球树的作用与 KD ~KD~ KD 树相同，都是对 KNN ~KNN~ KNN 的数据结构进行优化， KD ~KD~ KD 树适用于低维空间，球树适用于高维空间。 ① KNN ~KNN~ KNN 在测试过程中很慢，因为它做了很多不必要的工作。 ② KD ~KD~ KD 树对特征空间进行分区，这样我们就可以排除距离最近的 k ~k~ k 个邻居更远的整个分区。 但是，拆分是轴对齐的，无法很好地延伸到更高的维度。 ③ 球树划分了点所在的流形，而不是整个空间。这使得它在更高的维度上表现得更好。 二、决策树 2.1 核心思想 假设我们进行一个二分类问题，如果我们知道一个测试点属于一个100万个点的集群，所有这些点的标签都为正值，那么在我们计算测试点到这100万个距离中的每一个点的距离之前，我们也会知道它的邻居将为正值，由此就有了决策树的思想。 决策树的构建过程，我们不存储训练数据，而是使用训练数据来构建一个树结构，该结构递归地将空间划分为具有类似标签的区域。 决策树特点： ① 首先，决策树的根节点对应整个训练集 ② 然后，通过一个简单的阈值 t ~t~ t ，将该集合沿一个维度 l ~l~ l 大致分成两半。 ③ x(l)≥t ~x^{(l)}≥ t~ x(l)≥t 的数据点落在右子节点中，其他所有节点落在左子节点中。 ④ 选择阈值 t ~t~ t 和维度 l ~l~ l ，以便生成的子节点在类成员方面更纯粹。 ⑤ 理想情况下，所有的正节点都属于一个子节点，所有的负节点都属于另一个子节点。 ⑥ 满足上述条件后，则完成决策的构建，否则要继续对叶子结点进行分割，直到所有叶子结点都属于一个类或不再可分 决策树在KNN之上的优点： ① 决策树构建之后我们便不再需要存储各个训练数据，只需要存储所有叶子结点的标签 ② 决策树在测试期间速度非常快，因为测试输入只需遍历树到一片叶子，预测是叶子的主要标签 ③ 决策树不需要度量，因为分割基于特征阈值而不是距离。 2.2 构造决策树 我们所要构建的决策树的目标是： ① 使得决策树最大紧凑化 ② 使得叶子结点都只包含一种标签的结点 要找到一棵最小化的树是一个NP完全问题，但是我们可以用贪婪策略非常有效地近似它。 我们不断拆分数据，以最小化杂质函数，该函数用于测量子对象中的标签纯度。 我们首先了解一下构造决策树用到的一些重要概念。 2.2.1 基尼系数 首先我们假设数据集 S ~S~ S 为： S={(x1,y1),(x2,y2),...,(xn,yn)}yi∈{1,2,...,c}S=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}\\\\ y_i\\in\\{1,2,...,c\\} S={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)}yi​∈{1,2,...,c} 接着定义数据集的子集 Sk ~S_k~ Sk​ ： Sk={(x,y)∣y=k}S=S1∪S2∪...∪ScS_k=\\{(x,y)|y=k\\}\\\\ S=S_1\\cup S_2\\cup...\\cup S_c Sk​={(x,y)∣y=k}S=S1​∪S2​∪...∪Sc​ 然后我们可以定义输入分数 pk ~p_k~ pk​ ： pk=∣Sk∣∣S∣p_k=\\frac{|S_k|}{|S|} pk​=∣S∣∣Sk​∣​ 基尼不纯度：表示在样本集合中一个随机选中的样本被分错的概率，则整个数据集的基尼不纯度为： Gini(S)=∑k=1cpk(1−pk){Gini}(S)=\\sum_{k=1}^cp_k(1-p_k) Gini(S)=k=1∑c​pk​(1−pk​) 显然当数据集 S ~S~ S 中的标签只有一个时， G(S)=0 ~G(S)=0~ G(S)=0 ，同时我们也可以定义决策树的基尼不纯度： GiniT(S)=∣SL∣∣S∣GiniT(SL)+∣SR∣∣S∣GiniT(SR) {Gini}^T(S)=\\frac{|S_L|}{|S|} {Gini}^T(S_L)+\\frac{|S_R|}{|S|} {Gini}^T(S_R) GiniT(S)=∣S∣∣SL​∣​GiniT(SL​)+∣S∣∣SR​∣​GiniT(SR​) 2.2.2 信息熵 信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。直到1948年，香农提出了**“信息熵”**的概念，才解决了对信息的量化度量问题。信息熵这个词是香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。信源的不确定性越大，信息熵也越大。 从机器学习的角度来看，信息熵表示的是信息量的期望值，我们的假设与杂质函数的假设相同，则信息量的定义如下： Ik=log⁡pkI_k=\\log p_k Ik​=logpk​ 由于信息熵是信息量的期望值，所以信息熵 H(S) ~H(S)~ H(S) 的定义如下：信息熵反映的是不确定性 H(S)=−∑k=1cpklog⁡pkH(S)=-\\sum_{k=1}^cp_k\\log p_k H(S)=−k=1∑c​pk​logpk​ 同理我们可以定义决策树的熵： HT(S)=∣SL∣∣S∣HT(SL)+∣SR∣∣S∣HT(SR)H^T(S)=\\frac{|S_L|}{|S|}H^T(S_L)+\\frac{|S_R|}{|S|}H^T(S_R) HT(S)=∣S∣∣SL​∣​HT(SL​)+∣S∣∣SR​∣​HT(SR​) 在实际的场景中，我们可能需要研究数据集中某个特征等于某个值时的信息熵等于多少，这个时候就需要用到条件熵。条件熵 H(Y∣X) ~H(Y|X)~ H(Y∣X) 表示特征 X ~X~ X 为某个值的条件下，标签集为 Y ~Y~ Y 的熵。条件熵的计算公式如下： H(D,X)=H(Y∣X)=∑x∈Xp(x)H(Y∣X=x)=−∑x∈Xp(x)∑y∈Yp(y∣x)log⁡p(y∣x)\\begin{aligned} &amp;H(D,X)=H(Y|X)=\\sum_{x\\in X}p(x)H(Y|X=x)=-\\sum_{x\\in X}p(x)\\sum_{y\\in Y}p(y|x)\\log p(y|x) \\end{aligned} ​H(D,X)=H(Y∣X)=x∈X∑​p(x)H(Y∣X=x)=−x∈X∑​p(x)y∈Y∑​p(y∣x)logp(y∣x)​ 现在已经知道了什么是熵，什么是条件熵，接下来就可以看看什么是信息增益了。 所谓的信息增益就是表示我已知条件X后能得到信息Y的不确定性的减少程度。 就好比，我在玩读心术。你心里想一件东西，我来猜。我已开始什么都没问你，我要猜的话，肯定是瞎猜。这个时候我的熵就非常高。然后我接下来我会去试着问你是非题，当我问了是非题之后，我就能减小猜测你心中想到的东西的范围，这样其实就是减小了我的熵。那么我熵的减小程度就是我的信息增益。 所以信息增益如果套上机器学习的话就是，如果把特征A对训练集S的信息增益记为g(D, A)的话，那么g(D, A)的计算公式就是： g(D,A)=H(D)−H(D,A)g(D,A)=H(D)-H(D,A) g(D,A)=H(D)−H(D,A) 我们不妨通过一个例子理解上述与熵相关的概念：我们有如下&quot;客户流失数据集&quot;，0代表未流失，1代表流失 编号 性别 活跃度 是否流失 1 男 高 0 2 女 中 0 3 男 低 1 4 女 高 0 5 男 高 0 6 男 中 0 7 男 中 1 8 女 中 0 9 女 低 1 10 女 中 0 11 女 高 0 12 男 低 1 13 女 低 1 14 男 高 0 15 男 高 0 假如要算性别和活跃度这两个特征的信息增益的话，首先要先算总的信息熵和条件熵。 计算总的信息熵很简单：15条数据中标签为0的有10个，标签为1的有5个 yi∈{0,1}p0=∣S0∣∣S∣=1015=23p1=∣S1∣∣S∣=515=13y_i\\in\\{0,1\\}\\\\ p_0=\\frac{|S_0|}{|S|}=\\frac{10}{15}=\\frac23\\\\ p_1=\\frac{|S_1|}{|S|}=\\frac{5}{15}=\\frac13 yi​∈{0,1}p0​=∣S∣∣S0​∣​=1510​=32​p1​=∣S∣∣S1​∣​=155​=31​ 则可得总信息熵为： H(S)=−∑k=01pklog⁡pk=−23log⁡23−13log⁡13≈0.9182H(S)=-\\sum_{k=0}^1p_k\\log p_k=-\\frac23\\log\\frac23-\\frac13\\log\\frac13\\approx0.9182 H(S)=−k=0∑1​pk​logpk​=−32​log32​−31​log31​≈0.9182 接下来就是条件熵的计算，以性别为男的熵为例。表格中性别为男的数据有8条，这8条数据中有3条数据的标签为1，有5条数据的标签为0。所以根据条件熵的计算公式能够得出该条件熵为： H(Y∣gender=man)=−38log⁡38−58log⁡58≈0.9543H(Y| {gender=man})=-\\frac38\\log\\frac38-\\frac58\\log\\frac58\\approx0.9543 H(Y∣gender=man)=−83​log83​−85​log85​≈0.9543 同理，我们也可以计算出性别为女时的条件熵： H(Y∣gender=woman)=−27log⁡27−57log⁡57≈0.8631H(Y| {gender=woman})=-\\frac27\\log\\frac27-\\frac57\\log\\frac57\\approx0.8631 H(Y∣gender=woman)=−72​log72​−75​log75​≈0.8631 由此可得总的条件熵为： H(Y∣gender)=p(gender=max)H(Y∣gender=man)+p(gender=woman)H(Y∣gender=woman) =815×0.9543+715×0.8631≈0.9117\\begin{aligned} &amp;H(Y| {gender})=p( {gender=max})H(Y| {gender=man})+p( {gender=woman})H(Y| {gender=woman})\\\\ &amp;~~~~~~~~~~~~~~~~~~~~~~~~=\\frac8{15}\\times0.9543+\\frac7{15}\\times0.8631\\approx0.9117 \\end{aligned} ​H(Y∣gender)=p(gender=max)H(Y∣gender=man)+p(gender=woman)H(Y∣gender=woman) =158​×0.9543+157​×0.8631≈0.9117​ 接着我们可以按照相同的方法计算活跃度的条件熵： H(Y∣activation=low)=−44log⁡44−0=0H(Y∣activation=mid)=−45log⁡45−15log⁡15≈0.7219H(Y∣activation=high)=−66log⁡66−0=0H(Y∣activation)=515H(Y∣activation=mid)≈0.2406\\begin{aligned} &amp;H(Y| {activation=low})=-\\frac44\\log\\frac44-0=0\\\\ &amp;H(Y| {activation=mid})=-\\frac45\\log\\frac45-\\frac15\\log\\frac15\\approx0.7219\\\\ &amp;H(Y| {activation=high})=-\\frac66\\log\\frac66-0=0\\\\ &amp;H(Y| {activation})=\\frac5{15} H(Y| {activation=mid})\\approx0.2406 \\end{aligned} ​H(Y∣activation=low)=−44​log44​−0=0H(Y∣activation=mid)=−54​log54​−51​log51​≈0.7219H(Y∣activation=high)=−66​log66​−0=0H(Y∣activation)=155​H(Y∣activation=mid)≈0.2406​ 由此可得性别和活跃度两个特征的信息增益： g(S,gender)=H(S)−H(Y∣gender)=0.9182−0.9117=0.0065g(S,activation)=H(S)−H(Y∣activation)=0.9182−0.2406=0.6776\\begin{aligned} &amp;g(S, {gender})=H(S)-H(Y| {gender})=0.9182-0.9117=0.0065\\\\ &amp;g(S, {activation})=H(S)-H(Y| {activation})=0.9182-0.2406=0.6776 \\end{aligned} ​g(S,gender)=H(S)−H(Y∣gender)=0.9182−0.9117=0.0065g(S,activation)=H(S)−H(Y∣activation)=0.9182−0.2406=0.6776​ 那信息增益算出来之后有什么意义呢？回到读心术的问题，为了我能更加准确的猜出你心中所想，我肯定是问的问题越好就能猜得越准！换句话来说我肯定是要想出一个信息增益最大（减少不确定性程度最高）的问题来问你，显然上述两个特征中活跃度的信息增益最高，而这也是 ID3 ~ID3~ ID3 算法的基本思想。 同时支持 ID3 ~ID3~ ID3 算法的的一个定理为：信息增益一定非负，相关证明可以参考 如何证明信息增益一定大于0？ 2.2.3 ID3 ~ID3~ ID3 算法 （1）中止条件： ID3 ~ID3~ ID3 算法的终止条件为： ①子集中的所有数据点具有相同的标签 y ~y~ y ，停止拆分，并创建一个标签为 y ~y~ y 的叶子节点 ②没有更多的特征用于切分子集，比如两个数据点的特征向量相同但是标签不同，停止拆分，并创建一个标签为最常见标签的叶子节点 （2）算法过程： 比如我们辨别西瓜好坏的决策树如下： 很明显上述的 ID3 ~ID3~ ID3 算法是比较适合具有多项式特征的数据集的，而对于具有连续型的特征数据并不推荐该算法。 算法实现如下：首先我们用到的库为： 决策树模型为： 决策树分类器为： 验证模型时，我们用到的数据集为西瓜好坏数据集：watermelon20.xlsx 最终我们得到的决策树如下图所示： 算法一个明显的弊端是无法处理包含在训练集中未出现过的特征取值的测试点，这种情况经常出现在具有连续型特征的数据集上，这要求我们的训练集要足够大，保证对各种取值的覆盖，我们后面的 CART ~CART~ CART 算法会应对这个问题。 2.2.4 C4.5 ~C4.5~ C4.5 算法 C4.5 ~C4.5~ C4.5 算法是对 ID3 ~ID3~ ID3 算法的扩展，它们的区别在于 ID3 ~ID3~ ID3 每次选择信息增益最大的特征进行划分，而 C4.5 ~C4.5~ C4.5 每次选择信息增益率最大的特征进行划分，实现 C4.5 ~C4.5~ C4.5 算法只需要修改上述代码中的计算部分。 由于在使用信息增益这一指标进行划分时，更喜欢可取值数量较多的特征。为了减少这种偏好可能带来的不利影响，Ross Quinlan使用了信息增益率这一指标来选择最优划分属性，信息增益率的定义如下： 设数据集为 D ~D~ D ，某一特征为 A ~A~ A ， Gain(D,A) ~ {Gain}(D,A)~ Gain(D,A) 为信息增益， V ~V~ V 表示特征 A ~A~ A 取值的集合，则信息增益率定义如下： Gainratio(D,A)=Gain(D,A)−∑v∈V∣Dv∣∣D∣log⁡∣Dv∣∣D∣ {Gain ratio}(D,A)=\\frac{ {Gain}(D,A)}{-\\sum_{v\\in V}\\frac{|D^v|}{|D|}\\log\\frac{|D^v|}{|D|}} Gainratio(D,A)=−∑v∈V​∣D∣∣Dv∣​log∣D∣∣Dv∣​Gain(D,A)​ 还记得我们刚刚举的例子吗，我们回到客户流失数据集中，可以很容易得计算信息增益率： Gain(D,gender)=0.0065Gain(D,activation)=0.6776\\begin{aligned} &amp; {Gain}(D, {gender})=0.0065\\\\ &amp; {Gain}(D, {activation})=0.6776\\\\ \\end{aligned} ​Gain(D,gender)=0.0065Gain(D,activation)=0.6776​ 15条数据中8条是男性，7条是女性；4条低活跃度，5条中活跃度，6条高活跃度： Gainratio(D,gender)=Gain(D,gender)−815log⁡815−715log⁡715≈0.0065Gainratio(D,activation)=Gain(D,activation)−415log⁡415−515log⁡515−615log⁡615≈0.4238\\begin{aligned} &amp; {Gain ratio}(D, {gender})=\\frac{ {Gain}(D, {gender})}{-\\frac8{15}\\log\\frac8{15}-\\frac7{15}\\log\\frac7{15}}\\approx0.0065\\\\ &amp; {Gain ratio}(D, {activation})=\\frac{ {Gain}(D, {activation})}{-\\frac4{15}\\log\\frac4{15}-\\frac5{15}\\log\\frac5{15}-\\frac6{15}\\log\\frac6{15}}\\approx 0.4238 \\end{aligned} ​Gainratio(D,gender)=−158​log158​−157​log157​Gain(D,gender)​≈0.0065Gainratio(D,activation)=−154​log154​−155​log155​−156​log156​Gain(D,activation)​≈0.4238​ 我们可以发现活跃度的信息增益率要比信息增益小很多，这就是 C4.5 ~C4.5~ C4.5 算法的特点。 实现 C4.5 ~C4.5~ C4.5 算法仅需要修改 ID3 ~ID3~ ID3 算法的 calcInfoGain 函数： 值得一提的是：当信息增益为0时，对应的信息增益率的底数也为0，在编写函数时需要注意避免分母为0的情况，同时上述两个算法都可以优化成可以处理具有连续型特征的数据集的算法，只需要将划分不同取值分支的过程改为选择阈值的过程，这也是 CART ~CART~ CART 算法的思想，所以我们不再过多赘述，该思想将会在 CART ~CART~ CART 算法中实现。 最终我们得到的决策树如下字典所示：可以发现对于我们的西瓜数据集来说两个算法得到的决策树相同 2.2.5 CART ~CART~ CART 算法 CART ~CART~ CART 即 Classification and Regression Trees，它既可以作为分类树也可以作为回归树，并且它只能是二叉树。 在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益率来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？当然有！那就是基尼系数！ CART算法使用基尼系数来代替信息增益率，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益与信息增益率是相反的(它们都是越大越好)。 基尼系数的计算方式如下：数据集为 D ~D~ D ， pk ~p_k~ pk​ 表示第 k ~k~ k 个类别在数据集中所占的比例 Gini(D)=∑k=1cpk(1−pk)=1−∑k=1cpk2 {Gini}(D)=\\sum_{k=1}^cp_k(1-p_k)=1-\\sum_{k=1}^cp_k^2 Gini(D)=k=1∑c​pk​(1−pk​)=1−k=1∑c​pk2​ 我们还是以客户流失数据集为例：15条数据中，10条标签为0，5条标签为1，则有： Gini(D)=1−(p02+p12)=1−[(23)2+(13)2]≈0.4444 {Gini}(D)=1-(p_0^2+p_1^2)=1-\\big[(\\frac23)^2+(\\frac13)^2\\big]\\approx0.4444 Gini(D)=1−(p02​+p12​)=1−[(32​)2+(31​)2]≈0.4444 同时还有基于数据集 D ~D~ D 和特征 A ~A~ A 的 Gini ~ {Gini}~ Gini 系数， V ~V~ V 表示特征 A ~A~ A 取值的集合，则定义如下：计算过程类似于条件熵的计算 Gini(D,A)=∑v∈V∣Dv∣∣D∣Gini(Dv) {Gini}(D,A)=\\sum_{v\\in V}\\frac{|D^v|}{|D|} {Gini}(D^v) Gini(D,A)=v∈V∑​∣D∣∣Dv∣​Gini(Dv) 我们以客户流失数据集为例： ① 性别特征：15条数据中，8条男性，7条女性；男性数据中，5条标签0，3条标签1；女性数据中，5条标签0，2条标签1 ∣D∣=15,∣Dman∣=8,∣Dwoman∣=7Gini(Dman)=1−[(58)2+(38)2]≈0.46875Gini(Dwoman)=1−[(57)2+(27)2]≈0.40816Gini(D,gender)=815×0.46875+715×0.40816≈0.44048\\begin{aligned} &amp;|D|=15,|D^{ {man}}|=8,|D^{ {woman}}|=7\\\\ &amp; {Gini}(D^{ {man}})=1-\\big[(\\frac58)^2+(\\frac38)^2\\big]\\approx0.46875\\\\ &amp; {Gini}(D^{ {woman}})=1-\\big[(\\frac57)^2+(\\frac27)^2\\big]\\approx0.40816\\\\ &amp; {Gini}(D, {gender})=\\frac8{15}\\times 0.46875+\\frac7{15}\\times 0.40816\\approx0.44048 \\end{aligned} ​∣D∣=15,∣Dman∣=8,∣Dwoman∣=7Gini(Dman)=1−[(85​)2+(83​)2]≈0.46875Gini(Dwoman)=1−[(75​)2+(72​)2]≈0.40816Gini(D,gender)=158​×0.46875+157​×0.40816≈0.44048​ ② 活跃度特征计算同理： ∣D∣=15,∣Dlow∣=4,∣Dmid∣=5,∣Dhigh∣=6Gini(Dlow)=1−[(44)2+(04)2]=0Gini(Dmid)=1−[(45)2+(15)2]=0.32Gini(Dhigh)=1−[(66)2+(06)2]=0Gini(D,activation)=515×0.32=0.10667\\begin{aligned} &amp;|D|=15,|D^{ {low}}|=4,|D^{ {mid}}|=5,|D^{ {high}}|=6\\\\ &amp; {Gini}(D^{ {low}})=1-\\big[(\\frac44)^2+(\\frac04)^2\\big]=0\\\\ &amp; {Gini}(D^{ {mid}})=1-\\big[(\\frac45)^2+(\\frac15)^2\\big]=0.32\\\\ &amp; {Gini}(D^{ {high}})=1-\\big[(\\frac66)^2+(\\frac06)^2\\big]=0\\\\ &amp; {Gini}(D, {activation})=\\frac5{15}\\times 0.32=0.10667 \\end{aligned} ​∣D∣=15,∣Dlow∣=4,∣Dmid∣=5,∣Dhigh∣=6Gini(Dlow)=1−[(44​)2+(40​)2]=0Gini(Dmid)=1−[(54​)2+(51​)2]=0.32Gini(Dhigh)=1−[(66​)2+(60​)2]=0Gini(D,activation)=155​×0.32=0.10667​ 显然我们要选择活跃度特征，因为它的基尼系数小，不纯度更低。 当我们知道如何选择用于切分的特征后，应该思考如何在该特征上选择一个切分点，即如何寻找一个合适的阈值，在此我们面向连续型特征进行分析，而对于离散型的数据类比即可。 ① 首先，将数据集按照最优特征从大到小排列 ② 对于大小为 n ~n~ n 的样本，共有 n−1 ~n-1~ n−1 种切分方式，即有 n−1 ~n-1~ n−1 个切分点，但是这样切分计算量是很大的并且决策树不佳，我们将注意力放在切分的特征 A ~A~ A 上，设 A ~A~ A 有 m ~m~ m 种不同的取值，则我们只需要关注这 m−1 ~m-1~ m−1 个不同取值分界点处的切分即可，同时这样可以规避掉一个问题，即切分的阈值一定不属于所提供的数据集中的一个取值，不需要再考虑特征的取值等于阈值时，数据点划分到左子集还是右子集的问题，而在预测的时候如果出现特征取值等于阈值的情况可以考虑固定好搜索走向或者随机走到左右子树。 ③ 每个切分点将数据集划分为左右两部分： DL,DR ~D_L,D_R~ DL​,DR​ ，则该切分对应着一个基尼系数： Gini(DL,DR)=∣DL∣∣D∣Gini(DL)+∣DR∣∣D∣Gini(DR)\\begin{aligned} &amp; {Gini}(D_L,D_R)=\\frac{|D_L|}{|D|} {Gini}(D_L)+\\frac{|D_R|}{|D|} {Gini}(D_R) \\end{aligned} ​Gini(DL​,DR​)=∣D∣∣DL​∣​Gini(DL​)+∣D∣∣DR​∣​Gini(DR​)​ 找到基尼系数最小的切分方式，选择切分左右两个数据点特征的均值作为阈值。 我们通过代码实现基于 CART ~CART~ CART 算法的决策树模型：首先我们用到的库有： CART ~CART~ CART 树模型定义如下： 利用该分类树的分类器为： 我们可以利用鸢尾花数据集进行模型效果的验证：可以发现准确率很高，达到了0.96 可以发现 CART ~CART~ CART 算法的大体思想与 ID3 ~ID3~ ID3 和 C4.5 ~C4.5~ C4.5 算法相同，模型的实现也比较类似。 CART ~CART~ CART 是一个构造简单并且测试速度很快的树，但是它本身在准确性上并没竞争力，一些诸如 LightGBM 和 XGBoost 等高性能的第三方库提供的树模型具有更强大的性能，适用于机器学习竞赛中。 ","link":"https://2006wzt.github.io/post/机器学习实战（十四）：树模型/"},{"title":"机器学习实战（十三）：核函数","content":"核函数 一、核心思想 在前面我们所讨论的分类器中，基本都是线性分类器，但是当数据集不存在一个线性的决策边界时，线性分类器便无法很好得进行分类。 事实证明，有一种优雅的方法可以将非线性问题合并到大多数的线性分类器可解决的问题中，即将线性分类器非线性化，这便是核函数。 我们可以看一个经典的例子：如下图所示的数据集，显然它是不存在线性的决策边界的，但是我们可以通过函数 ϕ ~\\phi~ ϕ 对特征向量进行特征变换使得数据线性可分。 我们不妨将特征变换函数定义为： ϕ(x)=ϕ([x1,x2]T)=[x1,x2,∣x1⋅x2∣]T\\phi(x)=\\phi([x_1,x_2]^T)=[x_1,x_2,|x_1\\cdot x_2|]^T ϕ(x)=ϕ([x1​,x2​]T)=[x1​,x2​,∣x1​⋅x2​∣]T 我们所添加的维度捕捉了原始特征之间的非线性交互，使得数据变为了线性可分，升维的方法较为简便，并且使得问题保持凸且表现良好，但是缺点是升维可能会导致维度过高，使得模型变得复杂，比如下面这个例子所示： 通过特征变换，维度从 d ~d~ d 维变为了 2d ~2^d~ 2d 维，这种新的表示法 ϕ(x) ~\\phi(x)~ ϕ(x) 非常有表现力，允许复杂的非线性决策边界，但维数非常高。这使得我们的算法速度慢得令人无法忍受。 二、核技巧 核技巧是一种通过在更高维空间中学习函数来绕过这一困境的方法，而无需计算单个向量 ϕ(x) ~\\phi(x)~ ϕ(x) 或完整向量 w ~w~ w 。 2.1 梯度下降 我们考虑平方损失函数的梯度下降过程： l(w)=∑i=1n(wTxi−yi)2l(w)=\\sum_{i=1}^n(w^Tx_i-y_i)^2 l(w)=i=1∑n​(wTxi​−yi​)2 在梯度下降过程中，我们每次需要选择一个步长进行更新： wt+1=wt−s⋅(∂l(w)∂w)∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xiw_{t+1}=w_{t}-s\\cdot(\\frac{\\partial l(w)}{\\partial w})\\\\ \\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i wt+1​=wt​−s⋅(∂w∂l(w)​)∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​ 此处我们要引入一个重要的假设：我们可以将参数 w ~w~ w 表示为特征向量 xi ~x_i~ xi​ 的线性组合： w=∑i=1nαixiw=\\sum_{i=1}^n\\alpha_ix_i w=i=1∑n​αi​xi​ 根据该假设我们可以得到： wt+1 ~w_{t+1}~ wt+1​ 与 xi ~x_i~ xi​ 线性相关， wt ~w_t~ wt​ 与 xi ~x_i~ xi​ 线性相关，由此：梯度 ∂l(w)∂w ~\\frac{\\partial l(w)}{\\partial w}~ ∂w∂l(w)​ 与 xi ~x_i~ xi​ 线性相关： ∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xi=∑i=1nγixi\\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i=\\sum_{i=1}^n\\gamma_ix_i ∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​=i=1∑n​γi​xi​ 由于损失函数为凸函数，最终解与初始化无关我们可以将 w0 ~w_0~ w0​ 初始化为我们想要的任何值，我们不妨令： w0=[0,0,...,0]T,α0=[0,0,...,0]Tw_0=[0,0,...,0]^T,\\alpha^0=[0,0,...,0]^T w0​=[0,0,...,0]T,α0=[0,0,...,0]T 根据上述分析我们可以得到梯度下降的过程为： w1=w0−s⋅2∑i=1n(w0Txi−yi)xi=∑i=1nαi0xi−s∑i=1nγi0xi=∑i=1nαi1xi α1=α0−sγ0w2=w1−s⋅2∑i=1n(w1Txi−yi)xi=∑i=1nαi1xi−s∑i=1nγi1xi=∑i=1nαi2xi α2=α1−sγ1w3=w2−s⋅2∑i=1n(w2Txi−yi)xi=∑i=1nαi2xi−s∑i=1nγi2xi=∑i=1nαi2xi α3=α2−sγ2...wt=wt−1−s⋅2∑i=1n(wt−1Txi−yi)xi=∑i=1nαit−1xi−s∑i=1nγit−1xi=∑i=1nαitxi αt=αt−1−sγt−1\\begin{aligned} &amp;w_1=w_0-s\\cdot2\\sum_{i=1}^n(w_0^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^0x_i-s\\sum_{i=1}^n\\gamma_i^0x_i=\\sum_{i=1}^n\\alpha_i^1x_i~~~~\\alpha^1=\\alpha^0-s\\gamma^0\\\\ &amp;w_2=w_1-s\\cdot2\\sum_{i=1}^n(w_1^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^1x_i-s\\sum_{i=1}^n\\gamma_i^1x_i=\\sum_{i=1}^n\\alpha_i^2x_i~~~~\\alpha^2=\\alpha^1-s\\gamma^1\\\\ &amp;w_3=w_2-s\\cdot2\\sum_{i=1}^n(w_2^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^2x_i-s\\sum_{i=1}^n\\gamma_i^2x_i=\\sum_{i=1}^n\\alpha_i^2x_i~~~~\\alpha^3=\\alpha^2-s\\gamma^2\\\\ &amp;...\\\\ &amp;w_t=w_{t-1}-s\\cdot2\\sum_{i=1}^n(w_{t-1}^Tx_i-y_i)x_i=\\sum_{i=1}^n\\alpha_i^{t-1}x_i-s\\sum_{i=1}^n\\gamma_i^{t-1}x_i=\\sum_{i=1}^n\\alpha_i^tx_i~~~~\\alpha^t=\\alpha^{t-1}-s\\gamma^{t-1}\\\\ \\end{aligned} ​w1​=w0​−s⋅2i=1∑n​(w0T​xi​−yi​)xi​=i=1∑n​αi0​xi​−si=1∑n​γi0​xi​=i=1∑n​αi1​xi​ α1=α0−sγ0w2​=w1​−s⋅2i=1∑n​(w1T​xi​−yi​)xi​=i=1∑n​αi1​xi​−si=1∑n​γi1​xi​=i=1∑n​αi2​xi​ α2=α1−sγ1w3​=w2​−s⋅2i=1∑n​(w2T​xi​−yi​)xi​=i=1∑n​αi2​xi​−si=1∑n​γi2​xi​=i=1∑n​αi2​xi​ α3=α2−sγ2...wt​=wt−1​−s⋅2i=1∑n​(wt−1T​xi​−yi​)xi​=i=1∑n​αit−1​xi​−si=1∑n​γit−1​xi​=i=1∑n​αit​xi​ αt=αt−1−sγt−1​ 因为 αi0=0 ~\\alpha^0_i=0~ αi0​=0 ，则有： αi1= 0 −sγi0=−sγi0αi2=αi1−sγ01=−sγi0−sγi1αi3=αi2−sγi2=−sγi0−sγi1−sγi2...αit=αit−1−sγit−1=−s∑r=1t−1γir\\begin{aligned} &amp;\\alpha^1_i=~0~-s\\gamma^0_i=-s\\gamma^0_i\\\\ &amp;\\alpha^2_i=\\alpha^1_i-s\\gamma^1_0=-s\\gamma^0_i-s\\gamma^1_i\\\\ &amp;\\alpha^3_i=\\alpha_i^2-s\\gamma^2_i=-s\\gamma^0_i-s\\gamma^1_i-s\\gamma^2_i\\\\ &amp;...\\\\ &amp;\\alpha^t_i=\\alpha_i^{t-1}-s\\gamma^{t-1}_i=-s\\sum_{r=1}^{t-1}\\gamma_i^r \\end{aligned} ​αi1​= 0 −sγi0​=−sγi0​αi2​=αi1​−sγ01​=−sγi0​−sγi1​αi3​=αi2​−sγi2​=−sγi0​−sγi1​−sγi2​...αit​=αit−1​−sγit−1​=−sr=1∑t−1​γir​​ 我们用 xi ~x_i~ xi​ 的线性组合代替 w ~w~ w ，得到新的模型和损失函数： h(xi)=wtTxi=∑j=1nαjtxjTxil(w)=∑i=1n(wtTxi−yi)2=∑i=1n(∑j=1nαjtxjTxi−yi)2h(x_i)=w_t^Tx_i=\\sum_{j=1}^n\\alpha_j^tx_j^Tx_i\\\\ l(w)=\\sum_{i=1}^n(w^T_tx_i-y_i)^2=\\sum_{i=1}^n(\\sum_{j=1}^n\\alpha_j^tx_j^Tx_i-y_i)^2 h(xi​)=wtT​xi​=j=1∑n​αjt​xjT​xi​l(w)=i=1∑n​(wtT​xi​−yi​)2=i=1∑n​(j=1∑n​αjt​xjT​xi​−yi​)2 由此我们可以发现：为了学习具有平方损失的超平面分类器，我们需要的唯一信息是所有数据的特征向量对之间的内积。 2.2 计算内积 有了上述推导，我们将模型简化为只需要求解向量对之间的内积，我们回归到上述的升维操作中去： 在升维之后，内积的计算公式为： ϕ(x)Tϕ(z)=1+x1z1+x2z2+...x1x2...xdz1z2...zd=∏k=1d(1+xkzk)\\phi(x)^T\\phi(z)=1+x_1z_1+x_2z_2+...x_1x_2...x_dz_1z_2...z_d=\\prod_{k=1}^d(1+x_kz_k) ϕ(x)Tϕ(z)=1+x1​z1​+x2​z2​+...x1​x2​...xd​z1​z2​...zd​=k=1∏d​(1+xk​zk​) 我们可以发现，尽管特征向量是 2d ~2^d~ 2d 维的，但是计算其内积仅需要 d ~d~ d 次乘法运算，这极大提高了算法的速度。 我们由此即可定义核函数： k(xi,xj)=ϕ(xi)Tϕ(xj)k(x_i,x_j)=\\phi(x_i)^T\\phi(x_j) k(xi​,xj​)=ϕ(xi​)Tϕ(xj​) 核函数计算出的结果存储在核矩阵中： Kij=ϕ(xi)Tϕ(xj)K_{ij}=\\phi(x_i)^T\\phi(x_j) Kij​=ϕ(xi​)Tϕ(xj​) 诸如 ϕ ~\\phi~ ϕ 之类的用于升维的映射并不好找，因此我们用核函数 k(xi,xj) ~k(x_i,x_j)~ k(xi​,xj​) 去代替这样的映射，处理线性不可分问题。 则上述模型可以表示为：可以发现模型中唯一的未知参数即为 α ~\\alpha~ α ，我们需要对它进行求解 h(xi)=wTxi=∑j=1nαjxjTxi=∑j=1nαjk(xj,xi)h(x_i)=w^Tx_i=\\sum_{j=1}^n\\alpha_jx_j^Tx_i=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) h(xi​)=wTxi​=j=1∑n​αj​xjT​xi​=j=1∑n​αj​k(xj​,xi​) 同时我们也已经得到了： αit=−s∑r=1t−1γir\\alpha^t_i=-s\\sum_{r=1}^{t-1}\\gamma_i^r αit​=−sr=1∑t−1​γir​ 所以当下我们的求解目标变为了 γ ~\\gamma~ γ ： ∂l(w)∂w=2∑i=1n(wTxi−yi)⋅xi=∑i=1nγixiγi=2(wTxi−yi)\\frac{\\partial l(w)}{\\partial w}=2\\sum_{i=1}^n(w^Tx_i-y_i)\\cdot x_i=\\sum_{i=1}^n\\gamma_ix_i\\\\ \\gamma_i=2(w^Tx_i-y_i) ∂w∂l(w)​=2i=1∑n​(wTxi​−yi​)⋅xi​=i=1∑n​γi​xi​γi​=2(wTxi​−yi​) 在经过 ϕ ~\\phi~ ϕ 特征变换的新的高维空间中有： γi=2(wTϕ(xi)−yi)=2(∑j=1nαjk(xj,xi)−yi)\\gamma_i=2(w^T\\phi(x_i)-y_i)=2(\\sum_{j=1}^n\\alpha_jk(x_j,x_i)-y_i) γi​=2(wTϕ(xi​)−yi​)=2(j=1∑n​αj​k(xj​,xi​)−yi​) 则梯度下降的过程为： αit+1=αit−sγit=αit−2s(∑j=1nαjtk(xj,xi)−yi)\\alpha_i^{t+1}=\\alpha_i^t-s\\gamma_i^t=\\alpha_i^t-2s(\\sum_{j=1}^n\\alpha_j^tk(x_j,x_i)-y_i) αit+1​=αit​−sγit​=αit​−2s(j=1∑n​αjt​k(xj​,xi​)−yi​) 梯度下降过程中，每次更新 α ~\\alpha~ α 的计算量为 O(n2) ~O(n^2)~ O(n2) ，远好于 O(2d) ~O(2^d)~ O(2d) 三、一般核函数 3.1 常用核函数 （1）线性核函数： K(x,z)=xTzK(x,z)=x^Tz K(x,z)=xTz （2）多项式核函数： K(x,z)=(1+xTz)dK(x,z)=(1+x^Tz)^d K(x,z)=(1+xTz)d （3）高斯核函数（RBF\\text{RBF}RBF）： K(x,z)=e−∣∣x−z∣∣22σ2K(x,z)=e^{-\\frac{||x-z||_2^2}{\\sigma^2}} K(x,z)=e−σ2∣∣x−z∣∣22​​ （4）指数核函数： K(x,z)=e−∣∣x−z∣∣2σ2K(x,z)=e^{-\\frac{||x-z||}{2\\sigma^2}} K(x,z)=e−2σ2∣∣x−z∣∣​ （5）拉普拉斯核函数： K(x,z)=e−∣x−z∣σK(x,z)=e^{-\\frac{|x-z|}{\\sigma}} K(x,z)=e−σ∣x−z∣​ （6）Sigmoid\\text{Sigmoid}Sigmoid核函数： tanh⁡(x)=ex−e−xex+e−x ~\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}~ tanh(x)=ex+e−xex−e−x​ K(x,z)=tanh⁡(γxTz+r)K(x,z)=\\tanh(\\gamma x^Tz+r) K(x,z)=tanh(γxTz+r) 3.2 良定义核函数 良定义的核函数定义如下：通过递归组合以下一个或多个规则构建的核称为定义良好的核： ① k(x,z)=xTz② k(x,z)=ck1(x,z)③ k(x,z)=k1(x,z)+k2(x,z)④ k(x,z)=g(k(x,z))⑤ k(x,z)=k1(x,z)⋅k2(x,z)⑥ k(x,z)=f(x)k1(x,z)f(z)⑦ k(x,z)=ek1(x,z)⑧ k(x,z)=xTAz\\begin{aligned} &amp;①~k(x,z)=x^Tz\\\\ &amp;②~k(x,z)=ck_1(x,z)\\\\ &amp;③~k(x,z)=k_1(x,z)+k_2(x,z)\\\\ &amp;④~k(x,z)=g\\big(k(x,z)\\big)\\\\ &amp;⑤~k(x,z)=k_1(x,z)\\cdot k_2(x,z)\\\\ &amp;⑥~k(x,z)=f(x)k_1(x,z)f(z)\\\\ &amp;⑦~k(x,z)=e^{k_1(x,z)}\\\\ &amp;⑧~k(x,z)=x^TAz \\end{aligned} ​① k(x,z)=xTz② k(x,z)=ck1​(x,z)③ k(x,z)=k1​(x,z)+k2​(x,z)④ k(x,z)=g(k(x,z))⑤ k(x,z)=k1​(x,z)⋅k2​(x,z)⑥ k(x,z)=f(x)k1​(x,z)f(z)⑦ k(x,z)=ek1​(x,z)⑧ k(x,z)=xTAz​ 上述规则中 k1(x,z) ~k_1(x,z)~ k1​(x,z) 和 k2(x,z) ~k_2(x,z)~ k2​(x,z) 都是良定义的核函数， c≥0 ~c\\ge0~ c≥0 ， g ~g~ g 是一个正系数多项式函数， f ~f~ f 是任何函数， A ~A~ A 是半正定的 某个核函数是良定义的等价为： ①核矩阵 K ~K~ K 的特征值都是非负的 ②存在实矩阵 P ~P~ P 使得： K=PTP ~K=P^TP~ K=PTP ③核矩阵 K ~K~ K 是半正定的，即对于任何向量 x ~x~ x ，都有： xTKx≥0 ~x^TKx\\ge0~ xTKx≥0 定理 3-1 RBF核函数：k(x,z)=e−(x−z)2σ2是良定义的\\text{RBF}核函数：k(x,z)=e^{-\\frac{(x-z)^2}{\\sigma^2}}是良定义的 RBF核函数：k(x,z)=e−σ2(x−z)2​是良定义的 证明如下： k(x,z)=e−(x−z)2σ2=e−1σ2(xTx−2xTz+zTz)=e−xTxσ2⋅e2xTzσ2⋅e−xTxσ2\\begin{aligned} &amp;k(x,z)=e^{-\\frac{(x-z)^2}{\\sigma^2}}=e^{-\\frac1{\\sigma^2}(x^Tx-2x^Tz+z^Tz)}=e^{-\\frac{x^Tx}{\\sigma^2}}\\cdot e^{\\frac{2x^Tz}{\\sigma^2}}\\cdot e^{-\\frac{x^Tx}{\\sigma^2}} \\end{aligned} ​k(x,z)=e−σ2(x−z)2​=e−σ21​(xTx−2xTz+zTz)=e−σ2xTx​⋅eσ22xTz​⋅e−σ2xTx​​ 根据规则⑥：f(x)=e−xTxσ2,k1(x,z)=e2xTzσ2→k(x,z)=f(x)⋅k1(x,z)⋅f(z)根据规则⑦：k2(x,z)=2xTzσ2→k1(x,z)=ek2(x,z)根据规则②：k3(x,z)=xTz,c=2xTzσ2→k2(x,z)=c⋅k3(x,z)根据规则①：k3(x,z) is well defined→k2(x,z) is well defined→k1(x,z) is well defined\\begin{aligned} &amp;根据规则⑥：f(x)=e^{-\\frac{x^Tx}{\\sigma^2}},k_1(x,z)=e^{\\frac{2x^Tz}{\\sigma^2}}\\rightarrow k(x,z)=f(x)\\cdot k_1(x,z)\\cdot f(z)\\\\ &amp;根据规则⑦：k_2(x,z)=\\frac{2x^Tz}{\\sigma^2}\\rightarrow k_1(x,z)=e^{k_2(x,z)}\\\\ &amp;根据规则②：k_3(x,z)=x^Tz,c=\\frac{2x^Tz}{\\sigma^2}\\rightarrow k_2(x,z)=c\\cdot k_3(x,z)\\\\ &amp;根据规则①：k_3(x,z)\\text{ is well defined}\\rightarrow k_2(x,z)\\text{ is well defined}\\rightarrow k_1(x,z)\\text{ is well defined}\\\\ \\end{aligned} ​根据规则⑥：f(x)=e−σ2xTx​,k1​(x,z)=eσ22xTz​→k(x,z)=f(x)⋅k1​(x,z)⋅f(z)根据规则⑦：k2​(x,z)=σ22xTz​→k1​(x,z)=ek2​(x,z)根据规则②：k3​(x,z)=xTz,c=σ22xTz​→k2​(x,z)=c⋅k3​(x,z)根据规则①：k3​(x,z) is well defined→k2​(x,z) is well defined→k1​(x,z) is well defined​ 综上推导： k(x,z) is well definedk(x,z)\\text{ is well defined} k(x,z) is well defined 定理 3-2 S1,S2∈Ω,k(S1,S2)=e∣S1∩S2∣是良定义的S_1,S_2\\in \\Omega,k(S_1,S_2)=e^{|S_1\\cap S_2|}是良定义的 S1​,S2​∈Ω,k(S1​,S2​)=e∣S1​∩S2​∣是良定义的 证明如下： 将 Ω ~\\Omega~ Ω 中所有可能的元素排列成一个列表，S1,S2S_1,S_2S1​,S2​分别用一个大小为 ∣Ω∣ ~|\\Omega|~ ∣Ω∣ 的向量 xS1,xS2 ~x^{S_1},x^{S_2}~ xS1​,xS2​ 表示，如果 Ω ~\\Omega~ Ω 中的第 i ~i~ i 个元素属于 S ~S~ S ，则 xiS=1 ~x^{S}_i=1~ xiS​=1 ，反之则 xiS=0 ~x^{S}_i=0~ xiS​=0 ，则上述核函数可以表示为： k(S1,S2)=exS1TxS2\\begin{aligned} &amp;k(S_1,S_2)=e^{x_{S_1}^Tx_{S_2}} \\end{aligned} ​k(S1​,S2​)=exS1​T​xS2​​​ 根据规则⑦和规则①，我们可以得到： k(S1,S2) ~k(S_1,S_2)~ k(S1​,S2​) 为良定义核函数。 四、模型核化 一个算法可以通过三步实现核化： ①证明解决方案位于训练点的范围内，即对于某些 αi ~\\alpha_i~ αi​ ： w=∑i=1nαixiw=\\sum_{i=1}^n\\alpha_ix_i w=i=1∑n​αi​xi​ ②重构算法与分类器，使得输入的特征向量仅用于内积的计算 ③将内积替换为核函数： xiTxj→ϕ(xi)Tϕ(xj)x_i^Tx_j\\rightarrow\\phi(x_i)^T\\phi(x_j) xiT​xj​→ϕ(xi​)Tϕ(xj​) 4.1 线性回归核化 我们回顾一下普通的最小二乘回归 OLS ~OLS~ OLS ： l(w)=∑i=1n(xiTw−yi)w^MLE=argminw ∑i=1n(xiTw−yi)h(x)=wTxl(w)=\\sum_{i=1}^n(x_i^Tw-y_i)\\\\ \\hat{w}_{MLE}=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)\\\\ h(x)=w^Tx l(w)=i=1∑n​(xiT​w−yi​)w^MLE​=wargmin​ i=1∑n​(xiT​w−yi​)h(x)=wTx 输入的训练集为 X ~X~ X 和 Y ~Y~ Y ，则其闭合形式为： w=(XTX)−1XTYw=(X^TX)^{-1}X^TY w=(XTX)−1XTY 我们对该模型进行核化： X ~X~ X 为 n×d ~n\\times d~ n×d 维矩阵， Y ~Y~ Y 为 n×1 ~n\\times1~ n×1 维矩阵， w,xi ~w,x_i~ w,xi​ 为 d×1 ~d\\times1~ d×1 维矩阵， α ~\\alpha~ α 为 n×1 ~n\\times1~ n×1 维矩阵 ①将 w ~w~ w 表示为 xi ~x_i~ xi​ 的线性组合： α=[ α1,α2,...,αn ]T ~\\alpha=[~\\alpha_1,\\alpha_2,...,\\alpha_n~]^T~ α=[ α1​,α2​,...,αn​ ]T w=∑i=1nαixi=XTα\\begin{aligned} &amp;w=\\sum_{i=1}^n\\alpha_ix_i=X^T\\alpha \\end{aligned} ​w=i=1∑n​αi​xi​=XTα​ ②将模型修正为输入特征向量的内积： h(xi)=∑j=1nαjxjTxi=wTxi=αTXxih(x_i)=\\sum_{j=1}^n\\alpha_jx_j^Tx_i=w^Tx_i=\\alpha^TXx_i h(xi​)=j=1∑n​αj​xjT​xi​=wTxi​=αTXxi​ ③用核函数代替内积： h(xi)=∑j=1nαjk(xj,xi)h(x_i)=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) h(xi​)=j=1∑n​αj​k(xj​,xi​) 求解 α ~\\alpha~ α 的闭合形式： w=XTα=(XTX)−1XTY→XTXXTα=XTY→XXTα=YKij=k(xi,xj)→K=XXT→Kα=Yα=K−1Y\\begin{aligned} &amp;w=X^T\\alpha=(X^TX)^{-1}X^TY\\rightarrow X^TXX^T\\alpha=X^TY\\rightarrow XX^T\\alpha=Y\\\\ &amp;K_{ij}=k(x_i,x_j)\\rightarrow K=XX^T\\rightarrow K\\alpha=Y\\\\ &amp;\\alpha=K^{-1}Y \\end{aligned} ​w=XTα=(XTX)−1XTY→XTXXTα=XTY→XXTα=YKij​=k(xi​,xj​)→K=XXT→Kα=Yα=K−1Y​ 岭回归同理，我们也可以实现同样的核化并求解 α ~\\alpha~ α 的闭合形式： 模型为： l(w)=∑i=1n(xiTw−yi)+λ∣∣w∣∣22w^MAP=argminw ∑i=1n(xiTw−yi)+λ∣∣w∣∣22h(x)=∑j=1nαjk(xj,xi)l(w)=\\sum_{i=1}^n(x_i^Tw-y_i)+\\lambda||w||_2^2\\\\ \\hat{w}_{MAP}=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)+\\lambda||w||_2^2\\\\ h(x)=\\sum_{j=1}^n\\alpha_jk(x_j,x_i) l(w)=i=1∑n​(xiT​w−yi​)+λ∣∣w∣∣22​w^MAP​=wargmin​ i=1∑n​(xiT​w−yi​)+λ∣∣w∣∣22​h(x)=j=1∑n​αj​k(xj​,xi​) α ~\\alpha~ α 的闭合形式为： w=XTα=(XTX+λI)−1XTY→(XTX+λI)XTα=XTY→(XTXXT+λXT)α=XTY→(XXT+λI)α=Y→α=(K+λI)−1Y\\begin{aligned} &amp;w=X^T\\alpha=(X^TX+\\lambda I)^{-1}X^TY\\rightarrow (X^TX+\\lambda I)X^T\\alpha=X^TY\\\\ &amp;\\rightarrow (X^TXX^T+\\lambda X^T)\\alpha=X^TY\\rightarrow (XX^T+\\lambda I)\\alpha=Y\\\\ &amp;\\rightarrow \\alpha=(K+\\lambda I)^{-1}Y \\end{aligned} ​w=XTα=(XTX+λI)−1XTY→(XTX+λI)XTα=XTY→(XTXXT+λXT)α=XTY→(XXT+λI)α=Y→α=(K+λI)−1Y​ 4.2 KNN核化 我们以欧拉距离的 KNN ~KNN~ KNN 模型为例： dist(xi,xj)=(xi−xj)T(xi−xj)=xiTxi−2xiTxj+xjTxj=k(xi,xi)−2k(xi,xj)+k(xj,xj)\\text{dist}(x_i,x_j)=(x_i-x_j)^T(x_i-x_j)=x_i^Tx_i-2x_i^Tx_j+x_j^Tx_j=k(x_i,x_i)-2k(x_i,x_j)+k(x_j,x_j) dist(xi​,xj​)=(xi​−xj​)T(xi​−xj​)=xiT​xi​−2xiT​xj​+xjT​xj​=k(xi​,xi​)−2k(xi​,xj​)+k(xj​,xj​) 但是上述核化的意义并不大，因此我们一般不对 KNN ~KNN~ KNN 算法进行核化。 4.3 支持向量机核化 线性支持向量机结合核函数是一个很强大的模型，我们往往求解其对偶问题，所以我们需要先了解对偶问题的定义。 4.3.1 拉格朗日乘子法与KKT条件 在求解最优化问题中，拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件是两种最常用的方法。在有等式约束时使用拉格朗日乘子法，在有不等约束时使用KKT条件。 我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值，支持向量机模型即为该类最优化为问题。 在求解最优化问题时一般会遇到三种情况： （1）无约束条件： 这是最简单的情况，解决方法通常是函数对变量求导，令求导函数等于0的点可能是极值点。将结果带回原函数进行验证即可。 （2）等式约束条件： 设目标函数为 f(x) ~f(x)~ f(x) ，约束条件为 hk(x) ~h_k(x)~ hk​(x) ，有 l ~l~ l 个约束条件，则等式约束条件的最优化问题可以表示为： 求解目标：min⁡f(x)约束条件：hk(x)=0,k=1,2,...l\\begin{aligned} &amp;求解目标：\\min f(x)\\\\ &amp;约束条件：h_k(x)=0,k=1,2,...l \\end{aligned} ​求解目标：minf(x)约束条件：hk​(x)=0,k=1,2,...l​ 我们要用到拉格朗日乘子法处理该最优化问题：首先定义拉格朗日函数： F(x.λ)=f(x)+∑k=1lλkhk(x)F(x.\\lambda)=f(x)+\\sum_{k=1}^l\\lambda_kh_k(x) F(x.λ)=f(x)+k=1∑l​λk​hk​(x) 然后解变量的偏导方程： ∂F(x,λ)∂x1=0,∂F(x,λ)∂x2=0,...,∂F(x,λ)∂xd=0∂F(x,λ)∂λ1=0,∂F(x,λ)∂λ2=0,...,∂F(x,λ)∂λk=0\\frac{\\partial F(x,\\lambda)}{\\partial x_1}=0,\\frac{\\partial F(x,\\lambda)}{\\partial x_2}=0,...,\\frac{\\partial F(x,\\lambda)}{\\partial x_d}=0\\\\ \\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_1}=0,\\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_2}=0,...,\\frac{\\partial F(x,\\lambda)}{\\partial \\lambda_k}=0 ∂x1​∂F(x,λ)​=0,∂x2​∂F(x,λ)​=0,...,∂xd​∂F(x,λ)​=0∂λ1​∂F(x,λ)​=0,∂λ2​∂F(x,λ)​=0,...,∂λk​∂F(x,λ)​=0 （3）不等式约束条件： 设目标函数为 f(x) ~f(x)~ f(x) ，不等式约束条件为 gk(x) ~g_k(x)~ gk​(x) ，有 q ~q~ q 个不等式约束条件，等式约束条件为 hj(k) ~h_j(k)~ hj​(k) ，有 p ~p~ p 个等式约束条件： 求解目标：min⁡f(x)约束条件：hj(x)=0,j=1,2,...,p gk(x)≤0,k=1,2,...,q\\begin{aligned} &amp;求解目标：\\min f(x)\\\\ &amp;约束条件：h_j(x)=0,j=1,2,...,p\\\\ &amp;~~~~~~~~~~~~~~~~~g_k(x)\\le0,k=1,2,...,q \\end{aligned} ​求解目标：minf(x)约束条件：hj​(x)=0,j=1,2,...,p gk​(x)≤0,k=1,2,...,q​ 则我们可以定义不等式约束条件下的拉格朗日函数为： L(x,λ,μ)=f(x)+∑j=1pλjhj(x)+∑k=1qμkgk(x)L(x,\\lambda,\\mu)=f(x)+\\sum_{j=1}^p\\lambda_jh_j(x)+\\sum_{k=1}^q\\mu_kg_k(x) L(x,λ,μ)=f(x)+j=1∑p​λj​hj​(x)+k=1∑q​μk​gk​(x) 常用的方法是 KKT ~KKT~ KKT 条件：即最优值（局部最小值）必须满足以下条件： ① ∂L(x,λ,μ)∂x∣x=x∗=0② hj(x∗)=0③ μkgk(x∗)=0\\begin{aligned} &amp;①~\\frac{\\partial L(x,\\lambda,\\mu)}{\\partial x}|_{x=x^*}=0\\\\ &amp;②~h_j(x^*)=0\\\\ &amp;③~\\mu_kg_k(x^*)=0 \\end{aligned} ​① ∂x∂L(x,λ,μ)​∣x=x∗​=0② hj​(x∗)=0③ μk​gk​(x∗)=0​ 4.3.2 对偶问题的核化 我们首先回顾支持向量机的模型：为了便于计算我们引入一个常系数 12 ~\\frac12~ 21​ 求解目标：(w,b)=argminw,b 12∣∣w∣∣22约束条件：∀i , yi(wTxi+b)≥1\\begin{aligned} &amp;求解目标：(w,b)=\\underset{w,b}{argmin}~\\frac12||w||^2_2\\\\ &amp;约束条件：\\forall i~,~y_i(w^Tx_i+b)\\ge 1 \\end{aligned} ​求解目标：(w,b)=w,bargmin​ 21​∣∣w∣∣22​约束条件：∀i , yi​(wTxi​+b)≥1​ 则拉格朗日函数为： L(w,b,α)=12∣∣w∣∣22+∑i=1nαi(1−yi(wTxi+b))L(w,b,\\alpha)=\\frac12||w||_2^2+\\sum_{i=1}^n\\alpha_i\\big(1-y_i(w^Tx_i+b)\\big) L(w,b,α)=21​∣∣w∣∣22​+i=1∑n​αi​(1−yi​(wTxi​+b)) 根据 KKT ~KKT~ KKT 条件可得： ∂L(w,b,α)∂w=w−∑i=1nαiyixi=0∂L(w,b,α)∂b=−∑i=1nαiyi=0\\begin{aligned} &amp;\\frac{\\partial L(w,b,\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\\\ &amp;\\frac{\\partial L(w,b,\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_i y_i=0 \\end{aligned} ​∂w∂L(w,b,α)​=w−i=1∑n​αi​yi​xi​=0∂b∂L(w,b,α)​=−i=1∑n​αi​yi​=0​ 我们同时也知道，在 w,b ~w,b~ w,b 取得最优值时有： yi(wTxi+b)=1 ~y_i(w^Tx_i+b)=1~ yi​(wTxi​+b)=1 满足了 KKT ~KKT~ KKT 条件 综上，我们可得： w=∑i=1nαiyixi∑i=1nαiyi=0\\begin{aligned} &amp;w=\\sum_{i=1}^n\\alpha_iy_ix_i\\\\ &amp;\\sum_{i=1}^n\\alpha_iy_i=0 \\end{aligned} ​w=i=1∑n​αi​yi​xi​i=1∑n​αi​yi​=0​ 我们上述条件代入原式可得： (w,b)=argminw,b 12∑i=1n∑j=1nαiαjyiyjxiTxj+∑i=1nαi−∑i=1n∑j=1nαiyiyjxiTxj−b⋅∑i=1nαiyi→(w,b)=argminw,b ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj\\begin{aligned} &amp;(w,b)=\\underset{w,b}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j+\\sum_{i=1}^n\\alpha_i-\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_iy_iy_jx_i^Tx_j-b\\cdot\\sum_{i=1}^n\\alpha_iy_i\\\\ &amp;\\rightarrow(w,b)=\\underset{w,b}{argmin}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\end{aligned} ​(w,b)=w,bargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​+i=1∑n​αi​−i=1∑n​j=1∑n​αi​yi​yj​xiT​xj​−b⋅i=1∑n​αi​yi​→(w,b)=w,bargmin​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​​ 由此我们得到了支持向量机模型的对偶问题： 求解目标：α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj约束条件：∑i=1nαiyi=0\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0 \\end{aligned} ​求解目标：α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​约束条件：i=1∑n​αi​yi​=0​ 显然我们可以对该对偶问题进行核化： α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjk(xi,xj)\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j) α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​) 最终的模型为： h(x)=sign(wTx+b)=sign(∑i=1nαiyik(xi,x)+b)h(x)=\\text{sign}(w^Tx+b)=\\text{sign}\\big(\\sum_{i=1}^n\\alpha_iy_ik(x_i,x)+b\\big) h(x)=sign(wTx+b)=sign(i=1∑n​αi​yi​k(xi​,x)+b) 从支持向量的角度对对偶问题有一个很好的解释：对于原始公式，我们知道只有支持向量满足等式约束： yi(wTϕ(xi)+b)=1y_i\\big(w^T\\phi(x_i)+b\\big)=1 yi​(wTϕ(xi​)+b)=1 在对偶问题中我们可以使得支持向量所对应的 αi&gt;0 ~\\alpha_i&gt;0~ αi​&gt;0 ，而其他的输入向量对应的 αi=0 ~\\alpha_i=0~ αi​=0 ，在测试时我们只需要计算支持向量上 h(x) ~h(x)~ h(x) 的和，并在训练后丢弃所有 αi=0 ~\\alpha_i=0~ αi​=0 的特征向量。 对偶有一个明显的问题，就是 b ~b~ b 不再是优化的一部分了，但是我们需要它来进行分类，在对偶中支持向量是那些 αi&gt;0 ~α_i&gt;0~ αi​&gt;0 的向量，因此我们可以推导出 b ~b~ b ： yi(wTϕ(xi)+b)=1,yi∈{−1,+1}→b=yi−wTϕ(xi)→b=yi−∑j=1nαjyjk(xj,xi)\\begin{aligned} &amp;y_i(w^T\\phi(x_i)+b)=1,y_i\\in\\{-1,+1\\}\\rightarrow b=y_i-w^T\\phi(x_i)\\\\ &amp;\\rightarrow b=y_i-\\sum_{j=1}^n\\alpha_jy_jk(x_j,x_i) \\end{aligned} ​yi​(wTϕ(xi​)+b)=1,yi​∈{−1,+1}→b=yi​−wTϕ(xi​)→b=yi​−j=1∑n​αj​yj​k(xj​,xi​)​ 同时如果使用软间隔模型，则仅需添加一个新的约束： 0≤αi≤C0\\le\\alpha_i\\le C 0≤αi​≤C 五、模型实现 我们本次要手动实现核化的软间隔支持向量机，主要运用其对偶问题求解最优化问题。 5.1 数据集 我们本次要生成线性不可分的数据集，因为这样才能体现出核函数的优势所在，生成线性不可分数据集的代码如下所示：我们主要用到的是sklearn所提供的make_moons生成双半月环数据集，这是一个经典的线性不可分数据集。 生成的数据集如下图所示： 5.2 手动实现模型 5.2.1 模型阐述 求解思路来自：支持向量机（SVM）——对偶问题 我们首先要了解对于对偶形式的支持向量机模型的求解方法，考虑到软约束，我们的模型为： 求解目标：α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 我们最后求解出的模型为： h(xi)=∑j=1nαjyjxjTxi+bh(x_i)=\\sum_{j=1}^n\\alpha_jy_jx_j^Tx_i+b h(xi​)=j=1∑n​αj​yj​xjT​xi​+b 我们首先实现该模型的代码： 基于原始模型以及我们作出的假设， KKT ~KKT~ KKT 条件为： {0≤αi≤Cyi⋅h(xi)−1≥0αi(yi⋅h(xi)−1)=0\\begin{aligned} \\left\\{ \\begin{aligned} &amp;0\\le\\alpha_i\\le C\\\\ &amp;y_i\\cdot h(x_i)-1\\ge 0\\\\ &amp;\\alpha_i(y_i\\cdot h(x_i)-1)=0 \\end{aligned} \\right. \\end{aligned} ⎩⎪⎨⎪⎧​​0≤αi​≤Cyi​⋅h(xi​)−1≥0αi​(yi​⋅h(xi​)−1)=0​​ 因为我们有：支持向量所对应的 αi&gt;0 ~\\alpha_i&gt;0~ αi​&gt;0 ，而其他的输入向量对应的 αi=0 ~\\alpha_i=0~ αi​=0 的设定，支持向量是那些满足yih(xi)=1y_ih(x_i)=1yi​h(xi​)=1的点，所以有： αi(yi⋅h(xi)−1)=0\\alpha_i(y_i\\cdot h(x_i)-1)=0 αi​(yi​⋅h(xi​)−1)=0 接着我们需要求解核心问题： α=argmaxα ∑i=1nαi−12∑i=1n∑j=1nαiαjyiyjxiTxj\\alpha=\\underset{\\alpha}{argmax}~\\sum_{i=1}^n\\alpha_i-\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\ α=αargmax​ i=1∑n​αi​−21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​ 5.2.2 求解思路 这是一个二次规划问题，我们用到 SMO ~SMO~ SMO 算法对其进行求解，算法思路来自：SMO算法详解 我们不妨先将问题作一个变形： α=argminα 12∑i=1n∑j=1nαiαjyiyjxiTxj−∑i=1nαi\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i\\\\ α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​−i=1∑n​αi​ 我们要寻找一个满足约束条件的 α=[ α1,α2,...,αn ] ~\\alpha=[~\\alpha_1,\\alpha_2,...,\\alpha_n~]~ α=[ α1​,α2​,...,αn​ ] ，假设我们已经找到了这样的一个 α ~\\alpha~ α ，即最终的超平面已经确定： h(xi)=wTxi+bh(x_i)=w^Tx_i+b h(xi​)=wTxi​+b 那么该超平面是满足 KKT ~KKT~ KKT 条件的，则有： {yi⋅h(xi)≥1→ αi=0 ,xi在边界内，正确分类yi⋅h(xi)=1→0&lt;αi&lt;C,xi在边界上，是支持向量，正确分类yi⋅h(xi)≤1→ αi=C ,xi在两条边界之间\\begin{aligned} &amp;\\left\\{ \\begin{aligned} &amp;y_i\\cdot h(x_i)\\ge1\\rightarrow ~~~~\\alpha_i=0~~~~,x_i在边界内，正确分类\\\\ &amp;y_i\\cdot h(x_i)=1\\rightarrow 0&lt;\\alpha_i&lt; C,x_i在边界上，是支持向量，正确分类\\\\ &amp;y_i\\cdot h(x_i)\\le1\\rightarrow ~~~~\\alpha_i=C~~~~,x_i在两条边界之间 \\end{aligned} \\right. \\end{aligned} ​⎩⎪⎨⎪⎧​​yi​⋅h(xi​)≥1→ αi​=0 ,xi​在边界内，正确分类yi​⋅h(xi​)=1→0&lt;αi​&lt;C,xi​在边界上，是支持向量，正确分类yi​⋅h(xi​)≤1→ αi​=C ,xi​在两条边界之间​​ 反过来想，我们求解出来的 αi ~\\alpha_i~ αi​ 和 xi ~x_i~ xi​ 也要满足上述关系。 综上我们的求解过程便是初始化一个 α ~\\alpha~ α 并不断得对其进行优化，直到找到最优的那个 α ~\\alpha~ α ， SMO ~SMO~ SMO 算法每次选择两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 进行更新。 根据我们的约束，显然这两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 满足： αiyi+αjyj=−∑k=1,k=i,jnαkyk=ξ\\alpha_iy_i+\\alpha_jy_j=-\\sum_{k=1,k\\not=i,j}^n\\alpha_ky_k=\\xi αi​yi​+αj​yj​=−k=1,k​=i,j∑n​αk​yk​=ξ 因此有： αi=ξ−αjyjyi=(ξ−αjyj)yi , yi∈{−1,+1}\\alpha_i=\\frac{\\xi-\\alpha_jy_j}{y_i}=(\\xi-\\alpha_jy_j)y_i~,~y_i\\in\\{-1,+1\\} αi​=yi​ξ−αj​yj​​=(ξ−αj​yj​)yi​ , yi​∈{−1,+1} 因此我们只需要找到一个 αj ~\\alpha_j~ αj​ 进行优化，并且求出优化后的值，那么我们的 αi ~\\alpha_i~ αi​ 也完成了优化，为了便于表示，后面我们将挑选出的两个 αi,αj ~\\alpha_i,\\alpha_j~ αi​,αj​ 记作 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 从效果上考虑我们应该优化那些最不满足目标条件的 αi ~\\alpha_i~ αi​ ，而在我们优化过后它不满足目标条件的程度应该减小，而我们为了衡量一个 αi ~\\alpha_i~ αi​ 满足目标条件的程度，引入一个指标：我们首先定义一个误差： Ei=h(xi)−yiE_i=h(x_i)-y_i Ei​=h(xi​)−yi​ 我们发现 ∣E1−E2∣ ~|E_1-E_2|~ ∣E1​−E2​∣ 越大，优化后的 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 满足目标条件的程度就越大。 计算 Ei ~E_i~ Ei​ 的代码如下所示： 接着我们思考一个问题：我们该怎样确定优化后的 α ~\\alpha~ α 是朝着不满足目标条件程度减小的方向移动的呢？ 我们回归到原始的问题： 求解目标：α=argminα 12∑i=1n∑j=1nαiαjyiyjxiTxj−∑i=1nαi约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​xiT​xj​−i=1∑n​αi​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 我们求解目标是求解目标函数的局部最小值，那么我们不妨将 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 看作变量，其他 αi ~\\alpha_i~ αi​ 看作常量，则： α=argminα W(α1,α2)W(α1,α2)=Aα12+Bα22+Cα1α2+Dα1+Eα2+F\\alpha=\\underset{\\alpha}{argmin}~W(\\alpha_1,\\alpha_2)\\\\ W(\\alpha_1,\\alpha_2)=A\\alpha_1^2+B\\alpha_2^2+C\\alpha_1\\alpha_2+D\\alpha_1+E\\alpha_2+F α=αargmin​ W(α1​,α2​)W(α1​,α2​)=Aα12​+Bα22​+Cα1​α2​+Dα1​+Eα2​+F 代入 α1=(ξ−α2y2)y1 ~\\alpha_1=(\\xi-\\alpha_2y_2)y_1~ α1​=(ξ−α2​y2​)y1​ 可得： W(α2)=A[(ξ−α2y2)y1]2+Bα22+C(ξ−α2y2)y1α2+D(ξ−α2y2)y1+Eα2+F =(A+B−Cy1y2)α22+(E−2Ay2−Dy1y2)α2+Aξ2+(C+D)ξ+F\\begin{aligned} &amp;W(\\alpha_2)=A\\big[(\\xi-\\alpha_2y_2)y_1\\big]^2+B\\alpha_2^2+C(\\xi-\\alpha_2y_2)y_1\\alpha_2+D(\\xi-\\alpha_2y_2)y_1+E\\alpha_2+F\\\\ &amp;~~~~~~~~~~~~~=(A+B-Cy_1y_2)\\alpha_2^2+(E-2Ay_2-Dy_1y_2)\\alpha_2+A\\xi^2+(C+D)\\xi+F \\end{aligned} ​W(α2​)=A[(ξ−α2​y2​)y1​]2+Bα22​+C(ξ−α2​y2​)y1​α2​+D(ξ−α2​y2​)y1​+Eα2​+F =(A+B−Cy1​y2​)α22​+(E−2Ay2​−Dy1​y2​)α2​+Aξ2+(C+D)ξ+F​ 那么求 W ~W~ W 的极小值，只需要简单得令 ∂W(α2)∂α2=0 ~\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2}=0~ ∂α2​∂W(α2​)​=0 即可，即： 2(A+B−Cy1y2)α2+(E−2Ay2−Dy1y2)=02(A+B-Cy_1y_2)\\alpha_2+(E-2Ay_2-Dy_1y_2)=0 2(A+B−Cy1​y2​)α2​+(E−2Ay2​−Dy1​y2​)=0 最终我们只需要保证我们优化后得 α ~\\alpha~ α 是使得目标函数变小的，就可以确定优化后的 α ~\\alpha~ α 是朝着不满足目标条件程度减小的方向移动。 5.2.3 SMO算法 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的更新 为了处理线性不可分的数据，我们引入了核函数： 求解目标：α=argminα 12∑i=1n∑j=1nαiαjyiyjk(xi,xj)−∑i=1nαi约束条件：∑i=1nαiyi=0,0≤αi≤C\\begin{aligned} &amp;求解目标：\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j)-\\sum_{i=1}^n\\alpha_i\\\\ &amp;约束条件：\\sum_{i=1}^n\\alpha_iy_i=0,0\\le\\alpha_i\\le C \\end{aligned} ​求解目标：α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​)−i=1∑n​αi​约束条件：i=1∑n​αi​yi​=0,0≤αi​≤C​ 核函数的实现代码如下所示，我们有多样的选择： 优化前后 α ~\\alpha~ α 都必须满足 ∑i=1nαiyi=0 ~\\sum_{i=1}^n\\alpha_iy_i=0~ ∑i=1n​αi​yi​=0 ，即： α1newy1+α2newy2=α1oldy1+α2oldy2=ξ\\alpha_1^{new}y_1+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\xi α1new​y1​+α2new​y2​=α1old​y1​+α2old​y2​=ξ 在更新 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 时，有： α1=(ξ−α2y2)y10≤α1≤C,0≤α2≤C\\alpha_1=(\\xi-\\alpha_2y_2)y_1\\\\ 0\\le\\alpha_1\\le C,0\\le\\alpha_2\\le C α1​=(ξ−α2​y2​)y1​0≤α1​≤C,0≤α2​≤C 将上述两个信息结合我们可以进一步得缩小 α2 ~\\alpha_2~ α2​ 的范围：我们将 α2 ~\\alpha_2~ α2​ 的上界与下界定义为 H,L ~H,L~ H,L L≤α2≤H\\begin{aligned} &amp;L\\le\\alpha_2\\le H \\end{aligned} ​L≤α2​≤H​ 将 α1y1+α2y2=ξ ~\\alpha_1y_1+\\alpha_2y_2=\\xi~ α1​y1​+α2​y2​=ξ 看作一个方程，显然几何上这是一条直线，结合取值范围我们可以绘制出图像： （1） y1=y2 ~y_1\\not=y_2~ y1​​=y2​ 时：有两种情况： α1−α2=ξ ~\\alpha_1-\\alpha_2=\\xi~ α1​−α2​=ξ ， α1−α2=−ξ ~\\alpha_1-\\alpha_2=-\\xi~ α1​−α2​=−ξ 0≤α2≤C0≤α1=α2+ξ≤C→−ξ≤α2≤C−ξ0≤α1=α2−ξ≤C→ ξ≤α2≤C+ξ\\begin{aligned} &amp;0\\le\\alpha_2\\le C\\\\ &amp;0\\le\\alpha_1=\\alpha_2+\\xi\\le C\\rightarrow -\\xi\\le\\alpha_2\\le C-\\xi\\\\ &amp;0\\le \\alpha_1=\\alpha_2-\\xi\\le C\\rightarrow ~~~\\xi\\le\\alpha_2\\le C+\\xi\\\\ \\end{aligned} ​0≤α2​≤C0≤α1​=α2​+ξ≤C→−ξ≤α2​≤C−ξ0≤α1​=α2​−ξ≤C→ ξ≤α2​≤C+ξ​ ① α1−α2=ξ ~\\alpha_1-\\alpha_2=\\xi~ α1​−α2​=ξ 时： L=max⁡(0,−ξ)=max⁡(0,α2−α1) H=min⁡(C,C−ξ)=min⁡(C,C+α2−α1)\\begin{aligned} &amp;~L=\\max(0,-\\xi)=\\max(0,\\alpha_2-\\alpha_1)\\\\ &amp;~H=\\min(C,C-\\xi)=\\min(C,C+\\alpha_2-\\alpha_1) \\end{aligned} ​ L=max(0,−ξ)=max(0,α2​−α1​) H=min(C,C−ξ)=min(C,C+α2​−α1​)​ ② α1−α2=−ξ ~\\alpha_1-\\alpha_2=-\\xi~ α1​−α2​=−ξ 时： L=max⁡(0,ξ)=max⁡(0,α2−α1)H=min⁡(C,C+ξ)=min⁡(C,C+α2−α1)\\begin{aligned} &amp;L=\\max(0,\\xi)=\\max(0,\\alpha_2-\\alpha_1)\\\\ &amp;H=\\min(C,C+\\xi)=\\min(C,C+\\alpha_2-\\alpha_1) \\end{aligned} ​L=max(0,ξ)=max(0,α2​−α1​)H=min(C,C+ξ)=min(C,C+α2​−α1​)​ 可以发现两种情况所得的上下界求解公式相同。 （2） y1=y2 ~y_1=y_2~ y1​=y2​ 时，有两种情况： α1+α2=ξ ~\\alpha_1+\\alpha_2=\\xi~ α1​+α2​=ξ ， α1+α2=−ξ ~\\alpha_1+\\alpha_2=-\\xi~ α1​+α2​=−ξ ，推导过程与 y1=y2 ~y_1\\not=y_2~ y1​​=y2​ 时的完全相同： 0≤α2≤C0≤α1= ξ−α2≤C → ξ−C≤α2≤ξ0≤α1=−ξ−α2≤C→−ξ−C≤α2≤−ξ\\begin{aligned} &amp;0\\le\\alpha_2\\le C\\\\ &amp;0\\le\\alpha_1=~~\\xi-\\alpha_2\\le C~\\rightarrow ~~~\\xi-C\\le\\alpha_2\\le\\xi\\\\ &amp;0\\le\\alpha_1=-\\xi-\\alpha_2\\le C\\rightarrow-\\xi-C\\le\\alpha_2\\le-\\xi \\end{aligned} ​0≤α2​≤C0≤α1​= ξ−α2​≤C → ξ−C≤α2​≤ξ0≤α1​=−ξ−α2​≤C→−ξ−C≤α2​≤−ξ​ ① α1+α2=ξ ~\\alpha_1+\\alpha_2=\\xi~ α1​+α2​=ξ 时： L=max⁡(0,ξ−C)=max⁡(0,α1+α2−C)H=min⁡(C,ξ)=min⁡(C,α1+α2)\\begin{aligned} &amp;L=\\max(0,\\xi-C)=\\max(0,\\alpha_1+\\alpha_2-C)\\\\ &amp;H=\\min(C,\\xi)=\\min(C,\\alpha_1+\\alpha_2) \\end{aligned} ​L=max(0,ξ−C)=max(0,α1​+α2​−C)H=min(C,ξ)=min(C,α1​+α2​)​ ② α1+α2=−ξ ~\\alpha_1+\\alpha_2=-\\xi~ α1​+α2​=−ξ 时： L=max⁡(0,−ξ−C)=max⁡(0,α1+α2−C)H=min⁡(C,−ξ)=min⁡(C,α1+α2)\\begin{aligned} &amp;L=\\max(0,-\\xi-C)=\\max(0,\\alpha_1+\\alpha_2-C)\\\\ &amp;H=\\min(C,-\\xi)=\\min(C,\\alpha_1+\\alpha_2) \\end{aligned} ​L=max(0,−ξ−C)=max(0,α1​+α2​−C)H=min(C,−ξ)=min(C,α1​+α2​)​ 两种情况所得的上下界求解公式也完全相同。 综上，上下界的计算通式如下： {L=max⁡(0,α2−α1) ,H=min⁡(C,C+α2−α1) if y1=y2L=max⁡(0,α1+α2−C),H=min⁡(C,α1+α2) if y1=y2\\left\\{ \\begin{aligned} &amp;L=\\max(0,\\alpha_2-\\alpha_1)~~~~~~~~,H=\\min(C,C+\\alpha_2-\\alpha_1)~~~~\\text{if }y_1\\not=y_2\\\\ &amp;L=\\max(0,\\alpha_1+\\alpha_2-C),H=\\min(C,\\alpha_1+\\alpha_2)~~~~~~~~~~~~\\text{if }y_1=y_2 \\end{aligned} \\right. {​L=max(0,α2​−α1​) ,H=min(C,C+α2​−α1​) if y1​​=y2​L=max(0,α1​+α2​−C),H=min(C,α1​+α2​) if y1​=y2​​ 接着我们要对α1,α2\\alpha_1,\\alpha_2α1​,α2​进行更新： α=argminα 12∑i=1n∑j=1nαiαjyiyjk(xi,xj)−∑i=1nαi=argminα W(α1,α2)\\alpha=\\underset{\\alpha}{argmin}~\\frac12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jk(x_i,x_j)-\\sum_{i=1}^n\\alpha_i=\\underset{\\alpha}{argmin}~W(\\alpha_1,\\alpha_2) α=αargmin​ 21​i=1∑n​j=1∑n​αi​αj​yi​yj​k(xi​,xj​)−i=1∑n​αi​=αargmin​ W(α1​,α2​) 将 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 视为变量，其余 αi ~\\alpha_i~ αi​ 视为常量，则有：核矩阵 Kij=k(xi,xj) ~K_{ij}=k(x_i,x_j)~ Kij​=k(xi​,xj​) W(α1,α2)=12K11α12+12K22α22+K12y1y2α1α2+y1α1∑i=3nαiyiKi1+y2α2∑i=3nαiyiKi2−α1−α2−∑i=3nαiW(\\alpha_1,\\alpha_2)=\\frac12K_{11}\\alpha_1^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1\\sum_{i=3}^{n}\\alpha_iy_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^{n}\\alpha_iy_iK_{i2}-\\alpha_1-\\alpha_2-\\sum_{i=3}^n\\alpha_i W(α1​,α2​)=21​K11​α12​+21​K22​α22​+K12​y1​y2​α1​α2​+y1​α1​i=3∑n​αi​yi​Ki1​+y2​α2​i=3∑n​αi​yi​Ki2​−α1​−α2​−i=3∑n​αi​ 我们定义： vj=∑i=3nαiyiKij ~v_j=\\sum_{i=3}^n\\alpha_iy_iK_{ij}~ vj​=∑i=3n​αi​yi​Kij​ ， Constant=∑i=3nαi ~\\text{Constant}=\\sum_{i=3}^n\\alpha_i~ Constant=∑i=3n​αi​ ，则有： W(α1,α2)=12K11α12+12K22α22+K12y1y2α1α2+y1α1v1+y2α2v2−α1−α2−ConstantW(\\alpha_1,\\alpha_2)=\\frac12K_{11}\\alpha_1^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_1y_2\\alpha_1\\alpha_2+y_1\\alpha_1v_1+y_2\\alpha_2v_2-\\alpha_1-\\alpha_2-\\text{Constant} W(α1​,α2​)=21​K11​α12​+21​K22​α22​+K12​y1​y2​α1​α2​+y1​α1​v1​+y2​α2​v2​−α1​−α2​−Constant 代入 α1=(ξ−y2α2)y1 ~\\alpha_1=(\\xi-y_2\\alpha_2)y_1~ α1​=(ξ−y2​α2​)y1​ 可得： W(α2)=12K11(ξ−y2α2)2+12K22α22+K12y2(ξ−y2α2)α2+v1(ξ−y2α2)+y2v2α2−(ξ−y2α2)y1−α2−Constant =12(K11+K22−2K12)α22+(K12ξy2−K11ξy2−v1y2+v2y2+y2y1−1)α2+12K11ξ2+v1ξ−ξy1−Constant\\begin{aligned} &amp;W(\\alpha_2)=\\frac12K_{11}(\\xi-y_2\\alpha_2)^2+\\frac12K_{22}\\alpha_2^2+K_{12}y_2(\\xi-y_2\\alpha_2)\\alpha_2+v_1(\\xi-y_2\\alpha_2)+y_2v_2\\alpha_2-(\\xi-y_2\\alpha_2)y_1-\\alpha_2-\\text{Constant}\\\\ &amp;~~~~~~~~~~~~=\\frac12(K_{11}+K_{22}-2K_{12})\\alpha_2^2+(K_{12}\\xi y_2-K_{11}\\xi y_2-v_1y_2+v_2y_2+y_2y_1-1)\\alpha_2+\\frac12K_{11}\\xi^2+v_1\\xi-\\xi y_1-\\text{Constant} \\end{aligned} ​W(α2​)=21​K11​(ξ−y2​α2​)2+21​K22​α22​+K12​y2​(ξ−y2​α2​)α2​+v1​(ξ−y2​α2​)+y2​v2​α2​−(ξ−y2​α2​)y1​−α2​−Constant =21​(K11​+K22​−2K12​)α22​+(K12​ξy2​−K11​ξy2​−v1​y2​+v2​y2​+y2​y1​−1)α2​+21​K11​ξ2+v1​ξ−ξy1​−Constant​ 对 W ~W~ W 进行求导可得： ∂W(α2)∂α2=(K11+K22−2K12)α2+(K12ξy2−K11ξy2−v1y2+v2y2+y2y1−1)\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2}=(K_{11}+K_{22}-2K_{12})\\alpha_2+(K_{12}\\xi y_2-K_{11}\\xi y_2-v_1y_2+v_2y_2+y_2y_1-1) ∂α2​∂W(α2​)​=(K11​+K22​−2K12​)α2​+(K12​ξy2​−K11​ξy2​−v1​y2​+v2​y2​+y2​y1​−1) 我们令导数为0即可求得我们所期望更新的 α2 ~\\alpha_2~ α2​ ： (K11+K22−2K12)α2=(y2−y1+K11ξ−K12ξ+v1−v2)y2(K_{11}+K_{22}-2K_{12})\\alpha_2=(y_2-y_1+K_{11}\\xi-K_{12}\\xi+v_1-v_2)y_2 (K11​+K22​−2K12​)α2​=(y2​−y1​+K11​ξ−K12​ξ+v1​−v2​)y2​ 我们可以不计算 ξ ~\\xi~ ξ ，通过 α2old ~\\alpha_2^{old}~ α2old​ 更新 α2new ~\\alpha_2^{new}~ α2new​ ： vj=∑i=3nαiyiKij=h(xj)−∑i=12αiyik(xi,xj)−bv1=h(x1)−α1y1K11−α2y2K21−bv2=h(x2)−α1y1K12−α2y2K22−b\\begin{aligned} &amp;v_j=\\sum_{i=3}^n\\alpha_iy_iK_{ij}=h(x_j)-\\sum_{i=1}^2\\alpha_iy_ik(x_i,x_j)-b\\\\ &amp;v_1=h(x_1)-\\alpha_1y_1K_{11}-\\alpha_2y_2K_{21}-b\\\\ &amp;v_2=h(x_2)-\\alpha_1y_1K_{12}-\\alpha_2y_2K_{22}-b \\end{aligned} ​vj​=i=3∑n​αi​yi​Kij​=h(xj​)−i=1∑2​αi​yi​k(xi​,xj​)−bv1​=h(x1​)−α1​y1​K11​−α2​y2​K21​−bv2​=h(x2​)−α1​y1​K12​−α2​y2​K22​−b​ 由此可得： v1−v2=h(x1)−h(x2)−(y1K11−y1K12)α1−(y2K21−y2K22)α2v_1-v_2=h(x_1)-h(x_2)-(y_1K_{11}-y_1K_{12})\\alpha_1-(y_2K_{21}-y_2K_{22})\\alpha_2 v1​−v2​=h(x1​)−h(x2​)−(y1​K11​−y1​K12​)α1​−(y2​K21​−y2​K22​)α2​ 代入 α1=(ξ−y2α2)y1 ~\\alpha_1=(\\xi-y_2\\alpha_2)y_1~ α1​=(ξ−y2​α2​)y1​ 可得： v1−v2=h(x1)−h(x2)−(K11−K12)(ξ−y2α2)−(y2K21−y2K22)α2 =h(x1)−h(x2)+(K11+K22−2K12)y2α2+(K12−K11)ξ\\begin{aligned} &amp;v_1-v_2=h(x_1)-h(x_2)-(K_{11}-K_{12})(\\xi-y_2\\alpha_2)-(y_2K_{21}-y_2K_{22})\\alpha_2\\\\ &amp;~~~~~~~~~~~~~~=h(x_1)-h(x_2)+(K_{11}+K_{22}-2K_{12})y_2\\alpha_2+(K_{12}-K_{11})\\xi \\end{aligned} ​v1​−v2​=h(x1​)−h(x2​)−(K11​−K12​)(ξ−y2​α2​)−(y2​K21​−y2​K22​)α2​ =h(x1​)−h(x2​)+(K11​+K22​−2K12​)y2​α2​+(K12​−K11​)ξ​ 将 v1−v2 ~v_1-v_2~ v1​−v2​ 代入导数为0的式子可得： (K11+K22−2K12)α2new=([h(x1)−y1]−[h(x2)−y2])y2+(K11+K22−2K12)α2old(K_{11}+K_{22}-2K_{12})\\alpha_2^{new}=(\\big[h(x_1)-y_1\\big]-\\big[h(x_2)-y_2\\big])y_2+(K_{11}+K_{22}-2K_{12})\\alpha_2^{old} (K11​+K22​−2K12​)α2new​=([h(x1​)−y1​]−[h(x2​)−y2​])y2​+(K11​+K22​−2K12​)α2old​ 根据我们之前设置的误差： Ei=h(xi)−yiE_i=h(x_i)-y_i Ei​=h(xi​)−yi​ 代入可得： α2new=α2old+(E1−E2)y2K11+K22−2K12\\alpha_2^{new}=\\alpha_2^{old}+\\frac{(E_1-E_2)y_2}{K_{11}+K_{22}-2K_{12}} α2new​=α2old​+K11​+K22​−2K12​(E1​−E2​)y2​​ 综上我们终于得到了 α2 ~\\alpha_2~ α2​ 更新的递归式，但是不要忘记了约束条件，于是我们将该 α2 ~\\alpha_2~ α2​ 记作未经修剪的（unclipped）： α2new,unc ~\\alpha_2^{new,unc}~ α2new,unc​ 前面我们已经求出了 α2 ~\\alpha_2~ α2​ 上下界应该满足的条件，即： {L=max⁡(0,α2−α1) ,H=min⁡(C,C+α2−α1) if y1=y2L=max⁡(0,α1+α2−C),H=min⁡(C,α1+α2) if y1=y2\\left\\{ \\begin{aligned} &amp;L=\\max(0,\\alpha_2-\\alpha_1)~~~~~~~~,H=\\min(C,C+\\alpha_2-\\alpha_1)~~~~\\text{if }y_1\\not=y_2\\\\ &amp;L=\\max(0,\\alpha_1+\\alpha_2-C),H=\\min(C,\\alpha_1+\\alpha_2)~~~~~~~~~~~~\\text{if }y_1=y_2 \\end{aligned} \\right. {​L=max(0,α2​−α1​) ,H=min(C,C+α2​−α1​) if y1​​=y2​L=max(0,α1​+α2​−C),H=min(C,α1​+α2​) if y1​=y2​​ 所以可以得到修剪后的 α2 ~\\alpha_2~ α2​ ： α2new={ H , α2new,unc &gt;H α2new,unc , L≤α2new,unc ≤H L , α2new,unc &lt;L\\alpha_2^{new}=\\left\\{ \\begin{aligned} &amp;~~~~~H~~~~~~,~\\alpha_2^{new,unc}~&gt;H\\\\ &amp;~\\alpha_2^{new,unc}~,~L\\le\\alpha_2^{new,unc}~\\le H\\\\ &amp;~~~~~L~~~~~~,~\\alpha_2^{new,unc}~&lt;L \\end{aligned} \\right. α2new​=⎩⎪⎨⎪⎧​​ H , α2new,unc​ &gt;H α2new,unc​ , L≤α2new,unc​ ≤H L , α2new,unc​ &lt;L​ 选取修剪后的 α2 ~\\alpha_2~ α2​ 的代码如下所示： 我们还知道： α1newy1+α2newy2=α1oldy1+α2oldy2\\alpha_1^{new}y_1+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2 α1new​y1​+α2new​y2​=α1old​y1​+α2old​y2​ 由此我们也可以得到 α1 ~\\alpha_1~ α1​ 的更新公式： α1new=α1old+y1y2(α2old−α2new)\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new}) α1new​=α1old​+y1​y2​(α2old​−α2new​) b ~b~ b 的更新 我们每更新一次 α ~\\alpha~ α 后，都要对 b ~b~ b 进行一次更新，因为这关系到 h(x) ~h(x)~ h(x) 的计算，进而关系到 Ei ~E_i~ Ei​ 的计算。 ①当 0&lt;α1new&lt;C ~0&lt;\\alpha_1^{new}&lt;C~ 0&lt;α1new​&lt;C 时， x1 ~x_1~ x1​ 为支持向量，则有： y1(wTx1+b1)=1b1new=y1−∑i=1nαiyiKi1=y1−v1−α1newy1K11−α2newy2K12v1=h(x1)−α1oldy1K11−α2oldy2K21−boldy_1(w^Tx_1+b_1)=1\\\\ b_1^{new}=y_1-\\sum_{i=1}^n\\alpha_iy_iK_{i1}=y_1-v_1-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{12}\\\\ v_1=h(x_1)-\\alpha_1^{old}y_1K_{11}-\\alpha_2^{old}y_2K_{21}-b^{old} y1​(wTx1​+b1​)=1b1new​=y1​−i=1∑n​αi​yi​Ki1​=y1​−v1​−α1new​y1​K11​−α2new​y2​K12​v1​=h(x1​)−α1old​y1​K11​−α2old​y2​K21​−bold 则我们可以得到： b1new=bold−E1+(α1old−α1new)y1K11+(α2old−α2new)y2K21b_1^{new}=b^{old}-E_1+(\\alpha_1^{old}-\\alpha_1^{new})y_1K_{11}+(\\alpha_2^{old}-\\alpha_2^{new})y_2K_{21} b1new​=bold−E1​+(α1old​−α1new​)y1​K11​+(α2old​−α2new​)y2​K21​ ②当 0&lt;α2new&lt;C ~0&lt;\\alpha_2^{new}&lt;C~ 0&lt;α2new​&lt;C 时， x2 ~x_2~ x2​ 为支持向量，同理有： b2new=bold−E2+(α1old−α1new)y1K12+(α2old−α2new)y2K22b_2^{new}=b^{old}-E_2+(\\alpha_1^{old}-\\alpha_1^{new})y_1K_{12}+(\\alpha_2^{old}-\\alpha_2^{new})y_2K_{22} b2new​=bold−E2​+(α1old​−α1new​)y1​K12​+(α2old​−α2new​)y2​K22​ ③当 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 都不满足上述情况时，有： bnew=b1new+b2new2b^{new}=\\frac{b_1^{new}+b_2^{new}}2 bnew=2b1new​+b2new​​ 综上我们可以得到更新 b ~b~ b 的通式： bnew={ b1new ,0&lt;α1new&lt;C b2new , 0&lt;α2new&lt;Cb1new+b2new2,otherwiseb^{new}=\\left\\{ \\begin{aligned} &amp;~~~~~~b_1^{new}~~~~~~~,0&lt;\\alpha_1^{new}&lt;C\\\\ &amp;~~~~~~b_2^{new}~~~~~~~,~0&lt;\\alpha_2^{new}&lt;C\\\\ &amp;\\frac{b_1^{new}+b_2^{new}}2,\\text{otherwise} \\end{aligned} \\right. bnew=⎩⎪⎪⎪⎨⎪⎪⎪⎧​​ b1new​ ,0&lt;α1new​&lt;C b2new​ , 0&lt;α2new​&lt;C2b1new​+b2new​​,otherwise​ α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的选取 有了上述更新的方法，我们只剩下一个问题，就是如何选择 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ ，选择方法如下： ①选取 α1 ~\\alpha_1~ α1​ ：遍历所有 αi ~\\alpha_i~ αi​ ，把第一个不满足 KKT ~KKT~ KKT 条件的作为 α1 ~\\alpha_1~ α1​ ②选取 α2 ~\\alpha_2~ α2​ ：在所有不违反 KKT ~KKT~ KKT 条件的 αi ~\\alpha_i~ αi​ 中选取 ∣E1−E2∣ ~|E_1-E_2|~ ∣E1​−E2​∣ 最大的作为 α2 ~\\alpha_2~ α2​ 根据上述规则我们可以得到选取 α1,α2 ~\\alpha_1,\\alpha_2~ α1​,α2​ 的代码： 我们知道 KKT ~KKT~ KKT 条件为： {yi⋅h(xi)≥1→ αi=0 ,xi在边界内yi⋅h(xi)=1→0&lt;αi&lt;C,xi在边界上，是支持向量yi⋅h(xi)≤1→ αi=C ,xi在两条边界之间\\begin{aligned} &amp;\\left\\{ \\begin{aligned} &amp;y_i\\cdot h(x_i)\\ge1\\rightarrow ~~~~\\alpha_i=0~~~~,x_i在边界内\\\\ &amp;y_i\\cdot h(x_i)=1\\rightarrow 0&lt;\\alpha_i&lt; C,x_i在边界上，是支持向量\\\\ &amp;y_i\\cdot h(x_i)\\le1\\rightarrow ~~~~\\alpha_i=C~~~,x_i在两条边界之间 \\end{aligned} \\right. \\end{aligned} ​⎩⎪⎨⎪⎧​​yi​⋅h(xi​)≥1→ αi​=0 ,xi​在边界内yi​⋅h(xi​)=1→0&lt;αi​&lt;C,xi​在边界上，是支持向量yi​⋅h(xi​)≤1→ αi​=C ,xi​在两条边界之间​​ 那么违反 KKT ~KKT~ KKT 条件的情况为： ① yih(xi)&gt;1 但 αi&gt;0② yih(xi)=1 但 αi=0或C③ yih(xi)&lt;1 但 αi&lt;C\\begin{aligned} &amp;①~y_ih(x_i)&gt;1~但~\\alpha_i&gt;0\\\\ &amp;②~y_ih(x_i)=1~但~\\alpha_i=0或C\\\\ &amp;③~y_ih(x_i)&lt;1~但~\\alpha_i&lt;C\\\\ \\end{aligned} ​① yi​h(xi​)&gt;1 但 αi​&gt;0② yi​h(xi​)=1 但 αi​=0或C③ yi​h(xi​)&lt;1 但 αi​&lt;C​ 为了便于判断我们可以进一步得进行处理： yih(xi)−1=yiEiy_ih(x_i)-1=y_iE_i yi​h(xi​)−1=yi​Ei​ 那么违反 KKT ~KKT~ KKT 条件的情况为： ① yiEi&lt;0 但 αi&lt;C② yiEi&gt;0 但 αi&gt;0\\begin{aligned} &amp;①~y_iE_i&lt;0~但~\\alpha_i&lt;C\\\\ &amp;②~y_iE_i&gt;0~但~\\alpha_i&gt;0 \\end{aligned} ​① yi​Ei​&lt;0 但 αi​&lt;C② yi​Ei​&gt;0 但 αi​&gt;0​ 但在实际中 KKT ~KKT~ KKT 条件是极其苛刻的，我们通过引入容错率 tol ~tol~ tol 来缓解， tol ~tol~ tol 一般取值为 0.0001 ~0.0001~ 0.0001 ① yiEi&lt;tol 但 αi&lt;C② yiEi&gt;tol 但 αi&gt;0\\begin{aligned} &amp;①~y_iE_i&lt;tol~但~\\alpha_i&lt;C\\\\ &amp;②~y_iE_i&gt;tol~但~\\alpha_i&gt;0 \\end{aligned} ​① yi​Ei​&lt;tol 但 αi​&lt;C② yi​Ei​&gt;tol 但 αi​&gt;0​ 由此我们可以定义 KKT ~KKT~ KKT 条件的相关代码： 综上，我们可以实现 SMO ~SMO~ SMO 算法+核函数的高效的支持向量机模型： 为了避免随机性影响模型评估，我们使用固定的数据集，并且固定得划分训练集与测试集，用到的测试集如下图所示： 我们可以发现对于上述线性不可分的数据，我们使用高斯核，通过调整参数达到了0.99的正确率，效果很好。 5.3 可视化 我们可以通过如下代码实现对决策边界的可视化： 我们可以观察不同的核函数得到的决策边界： ①高斯核函数： ②线性核函数： ③多项式核函数： 我们可以发现对于线性不可分数据高斯核函数的效果非常好。 5.4 调库实现模型 当然sklearn也提供了可以利用核函数的支持向量机，主要用到的函数为SVC，其函数原型为： 其中一些重要的参数为： ①C：软间隔的惩罚系数 ②kernel：用到的核函数：线性核：”linear“，多项式核：”poly“，高斯核：”rbf“，sigmoid核：”sigmoid“，预计算：”precomputed“ ③degree：多项式核的指数 ④gamma：”poly“，”rbf“，”sigmoid“核的系数选择方式 ● if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, ● if ‘auto’, uses 1 / n_features. 更多参数作用请详见官方说明文档：sklearn.svm.SVC 注意可视化处作了部分调整： 最终我们得到了0.995的准确率，决策边界如下图所示： 对比之下不难发现我们的模型虽然正确率也很高，但是存在一定程度的过拟合，调库所得的决策边界更好一些。 ","link":"https://2006wzt.github.io/post/机器学习实战（十三）：核函数/"},{"title":"机器学习实战（十二）：参数权衡","content":"参数权衡 一、模型评估 我们需要确定自己的模型当前效果如何来制定合理的优化策略，因此需要在线下对模型进行评估。 1.1 K折交叉验证 将数据集按层划分为 K ~K~ K 个互斥的子集，每次选择 K−1 ~K-1~ K−1 个子集作为训练集，剩下的一个子集作为验证集总共训练 K ~K~ K 次，将 K ~K~ K 次训练的测试结果取平均。 K ~K~ K 折交叉验证的误差为： ϵK−Fold=1n∑k=1K∑xi∈DkI(h(xi;D−Dk)=yi)\\epsilon_{K-Fold}=\\frac1n\\sum_{k=1}^K\\sum_{x_i\\in D_k}I(h(x_i;D-D_k)\\not=y_i) ϵK−Fold​=n1​k=1∑K​xi​∈Dk​∑​I(h(xi​;D−Dk​)​=yi​) 其中 D−Dk ~D-D_k~ D−Dk​ 是用 Dk ~D_k~ Dk​ 作验证集时的训练集， h(xi;D−Dk) ~h(x_i;D-D_k)~ h(xi​;D−Dk​) 是在 D−Dk ~D-D_k~ D−Dk​ 上训练出的模型。 1.2 排除交叉验证 在极端情况下，可以使用 K=n ~K=n~ K=n ，即只保留一个数据点（这通常称为LOOCV-保留一个交叉验证）。如果我们的数据集很小，并且无法保留许多数据点进行评估，则LOOCV很重要。 它的误差为： ϵLOOCV=1n∑i=1nI(h(xi;D−i)=yi)\\epsilon_{LOOCV}=\\frac1n\\sum_{i=1}^n I(h(x_i;D_{-i})\\not= y_i) ϵLOOCV​=n1​i=1∑n​I(h(xi​;D−i​)​=yi​) 其中 D−i ~D_{-i}~ D−i​ 是 D ~D~ D 去掉第 i ~i~ i 个数据点后的数据集， h(xi;D−i) ~h(x_i;D_{-i})~ h(xi​;D−i​) 是在 D−i ~D_{-i}~ D−i​ 上训练出的模型。 二、参数调整 我们以经验风险最小化为例： l(w)=1n∑i=1nI(h(xi),yi)+λr(w)l(w)=\\frac1n\\sum_{i=1}^nI(h(x_i),y_i)+\\lambda r(w) l(w)=n1​i=1∑n​I(h(xi​),yi​)+λr(w) 我们应该思考如何选择 λ ~\\lambda~ λ 才能尽可能得优化模型。 2.1 欠拟合与过拟合 回顾我们之前所学：在数据集上学习分类器时，可能会出现欠拟合和过拟合两种情况，每种情况都与训练集中的数据外推到未知数据的程度有关： ①欠拟合：在训练集上学习的分类器表达能力不足，甚至无法解释所提供的数据。 在这种情况下，训练误差和测试误差都会很高，因为分类器没有考虑训练集中存在的相关信息。 ②过拟合：在训练集上学习的分类器过于具体，无法用于准确推断有关未看到数据的任何内容。 虽然随着时间的推移，训练错误会继续减少，但随着分类器开始根据仅存在于训练集中而不存在于更广泛分布中的模式做出决策，测试错误会再次增加。 我们所追求的是模型在验证集上的误差最小，即寻找甜点。 2.2 显微镜式搜索 以经验风险最小化的参数 λ ~\\lambda~ λ 为例： ①我们首先找到 λ ~\\lambda~ λ 的最佳数量级：比如 λ ~\\lambda~ λ 数量级为：0.01,0.1,1,10,1000.01,0.1,1,10,1000.01,0.1,1,10,100中的 10 ~10~ 10 ②围绕最佳的数量级进行更细致的搜索：比如围绕 λ ~\\lambda~ λ 最佳的数量级 10 ~10~ 10 搜索： 5,10,15,20,...,95 ~5,10,15,20,...,95~ 5,10,15,20,...,95 三、偏差-方差权衡 3.1 问题假设 首先我们对数据集进行假设：数据集 D ~D~ D 是从某个分布 P(x,y) ~P(x,y)~ P(x,y) 的独立同分布中获取的。 ①期望标签：对于一个特征向量 x ~x~ x ，可能并不存在唯一的标签 y ~y~ y ，比如 x ~x~ x 描述一所房子的特征， y ~y~ y 是房子的价格，但是两所相同条件的房子可能会以不同的价格出售，所以对于任何给定的特征向量 x ~x~ x ，在可能的标签上都有一个分布。为此我们引入期望标签： y‾=Ey∣x[Y]=∫yy⋅P(y∣x) ∂y\\overline{y}=E_{y|x}[Y]=\\int_yy\\cdot P(y|x)~\\partial y y​=Ey∣x​[Y]=∫y​y⋅P(y∣x) ∂y 期望标签表示给定特征向量 x ~x~ x 时预期获得的标签。 ②期望测试误差：我们用算法 A ~\\mathcal{A}~ A 学习出一个模型 h ~h~ h ，记作： hD=A(D) ~h_D=\\mathcal{A}(D)~ hD​=A(D) ，由此我们可以定义泛化误差： E(x,y)∼P[(hD(x)−y)2]=∫x∫y(hD(x)−y)2P(x,y) ∂y∂xE_{(x,y)\\sim P}\\big[(h_D(x)-y)^2\\big]=\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x E(x,y)∼P​[(hD​(x)−y)2]=∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x 我们也可以假设 hD ~h_D~ hD​ 是一个随机的模型，我们目前只知道算法 A ~\\mathcal{A}~ A ，对测试误差求取期望： E(x,y)∼P,D∼Pn[(hD(x)−y)2]=∫D∫x∫y(hD(x)−y)2P(x,y) ∂y∂x∂DE_{(x,y)\\sim P,D\\sim P^n}\\big[(h_D(x)-y)^2\\big]=\\int_D\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x\\partial D E(x,y)∼P,D∼Pn​[(hD​(x)−y)2]=∫D​∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x∂D ③期望模型：模型 h ~h~ h 其实也是数据集 D ~D~ D 的一个函数，对于给定的算法 A ~\\mathcal{A}~ A ，我们可以对模型 h ~h~ h 求取期望： h‾=ED∼Pn[hD]=∫DhDP(D)∂D\\overline{h}=E_{D\\sim P^n}\\big[h_D\\big]=\\int_D h_DP(D)\\partial D h=ED∼Pn​[hD​]=∫D​hD​P(D)∂D P(D) ~P(D)~ P(D) 是从分布 Pn ~P^n~ Pn 中取出数据集 D ~D~ D 的概率， h‾ ~\\overline{h}~ h 是模型的加权平均。 3.2 期望测试误差的分解 我们可以对期望测试误差进行分解： E(x,y)∼P,D∼Pn[(hD(x)−y)2]=∫D∫x∫y(hD(x)−y)2P(x,y) ∂y∂x∂DE_{(x,y)\\sim P,D\\sim P^n}\\big[(h_D(x)-y)^2\\big]=\\int_D\\int_x\\int_y(h_D(x)-y)^2P(x,y)~\\partial y\\partial x\\partial D E(x,y)∼P,D∼Pn​[(hD​(x)−y)2]=∫D​∫x​∫y​(hD​(x)−y)2P(x,y) ∂y∂x∂D 分解过程如下： Ex,y,D[(hD(x)−y)2]=Ex,y,D[(hD(x)−h‾(x)+h‾(x)−y)2]=Ex,D[(hD(x)−h‾(x))2]+2Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]+Ex,y[(h‾(x)−y)2]\\begin{aligned} &amp;E_{x,y,D}\\big[(h_D(x)-y)^2\\big]=E_{x,y,D}\\big[ (h_D(x)-\\overline{h}(x)+\\overline{h}(x)-y)^2\\big]\\\\ &amp;=E_{x,D}\\big[(h_D(x)-\\overline{h}(x))^2 \\big]+2E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]+E_{x,y}\\big[(\\overline{h}(x)-y)^2\\big] \\end{aligned} ​Ex,y,D​[(hD​(x)−y)2]=Ex,y,D​[(hD​(x)−h(x)+h(x)−y)2]=Ex,D​[(hD​(x)−h(x))2]+2Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]+Ex,y​[(h(x)−y)2]​ 我们对第2项继续分解： Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]=Ex,y[(ED[hD(x)]−h‾(x))(h‾(x)−y)]∵ED[hD(x)]=h‾(x)∴Ex,y,D[(hD(x)−h‾(x))(h‾(x)−y)]=0\\begin{aligned} &amp;E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]=E_{x,y}\\big[(E_D\\big[h_D(x)\\big]-\\overline{h}(x))(\\overline{h}(x)-y)\\big]\\\\ &amp;\\because E_D\\big[h_D(x)\\big]=\\overline{h}(x)\\therefore E_{x,y,D}\\big[(h_D(x)-\\overline{h}(x))(\\overline{h}(x)-y)\\big]=0 \\end{aligned} ​Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]=Ex,y​[(ED​[hD​(x)]−h(x))(h(x)−y)]∵ED​[hD​(x)]=h(x)∴Ex,y,D​[(hD​(x)−h(x))(h(x)−y)]=0​ 我们对第3项继续分解： Ex,y[(h‾(x)−y)2]=Ex,y[(h‾(x)−y‾(x)+y‾(x)−y)2]=Ex[(h‾(x)−y‾(x))2]+2Ex,y[(h‾(x)−y‾(x))(y‾(x)−y)]+Ex,y[(y‾(x)−y)2]\\begin{aligned} &amp;E_{x,y}\\big[(\\overline{h}(x)-y)^2\\big]=E_{x,y}\\big[(\\overline{h}(x)-\\overline{y}(x)+\\overline{y}(x)-y)^2\\big]\\\\ &amp;=E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))^2\\big]+2E_{x,y}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]+E_{x,y}\\big[(\\overline{y}(x)-y)^2\\big] \\end{aligned} ​Ex,y​[(h(x)−y)2]=Ex,y​[(h(x)−y​(x)+y​(x)−y)2]=Ex​[(h(x)−y​(x))2]+2Ex,y​[(h(x)−y​(x))(y​(x)−y)]+Ex,y​[(y​(x)−y)2]​ 继续分解： Ex[(h‾(x)−y‾(x))(y‾(x)−y)]=Ey[(h‾(x)−y‾(x))(y‾(x)−Ey∣x[y])]∵Ey∣x[y]=y‾(x)∴Ex[(h‾(x)−y‾(x))(y‾(x)−y)]=0\\begin{aligned} &amp;E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]=E_y\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-E_{y|x}\\big[y\\big])\\big]\\\\ &amp;\\because E_{y|x}\\big[y\\big]=\\overline{y}(x)\\therefore E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))(\\overline{y}(x)-y)\\big]=0 \\end{aligned} ​Ex​[(h(x)−y​(x))(y​(x)−y)]=Ey​[(h(x)−y​(x))(y​(x)−Ey∣x​[y])]∵Ey∣x​[y]=y​(x)∴Ex​[(h(x)−y​(x))(y​(x)−y)]=0​ 综上可得： Ex,y,D[(hD(x)−y)2]=Ex,D[(hD(x)−h‾(x))2]+Ex[(h‾(x)−y‾(x))2]+Ex,y[(y‾(x)−y)2]E_{x,y,D}\\big[(h_D(x)-y)^2\\big]=E_{x,D}\\big[(h_D(x)-\\overline{h}(x))^2 \\big]+E_{x}\\big[(\\overline{h}(x)-\\overline{y}(x))^2\\big]+E_{x,y}\\big[(\\overline{y}(x)-y)^2\\big] Ex,y,D​[(hD​(x)−y)2]=Ex,D​[(hD​(x)−h(x))2]+Ex​[(h(x)−y​(x))2]+Ex,y​[(y​(x)−y)2] 其中：Variance为方差，Noise为噪声，Bias为偏差。 3.3 方差、偏差、噪声 ①方差：它可以衡量我们用另一个训练集训练模型时，分类器的变化程度，如果存在最优分类器，则方差则衡量它离最优分类器有多远 ②偏差：偏差是模型所固有的，它可以用来衡量分类器有多偏向于某个特定的方案 ③噪声：噪声是数据所固有的，它可以用来衡量由于数据分布和特征表示而产生的歧义 如下图所示，假设靶心为我们所期望的最优模型的测试结果，利用不同的训练集 D ~D~ D 训练出的模型测试结果如各点分布，离靶心越近说明模型的偏差越小，测试的点越聚合说明模型的方差越小。 偏差和方差随模型复杂性的变化：由图可知模型越复杂，偏差越小，方差越大，因此我们需要对二者进行权衡 3.4 调试算法 此处我们讨论如何对机器学习算法进行调试：如果分类器性能不佳（例如，如果测试或训练误差过高）。有几种方法可以提高性能，要找出这些技术中哪一种适合这种情况，第一步是确定问题的根源。 如下图所示是训练误差和测试误差随着训练集大小的变化： 上图中有两个主要区域： ①Regime#1：训练误差低于期望的误差阈值 ε ~\\varepsilon~ ε ，但是测试误差较高，出现过拟合 ②Regime#2：训练误差和测试误差很接近，但二者都高于期望的误差阈值 ε ~\\varepsilon~ ε 3.4.1 高方差 区域Regime#1所反映的问题是高方差所导致的，由于方差较高，导致训练出的模型与我们所期望的模型差距较大。 辨别高方差的方法为： ①训练误差低于测试误差 ②训练误差下小于 ε ~\\varepsilon~ ε ，测试误差高于 ε ~\\varepsilon~ ε 解决高方差的方法为：利用更多的训练实例、降低模型复杂度等 3.4.2 高偏差 区域Regime#2所反映的问题是高偏差所导致的，由于偏差较高，导致模型拟合的结果与我们所期望的结果差距较大。 辨别高偏差的方法为：训练误差大于 ε ~\\varepsilon~ ε 解决高偏差的方法为：使用更复杂的模型、添加功能 ","link":"https://2006wzt.github.io/post/机器学习实战（十二）：参数权衡/"},{"title":"机器学习实战（十一）：经验风险最小化","content":"经验风险最小化 一、形式化定义 1.1 知识回顾 我们首先回顾一下SVM模型： (w,b)=argminw,b wTw+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ wTw+Ci=1∑n​max(1−yi​(wTxi​+b),0) 其对应的损失函数为： l(w,b)=∣∣w∣∣22+C∑i=1nmax⁡(1−yi(wTxi+b),0)l(w,b)=||w||_2^2+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) l(w,b)=∣∣w∣∣22​+Ci=1∑n​max(1−yi​(wTxi​+b),0) 我们将 C∑i=1nmax⁡(1−yi(wTxi+b),0) ~C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0)~ C∑i=1n​max(1−yi​(wTxi​+b),0) 称为铰链损失函数， ∣∣w∣∣22 ~||w||_2^2~ ∣∣w∣∣22​ 称为 l2 ~l2~ l2 正则化项 损失函数是惩罚训练错误的连续函数，正则化器是惩罚分类器复杂性的连续函数。 经验风险最小化模型的一般形式为：由损失函数 I(h(x),y) ~I(h(x),y)~ I(h(x),y) 和正则化项 r(w) ~r(w)~ r(w) 构成： min⁡w1n∑i=1nI(h(xi),yi)+λr(w)\\min_{w}\\frac1n\\sum_{i=1}^n I(h(x_i),y_i)+\\lambda r(w) wmin​n1​i=1∑n​I(h(xi​),yi​)+λr(w) 经验风险最小化（Empirical Risk Minimization）策略认为经验风险最小的模型是最优的模型，当样本容量足够大时，经验风险最小化能保证有很好的学习效果。比如，极大似然估计就是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但当样本容量很小时，经验风险最小化容易导致“过拟合”。 1.2 核心思想 我们考虑一个监督学习常见的场景： （1）我们有两个对象空间： X ~\\mathcal{X}~ X 和 Y ~\\mathcal{Y}~ Y ，X∈X,Y∈YX\\in \\mathcal{X},Y\\in\\mathcal{Y}X∈X,Y∈Y 我们要建模一个函数 h∈H ~h\\in\\mathcal{H}~ h∈H ，函数 h ~h~ h 是 X→Y ~X\\rightarrow Y~ X→Y 的映射，对于任意 x∈X ~x\\in X~ x∈X ，有 h(x)=y∈Y ~h(x)=y\\in Y~ h(x)=y∈Y （2）而模型的训练建立在我们所拥有的数据集： D={(x1,y1),(x2,y2),...,(xn,yn)} ~D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}~ D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)} 我们假设存在一个建立在 X ~X~ X 和 Y ~Y~ Y 上的联合分布 P(x,y) ~P(x,y)~ P(x,y) ， D ~D~ D 是从 P(x,y) ~P(x,y)~ P(x,y) 对应的一个独立同分布中获得的数据 独立同分布：每次抽样之间独立而且数据属于同一分布 注意：联合概率分布的假设允许我们对预测中的不确定性进行建模，因为 y ~y~ y 不是 x ~x~ x 的确定函数，而是一个具有固定 x ~x~ x 的条件分布 P(y∣x) ~P(y|x)~ P(y∣x) 的随机变量。 （3）我们还假设我们得到了一个非负的损失函数 L(h(x),y) ~L(h(x),y)~ L(h(x),y) ，它能反映出我们对标签的估计值和实际值之间的差距 然后我们将与假设 h(x) ~h(x)~ h(x) 的相关风险定义为损失函数的期望值： R(h)=E[L(h(x),y)]=∫L(h(x),y) dP(x,y)R(h)=E\\big[L(h(x),y)\\big]=\\int L(h(x),y)~\\mathrm{d}P(x,y) R(h)=E[L(h(x),y)]=∫L(h(x),y) dP(x,y) （4）我们的最终目标为找到一个： h∗=argminh R(h)h^*=\\underset{h}{argmin}~R(h) h∗=hargmin​ R(h) （5）通常，风险 R(h) ~R(h)~ R(h) 无法计算，因为学习算法并不知道联合分布 P(x,y) ~P(x,y)~ P(x,y) ，这种情况称为不可知学习 然而，我们可以通过平均训练集上的损失函数来计算近似值，我们称之为经验风险： Remp=1n∑i=1n L(h(xi),yi)R_{emp}=\\frac1n\\sum_{i=1}^n~L(h(x_i),y_i) Remp​=n1​i=1∑n​ L(h(xi​),yi​) 相应的，我们经验风险最小化所要建立的模型即为： h∗=argminh∈H Remp(h)h^*=\\underset{h\\in\\mathcal{H}}{argmin}~R_{emp}(h) h∗=h∈Hargmin​ Remp​(h) 经验风险最小化（ERM）是统计学习理论中的一个原则，它定义了一系列学习算法，并用于给出其性能的理论界限 二、损失函数 2.1 分类问题损失函数 （1）Hinge-Loss：在 p=1 ~p=1~ p=1 时，是软约束的支持向量机所引入的铰链损失函数 l(h(xi),yi)=max⁡(1−yi(wTxi+b),0)pl(h(x_i),y_i)=\\max(1-y_i(w^Tx_i+b),0)^p l(h(xi​),yi​)=max(1−yi​(wTxi​+b),0)p （2）Log-Loss：用于逻辑回归的损失函数 l(h(xi),yi)=log⁡(1+e−yih(xi))l(h(x_i),y_i)=\\log(1+e^{-y_ih(x_i)}) l(h(xi​),yi​)=log(1+e−yi​h(xi​)) （3）Exponential-Loss：指数损失函数，一般用于自适应提升算法 l(h(xi),yi)=e−yih(xi)l(h(x_i),y_i)=e^{-y_ih(x_i)} l(h(xi​),yi​)=e−yi​h(xi​) （4）Zero-One-Loss：0-1损失函数，用于分类问题 l(h(xi),yi)=I(h(xi)=yi)l(h(x_i),y_i)=I(h(x_i)\\not= y_i) l(h(xi​),yi​)=I(h(xi​)​=yi​) 如上图所示，横坐标为 yih(xi) ~y_ih(x_i)~ yi​h(xi​) 的值，纵坐标代表损失函数的值。 2.2 回归问题损失函数 （1）Squared-Loss：平方损失函数 l(h(xi),yi)=(h(xi)−yi)2l(h(x_i),y_i)=(h(x_i)-y_i)^2 l(h(xi​),yi​)=(h(xi​)−yi​)2 （2）Absolute-Loss：绝对值损失函数 l(h(xi),yi)=∣h(xi)−yi∣l(h(x_i),y_i)=|h(x_i)-y_i| l(h(xi​),yi​)=∣h(xi​)−yi​∣ （3）Huber-Loss：对 MSE ~MSE~ MSE 和 MAE ~MAE~ MAE 的结合，超参数 δ ~\\delta~ δ 决定了Huber-Loss对二者的侧重性 l(h(xi),yi)={ 12(h(xi)−yi)2 ,∣h(xi)−yi∣&lt;δδ⋅(∣h(xi)−yi∣−δ2),∣h(xi)−yi∣≥δl(h(x_i),y_i)=\\left\\{ \\begin{aligned} &amp;~~~~~\\frac12(h(x_i)-y_i)^2~~~~~,|h(x_i)-y_i|&lt;\\delta\\\\ &amp;\\delta\\cdot(|h(x_i)-y_i|-\\frac{\\delta}2),|h(x_i)-y_i|\\ge\\delta \\end{aligned} \\right. l(h(xi​),yi​)=⎩⎪⎪⎨⎪⎪⎧​​ 21​(h(xi​)−yi​)2 ,∣h(xi​)−yi​∣&lt;δδ⋅(∣h(xi​)−yi​∣−2δ​),∣h(xi​)−yi​∣≥δ​ （4）Log-Cosh-Loss： cosh⁡(x)=ex+e−x2 ~\\cosh(x)=\\frac{e^x+e^{-x}}{2}~ cosh(x)=2ex+e−x​ l(h(xi),yi)=log⁡cosh⁡(h(xi)−yi)l(h(x_i),y_i)=\\log\\cosh(h(x_i)-y_i) l(h(xi​),yi​)=logcosh(h(xi​)−yi​) 如上图所示，横坐标为 h(xi)−yi ~h(x_i)-y_i~ h(xi​)−yi​ 的值，纵坐标为损失函数的值。 三、正则化 在数学、统计学和计算机科学中，特别是在机器学习问题中，正则化是指添加信息以解决不确定问题或防止过度拟合的过程。 正则化器有助于改变优化问题的公式，以获得更好的几何直觉。 （1） l1 ~l1~ l1 正则化项： r(w)=∣∣w∣∣1=∑i=1dwir(w)=||w||_1=\\sum_{i=1}^d w_i r(w)=∣∣w∣∣1​=i=1∑d​wi​ （2） l2 ~l2~ l2 正则化项： r(w)=∣∣w∣∣22=∑i=1dwi2r(w)=||w||_2^2=\\sum_{i=1}^d w_i^2 r(w)=∣∣w∣∣22​=i=1∑d​wi2​ （3） lp ~lp~ lp 范数： r(w)=∣∣w∣∣p=(∑i=1dwip)1pr(w)=||w||_p=(\\sum_{i=1}^d w_i^p)^{\\frac1p} r(w)=∣∣w∣∣p​=(i=1∑d​wip​)p1​ （4）弹性网络（Elastic-Net）：对 l1 ~l1~ l1 和 l2 ~l2~ l2 正则化项的结合，超参数 α ~\\alpha~ α 决定了弹性网络对二者的侧重性， α ∈[0,1)~\\alpha~\\in[0,1) α ∈[0,1) r(w)=α∣∣w∣∣1+(1−α)∣∣w∣∣22r(w)=\\alpha||w||_1+(1-\\alpha)||w||_2^2 r(w)=α∣∣w∣∣1​+(1−α)∣∣w∣∣22​ 一些著名特例为： （1）最小二乘法： w^MLE=argminw 1n∑i=1n(xiTw−yi)2\\hat{w}_{MLE}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2 w^MLE​=wargmin​ n1​i=1∑n​(xiT​w−yi​)2 （2）岭回归： w^MAP=argminw 1n∑i=1n(xiTw−yi)2+λ∣∣w∣∣22\\hat{w}_{MAP}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_2^2 w^MAP​=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣22​ （3）Lasso回归： w^=argminw 1n∑i=1n(xiTw−yi)2+λ∣∣w∣∣1\\hat{w}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_1 w^=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣1​ （4）弹性网： w^=argminw 1n∑i=1n(xiTw−yi)2+α∣∣w∣∣1+(1−α)∣∣w∣∣22\\hat{w}=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\alpha||w||_1+(1-\\alpha)||w||_2^2 w^=wargmin​ n1​i=1∑n​(xiT​w−yi​)2+α∣∣w∣∣1​+(1−α)∣∣w∣∣22​ （5）逻辑回归： w^=argminw log⁡(1+e−yi(wTxi+b))\\hat{w}=\\underset{w}{argmin}~\\log(1+e^{-y_i(w^Tx_i+b)}) w^=wargmin​ log(1+e−yi​(wTxi​+b)) （6）支持向量机： (w,b)=argminw,b ∣∣w∣∣22+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~||w||_2^2+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ ∣∣w∣∣22​+Ci=1∑n​max(1−yi​(wTxi​+b),0) ","link":"https://2006wzt.github.io/post/机器学习实战（十一）：经验风险最小化/"},{"title":"机器学习实战（十）：支持向量机","content":"支持向量机 一、基本思想 支持向量机（Support Vector Machine）可以看作是感知机的扩展。如果存在一个超平面可以将数据点分为两类，感知机将会找到这个超平面，而支持向量机则是找到一个具有最大边距的超平面。 其形式化定义与感知机相同： 数据集：D={(x1,y1),(x2,y2),...,(xn,yn)},yi∈{−1,+1}模型：h(x)=sign(wTx+b)\\begin{aligned} &amp;数据集：D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\},y_i\\in\\{-1,+1\\}\\\\ &amp;模型：h(x)=\\text{sign}(w^Tx+b) \\end{aligned} ​数据集：D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)},yi​∈{−1,+1}模型：h(x)=sign(wTx+b)​ 如果数据是二元可分的，感知机可以找到很多不同的超平面，但是哪个超平面是最好的呢？这就需要我们用SVM去寻找 SVM找到的是一个具有最大边距的超平面如下图所示，γ是超平面到两类点之间的最小距离，而我们要找到一个超平面使得γ最大，这也意味着该超平面在这两类数据点的中间，即SVM找到的超平面到两类点的最小距离是相等的。 二、线性支持向量机 2.1 计算边距γ\\gammaγ 假设当前的超平面为： H={x∣wTx+b=0}\\mathcal{H}=\\{x|w^Tx+b=0\\} H={x∣wTx+b=0} 任取超平面中的一个点xxx，则它到超平面的距离为：设xxx到超平面的距离为ddd，它在超平面上的投影为xPx^PxP 根据向量关系，则有：xP+d=x，xP∈H∵w为H的法向量 ∴w//d，d=αw∴xP=x−d=x−αw∴wTxP+b=wT(x−αw)+b=0∴α=wTx+bwTw，d=αw=dTd=∣α∣wTw=∣wTx+b∣wTw\\begin{aligned} &amp;根据向量关系，则有：x^P+d=x，x^P\\in \\mathcal{H}\\\\ &amp;\\because w为\\mathcal{H}的法向量~~~~\\therefore w//d，d=\\alpha w\\\\ &amp;\\therefore x^P=x-d=x-\\alpha w\\\\ &amp;\\therefore w^Tx^P+b=w^T(x-\\alpha w)+b=0\\\\ &amp;\\therefore \\alpha =\\frac{w^Tx+b}{w^Tw}，d=\\alpha w=\\sqrt{d^Td}=|\\alpha|\\sqrt{w^Tw}=\\frac{|w^Tx+b|}{\\sqrt{w^Tw}} \\end{aligned} ​根据向量关系，则有：xP+d=x，xP∈H∵w为H的法向量 ∴w//d，d=αw∴xP=x−d=x−αw∴wTxP+b=wT(x−αw)+b=0∴α=wTwwTx+b​，d=αw=dTd​=∣α∣wTw​=wTw​∣wTx+b∣​​ 由此我们得到： γ(w,b)=min⁡x∈D∣wTx+b∣wTw\\gamma(w,b)=\\min_{x\\in D}\\frac{|w^Tx+b|}{\\sqrt{w^Tw}} γ(w,b)=x∈Dmin​wTw​∣wTx+b∣​ 根据比例关系，我们可以进一步得简化计算过程： γ(w,b)=min⁡x∈D∣wTx+b∣wTw=min⁡x∈Dβ∣wTx+b∣βwTw=r(βw,βb),β=0\\begin{aligned} &amp;\\gamma(w,b)=\\min_{x\\in D}\\frac{|w^Tx+b|}{\\sqrt{w^Tw}}=\\min_{x\\in D}\\frac{\\beta|w^Tx+b|}{\\beta\\sqrt{w^Tw}}=r(\\beta w,\\beta b),\\beta\\not=0 \\end{aligned} ​γ(w,b)=x∈Dmin​wTw​∣wTx+b∣​=x∈Dmin​βwTw​β∣wTx+b∣​=r(βw,βb),β​=0​ 我们可以找到一个合适的β\\betaβ，对于使得γ(w,b)\\gamma(w,b)γ(w,b)取得最小值的xxx有： β∣wTx+b∣=1\\beta|w^Tx+b|=1 β∣wTx+b∣=1 我们对空间进行比例变换：w=βw，b=βbw=\\beta w，b=\\beta bw=βw，b=βb，则γ(w,b)\\gamma(w,b)γ(w,b)可表示为： γ(w,b)=1∣∣w∣∣2\\gamma(w,b)=\\frac{1}{||w||_2} γ(w,b)=∣∣w∣∣2​1​ 2.2 最大化边距γ\\gammaγ 我们可以将求解γ最大值的过程等效为一个约束化问题： 求解问题：(w,b)=argmaxw,b γ(w,b)约束条件：yi(wTxi+b)≥0\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmax}~\\gamma(w,b)\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 0 \\end{aligned} ​求解问题：(w,b)=w,bargmax​ γ(w,b)约束条件：yi​(wTxi​+b)≥0​ 结合我们上述的比例变换，则约束化问题等效为： 求解问题：(w,b)=argmaxw,b γ(w,b)=argmaxw,b 1∣∣w∣∣2=argminw,b wTw约束条件：min⁡x∈D∣wTx+b∣=1,yi(wTx+b)≥0\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmax}~\\gamma(w,b)=\\underset{w,b}{argmax}~\\frac{1}{||w||_2}=\\underset{w,b}{argmin}~w^Tw\\\\ &amp;约束条件：\\min_{x\\in D}|w^Tx+b|=1,y_i(w^Tx+b)\\ge 0 \\end{aligned} ​求解问题：(w,b)=w,bargmax​ γ(w,b)=w,bargmax​ ∣∣w∣∣2​1​=w,bargmin​ wTw约束条件：x∈Dmin​∣wTx+b∣=1,yi​(wTx+b)≥0​ 我们可以对约束条件继续进行整合： min⁡x∈D∣wTx+b∣=1→∣wTx+b∣≥1yi∈{−1,+1}→yi(wTxi+b)≥1\\begin{aligned} &amp;\\min_{x\\in D}|w^Tx+b|=1\\rightarrow |w^Tx+b|\\ge 1\\\\ &amp;y_i\\in\\{-1,+1\\}\\rightarrow y_i(w^Tx_i+b)\\ge 1 \\end{aligned} ​x∈Dmin​∣wTx+b∣=1→∣wTx+b∣≥1yi​∈{−1,+1}→yi​(wTxi​+b)≥1​ 最终，我们得到的约束化问题为： 求解问题：(w,b)=argminw,b wTw约束条件：yi(wTxi+b)≥1\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 1\\\\ \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw约束条件：yi​(wTxi​+b)≥1​ 在我们求得最终的w,bw,bw,b时，有yi(wTxi+b)=1y_i(w^Tx_i+b)=1yi​(wTxi​+b)=1 这是一个二次最优化问题，目标是二次的，约束是线性的，我们可以用任何QCQP（二次约束二次规划解算器）唯一且有效地求解它。 如上图所示，我们将离决策边界最近的几个点称为支持向量，支持向量机的最终模型仅仅与支持向量有关 三、软约束 3.1 噪声影响 如果存在噪声，可能使得数据线性不可分，如下图所示，这就导致我们无法找到一个最终的超平面，而感知机模型是通过限制最大训练轮数避免死循环的，支持向量机则是通过引入软约束处理噪声。 在能找到超平面的情况下也可能存在一些噪声点使得超平面的选取并不好，如下图所示，我们仍要尽量规避掉这类噪声。 3.2 软约束实现 对于上述的例子，我们可以将那些噪音点忽略掉，并将它们看作损失，统计进行忽略后不匹配的点的个数加到目标公式中，则有： 求解问题：(w,b)=argminw,b wTw+C∑i=1nI(yi=sign(wTxi+b))约束条件：正确分类的yi:yi(wTxi+b)≥1，不正确分类的yi(wTxi+b)≥∞\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^nI(y_i\\not=\\text{sign}(w^Tx_i+b))\\\\ &amp;约束条件：正确分类的y_i:y_i(w^Tx_i+b)\\ge 1，不正确分类的y_i(w^Tx_i+b)\\ge\\infty \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw+Ci=1∑n​I(yi​​=sign(wTxi​+b))约束条件：正确分类的yi​:yi​(wTxi​+b)≥1，不正确分类的yi​(wTxi​+b)≥∞​ 即：我们对分类错误的点不加以任何约束，CCC是对大边距和噪声点耐受性的权衡 但是上述计算公式是非线性且不连续的，为此我们引入一个线性松弛变量ξi\\xi_iξi​： 求解问题：(w,b)=argminw,b wTw+C∑i=1nξi约束条件：yi(wTxi+b)≥1−ξi\\begin{aligned} &amp;求解问题：(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\xi_i\\\\ &amp;约束条件：y_i(w^Tx_i+b)\\ge 1-\\xi_i \\end{aligned} ​求解问题：(w,b)=w,bargmin​ wTw+Ci=1∑n​ξi​约束条件：yi​(wTxi​+b)≥1−ξi​​ 松弛变量 ξi ~\\xi_i~ ξi​ 允许输入xix_ixi​更接近超平面（甚至位于错误的一侧），但这种“松弛”在目标函数中会受到惩罚。 CCC非常大时，SVMSVMSVM将变得非常严格，尝试将所有的点都分到正确的一侧去 CCC非常小时，SVMSVMSVM将变得非常松散，可能会牺牲一些简单的点来获得超平面 松弛变量 ξi ~\\xi_i~ ξi​ 的定义方式如下：我们可以将 ξi ~\\xi_i~ ξi​ 看成损失函数，SVMSVMSVM的建立要尽可能得最小化 ξi ~\\xi_i~ ξi​ ξi={1−yi(wTxi+b) , yi(wTxi+b)&lt;1 0 , yi(wTxi+b)≥1\\begin{aligned} &amp;\\xi_i=\\left\\{ \\begin{aligned} &amp;1-y_i(w^Tx_i+b)~,~y_i(w^Tx_i+b)&lt;1\\\\ &amp;~~~~~~~~~~~~0~~~~~~~~~~~~~~~~,~y_i(w^Tx_i+b)\\ge 1 \\end{aligned} \\right. \\end{aligned} ​ξi​={​1−yi​(wTxi​+b) , yi​(wTxi​+b)&lt;1 0 , yi​(wTxi​+b)≥1​​ 即：ξi=max⁡(1−yi(wTxi+b),0)\\xi_i=\\max(1-y_i(w^Tx_i+b),0)ξi​=max(1−yi​(wTxi​+b),0) 最终我们的软约束模型为： (w,b)=argminw,b wTw+C∑i=1nmax⁡(1−yi(wTxi+b),0)(w,b)=\\underset{w,b}{argmin}~w^Tw+C\\sum_{i=1}^n\\max(1-y_i(w^Tx_i+b),0) (w,b)=w,bargmin​ wTw+Ci=1∑n​max(1−yi​(wTxi​+b),0) 参数CCC对支持向量机最后拟合结果的影响如图所示： 四、算法实现 支持向量机的手动实现结合核函数最优，因此我们此处仅通过调库实现模型。 4.1 数据集 我们此处选择自己生成数据集，依靠的是sklearn的make_blobs函数，这是一个用于生成聚类数据的函数 我们生成的数据集分布的例子如下： 4.2 模型实现 我们实现的是线性支持向量机，不可以用核函数进行优化，主要用到的是sklearn的LinearSVC函数，它的函数原型如下： ①penalty：正则化项，可选择：&quot;l1&quot;,&quot;l2&quot; ②loss：损失函数，可选择：&quot;hinge&quot;（合页损失函数），&quot;squared_hinge&quot;（合页损失函数的平方） ③dual：求解对偶问题，当样本数量&gt;特征数量时倾向于解决原始问题 ④tol：中止迭代的阈值 ⑤C：软约束的惩罚系数 ⑥multi_class：多分类问题的策略，&quot;ovr&quot;（one-vs-rest分类策略），&quot;crammer_singer&quot;（多类联合分类） ⑦fit_intercept：是否考虑截距b ⑧max_iter：最大训练轮数 五、总结 ①支持向量机（SVM）是一种线性分类器，可以看作是感知器的扩展，支持向量机寻找最大边距分离超平面。 ②边距是从超平面到两个类中最近点的距离。 ③最大边距分类器是在所有数据点必须位于超平面的正确一侧的约束下，使边距最大化。 ④对于最优的w，b对，一些训练点会有严格的约束，我们将这些训练点称为支持向量，严格的约束为： yi(wTxi+b)=1y_i(w^Tx_i+b)=1 yi​(wTxi​+b)=1 ⑤支持向量机是凸二次函数，可以用QCQP求解器求解。 ⑥通过在目标中加入松弛变量，我们可以得到无约束SVM公式：软约束SVM。 ","link":"https://2006wzt.github.io/post/机器学习实战（十）：支持向量机/"},{"title":"机器学习实战（九）：线性回归","content":"线性回归 一、形式化定义 在统计学中，线性回归是一种线性方法，用于建模标量响应与一个或多个解释变量之间的关系。 当只有一个解释变量时，我们称之为简单线性回归： 数据假设：yi∈R模型假设：yi=xiTw+ϵiϵi∼N(0,σ2),yi∣xi∼N(xiTw,σ2)\\begin{aligned} &amp;数据假设：y_i\\in R\\\\ &amp;模型假设：y_i=x_i^Tw+\\epsilon_i\\\\ &amp;\\epsilon_i\\sim N(0,\\sigma^2),y_i|x_i\\sim N(x_i^Tw,\\sigma^2) \\end{aligned} ​数据假设：yi​∈R模型假设：yi​=xiT​w+ϵi​ϵi​∼N(0,σ2),yi​∣xi​∼N(xiT​w,σ2)​ 根据对分布的模型假设，我们最终得到的条件概率为： P(yi∣xi,w)=12πσ2e−(xiTw−yi)22σ2P(y_i|x_i,w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_i^Tw-y_i)^2}{2\\sigma^2}} P(yi​∣xi​,w)=2πσ2​1​e−2σ2(xiT​w−yi​)2​ 我们假设模型是一条过原点的直线（类似于感知机通过升维即可使得函数过原点）而对于每个特征xix_ixi​，它的标签yiy_iyi​则从一个平均值为wTxiw^Tx_iwTxi​，方差为σ2σ^2σ2的高斯分布中得出，我们在线性回归建模中的任务便是根据数据集估计斜率www。 单标量预测变量xxx和单标量响应变量yyy的最简单情况称为简单线性回归，而多个预测变量xxx参与的预测为多元线性回归，几乎所有现实世界的回归模型都涉及多个预测因子，线性回归的基本描述通常用多元回归模型来表述。 二、参数估计 线性回归模型中的重要参数就是“斜率”www，我们同样可以通过MLEMLEMLE和MAPMAPMAP两种方式对它进行估计。 进行下列推导之前，我们先了解几个常用的矩阵求导公式： ∂Ax∂x=AT,∂xTA∂x=A,∂ATxB∂x=ABT∂ATxTB∂x=BAT,∂ATxxTB∂x=(ABT+BAT)x,∂xTAx∂x=2Ax\\frac{\\partial Ax}{\\partial x}=A^T,\\frac{\\partial x^TA}{\\partial x}=A,\\frac{\\partial A^TxB}{\\partial x}=AB^T\\\\\\frac{\\partial A^Tx^TB}{\\partial x}=BA^T,\\frac{\\partial A^Txx^TB}{\\partial x}=(AB^T+BA^T)x,\\frac{\\partial x^TAx}{\\partial x}=2Ax ∂x∂Ax​=AT,∂x∂xTA​=A,∂x∂ATxB​=ABT∂x∂ATxTB​=BAT,∂x∂ATxxTB​=(ABT+BAT)x,∂x∂xTAx​=2Ax 再了解一些矩阵的相关性质： (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT(A^T)^T=A,(A+B)^T=A^T+B^T,(AB)^T=B^TA^T (AT)T=A,(A+B)T=AT+BT,(AB)T=BTAT 2.1 极大似然估计MLE w^MLE=argmaxw P(Y,X∣w)=argmaxw ∏i=1nP(yi,xi∣w) =argmaxw ∏i=1nP(yi∣xi,w)P(xi∣w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡12πσ2e−(wTxi−yi)22σ2 =argminw ∑i=1n(xiTw−yi)2=argminw 1n∑i=1n(xiTw−yi)2\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y,X|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)P(x_i|w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(w^Tx_i-y_i)^2}{2\\sigma^2}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2=\\underset{w}{argmin}~\\frac1n\\sum_{i=1}^n(x_i^Tw-y_i)^2 \\end{aligned} ​w^MLE​=wargmax​ P(Y,X∣w)=wargmax​ i=1∏n​P(yi​,xi​∣w) =wargmax​ i=1∏n​P(yi​∣xi​,w)P(xi​∣w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log2πσ2​1​e−2σ2(wTxi​−yi​)2​ =wargmin​ i=1∑n​(xiT​w−yi​)2=wargmin​ n1​i=1∑n​(xiT​w−yi​)2​ 我们可以发现，线性回归问题的极大似然估计结果与最小二乘法（OLS）的形式完全相同。 此外我们还可以推导出www的闭合形式解，这样会使得计算更加方便：根据上述估计值，我们可以定义损失函数l(w)l(w)l(w) l(w)=(Xw−Y)2\\begin{aligned} &amp;l(w)=(Xw-Y)^2 \\end{aligned} ​l(w)=(Xw−Y)2​ 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www，XXX为n×dn\\times dn×d维矩阵，www为d×1d\\times 1d×1维矩阵，YYY为n×1n\\times1n×1维矩阵： l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂l(w)∂w=2XTXw−2XTY=0则有：w=(XTX)−1XTY\\begin{aligned} &amp;l(w)=(Xw-Y)^T(Xw-Y)=(w^TX^T-Y^T)(Xw-Y)\\\\ &amp;~~~~~~~=w^TX^TXw-w^TX^TY-Y^TXw-Y^TY\\\\ &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY=0\\\\ &amp;则有：w=(X^TX)^{-1}X^TY \\end{aligned} ​l(w)=(Xw−Y)T(Xw−Y)=(wTXT−YT)(Xw−Y) =wTXTXw−wTXTY−YTXw−YTY∂w∂l(w)​=2XTXw−2XTY=0则有：w=(XTX)−1XTY​ 2.2 极大后验估计MAP 引入一个附加的模型假设： P(w)=12πτ2e−wTw2τ2P(w)=\\frac1{\\sqrt{2\\pi\\tau^2}}e^{-\\frac{w^Tw}{2\\tau^2}} P(w)=2πτ2​1​e−2τ2wTw​ 则极大后验估计的过程为： w^MAP=argmaxw P(w∣Y,X)=argmaxw P(Y,X∣w)P(w)P(Y,X) =argmaxw P(Y,X∣w)P(w)=argmaxw ∏i=1nP(yi,xi∣w)P(w) =argmaxw ∑i=1nlog⁡P(yi,xi∣w)+log⁡P(w) =argmaxw −∑i=1n(xiTw−yi)2−12τ2wTw =argminw ∑i=1n(xiTw−yi)2+λ∣∣w∣∣22\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|Y,X)=\\underset{w}{argmax}~\\frac{P(Y,X|w)P(w)}{P(Y,X)}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~P(Y,X|w)P(w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i,x_i|w)P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i,x_i|w)+\\log P(w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\sum_{i=1}^n(x_i^Tw-y_i)^2-\\frac{1}{2\\tau^2}w^Tw\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmin}~\\sum_{i=1}^n(x_i^Tw-y_i)^2+\\lambda||w||_2^2 \\end{aligned} ​w^MAP​=wargmax​ P(w∣Y,X)=wargmax​ P(Y,X)P(Y,X∣w)P(w)​ =wargmax​ P(Y,X∣w)P(w)=wargmax​ i=1∏n​P(yi​,xi​∣w)P(w) =wargmax​ i=1∑n​logP(yi​,xi​∣w)+logP(w) =wargmax​ −i=1∑n​(xiT​w−yi​)2−2τ21​wTw =wargmin​ i=1∑n​(xiT​w−yi​)2+λ∣∣w∣∣22​​ 我们将这一回归称为岭回归，它同样存在闭合形式：我们定义损失函数为： l(w)=(Xw−Y)2+λwTwl(w)=(Xw-Y)^2+\\lambda w^Tw l(w)=(Xw−Y)2+λwTw 我们要求解l(w)′=0l(w)&#x27;=0l(w)′=0时所对应的www： ∂l(w)∂w=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=2X^TXw-2X^TY+2\\lambda Iw=0\\\\ &amp;则有：w=(X^TX+\\lambda I)^{-1}X^TY \\end{aligned} ​∂w∂l(w)​=2XTXw−2XTY+2λIw=0则有：w=(XTX+λI)−1XTY​ 三、模型实现 对于简单的线性回归模型，我们通过闭合形式可以直接求解，将分别通过手动实现和调库的方式构造模型。 3.1 数据集 我们使用波士顿房价数据集，查看数据集的内容并对其进行存储依靠以下部分代码： 波士顿房价数据集有506个样本，每个样本有13个特征，接下来我们利用线性回归模型对数据集进行拟合。 3.2 手动实现模型 我们的评估指标选择r2r2r2系数，其定义如下： R2=1−∑i=1n(yi−yi^)2∑i=1n(yi−y‾)2=1−MSEVarR^2=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y_i})^2}{\\sum_{i=1}^n(y_i-\\overline{y})^2}=1-\\frac{MSE}{Var} R2=1−∑i=1n​(yi​−y​)2∑i=1n​(yi​−yi​^​)2​=1−VarMSE​ 显然，预测越准r2r2r2系数越趋近于1。 3.3 调库实现模型 调库主要用到的是sklearn中的LinearRegression模型，其具体参数请查阅文档 ","link":"https://2006wzt.github.io/post/机器学习实战（九）：线性回归/"},{"title":"机器学习实战（八）：梯度下降","content":"梯度下降 一、基本思想 1.1 知识回顾 我们首先回顾逻辑回归模型的参数估计： 对于给定的数据集D={(x1,y1),(x2,y2),...,(xn,yn)},xi∈Rd,yi∈{0,1}w^MLE=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))w^MAP=argminw∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;对于给定的数据集D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\},x_i\\in R^d,y_i\\in\\{0,1\\}\\\\ &amp;\\hat{w}_{MLE}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;\\hat{w}_{MAP}=\\underset{w}{argmin}\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​对于给定的数据集D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)},xi​∈Rd,yi​∈{0,1}w^MLE​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))w^MAP​=wargmin​i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 由此我们得到两个估计过程的损失函数： l(w)MLE=∑i=1nlog⁡(1+e−yi(wTxi+b))l(w)MAP=∑i=1nlog⁡(1+e−yi(wTxi+b))+λwTw\\begin{aligned} &amp;l(w)_{MLE}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})\\\\ &amp;l(w)_{MAP}=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i+b)})+\\lambda w^Tw \\end{aligned} ​l(w)MLE​=i=1∑n​log(1+e−yi​(wTxi​+b))l(w)MAP​=i=1∑n​log(1+e−yi​(wTxi​+b))+λwTw​ 我们的目标是最小化一个凸的、连续的、可微的损失函数l(w)l(w)l(w)。 凸函数定义为： 对∀t∈[0,1],若f(x)满足:f(tx1+(1−t)x2)≤tf(x1)+(1−t)f(x2),则称f(x)为凸函数对\\forall t\\in[0,1],若f(x)满足:f(tx_1+(1-t)x_2)\\le tf(x_1)+(1-t)f(x_2),则称f(x)为凸函数 对∀t∈[0,1],若f(x)满足:f(tx1​+(1−t)x2​)≤tf(x1​)+(1−t)f(x2​),则称f(x)为凸函数 ①l(w)l(w)l(w)是凸函数：这使得我们找到的局部最小值也是全局最小值 ②l(w)l(w)l(w)至少三次连续可微：我们将广泛得使用泰勒近似，这个假设大大简化了讨论 ③在www上没有设置任何约束，添加约束会增加问题的复杂程度，我们在这里将不讨论 1.2 局部最小值 1.2.1 定义 定义：我们将w∗w^*w∗称为l(w)l(w)l(w)的一个局部最小值，如果有： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)≤l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)\\le l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)≤l(w) LPLPLP范数：∣∣x∣∣p=(∑i=1n∣xi∣p)1p||x||_p=(\\sum_{i=1}^n |x_i|^p)^{\\frac1p}∣∣x∣∣p​=(∑i=1n​∣xi​∣p)p1​ 因为我们之前有l(w)l(w)l(w)是凸函数的假设，所以找到的局部最小值w∗w^*w∗也是全局最小值，有： ∀w∈Rd,都有l(w∗)≤l(w)\\forall w\\in R^d,都有l(w^*)\\le l(w) ∀w∈Rd,都有l(w∗)≤l(w) 当满足如下条件时，我们称w∗w^*w∗为l(w)l(w)l(w)的一个严格的局部最小值： ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2&lt;ϵ},都有l(w∗)&lt;l(w)\\exist \\epsilon&gt;0,使得\\forall w\\in\\{w:||w-w^*||_2&lt;\\epsilon\\},都有l(w^*)&lt; l(w) ∃ϵ&gt;0,使得∀w∈{w:∣∣w−w∗∣∣2​&lt;ϵ},都有l(w∗)&lt;l(w) 注意不是所有的凸函数都有严格的局部最小值，比如l(w)=1l(w)=1l(w)=1 1.2.2 必要条件 一个点w∗w^*w∗成为局部最小值的必要条件为： ∇l(w∗)=0→\\nabla l(w^*)=\\overrightarrow0 ∇l(w∗)=0 ∇为梯度算子，∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]T\\nabla为梯度算子，\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T ∇为梯度算子，∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 如果点w∗w^*w∗满足上述必要条件，则该点成为严格的局部最小值的充分条件是HessianHessianHessian矩阵为正定的： 即：∇2l(w∗)=H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]是正定的。即：\\nabla^2l(w^*)=H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right]是正定的。 即：∇2l(w∗)=H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​是正定的。 正定矩阵的定义为： 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的设A是n阶方阵，即A\\in R^{n\\times n},如果对于任何非零向量X\\in R^n都有：X^TA~X&gt;0,那么A是正定的 设A是n阶方阵，即A∈Rn×n,如果对于任何非零向量X∈Rn都有：XTA X&gt;0,那么A是正定的 二、泰勒展开 为了理解更新过程中的损失函数变化，我们对损失函数进行泰勒展开： 2.1 一阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2,...,∂l(w)∂wd]Tl(w+\\Delta w)=l(w)+g(w)^T\\Delta w\\\\ g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2},...,\\frac{\\partial l(w)}{\\partial w_d}]^T l(w+Δw)=l(w)+g(w)TΔwg(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​,...,∂wd​∂l(w)​]T 一阶泰勒展开对应于线性近似： 在Δw\\Delta wΔw很小时，这些近似是合理可靠的，线性近似的误差为： O(∣∣Δw∣∣22)=∑i=1dΔwi2O(||\\Delta w||_2^2)=\\sum_{i=1}^d\\Delta w_i^2 O(∣∣Δw∣∣22​)=i=1∑d​Δwi2​ 2.2 二阶泰勒展开 l(w+Δw)=l(w)+g(w)TΔw+12ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j=∂2l(w)∂wi∂wjl(w+\\Delta w)=l(w)+g(w)^T\\Delta w+\\frac12\\Delta w^TH(w)\\Delta w\\\\ H(w)为Hessian矩阵，[H(w)]_{i,j}=\\frac{\\partial^2 l(w)}{\\partial w_i\\partial w_j} l(w+Δw)=l(w)+g(w)TΔw+21​ΔwTH(w)ΔwH(w)为Hessian矩阵，[H(w)]i,j​=∂wi​∂wj​∂2l(w)​ 二阶泰勒展开对应于二次近似： 二次近似的误差为： O(∣∣Δw∣∣23)=(∑i=1nΔwi2)32O(||\\Delta w||_2^3)=(\\sum_{i=1}^n\\Delta w_i^2)^\\frac32 O(∣∣Δw∣∣23​)=(i=1∑n​Δwi2​)23​ 三、搜索方向方法 我们求解问题的目标是：寻找w∗w^*w∗使得l(w)l(w)l(w)取得最小值，我们需要研究寻找w∗w^*w∗的方向。 3.1 核心思想 给定一个初始点w0w^0w0，寻找一个迭代序列：w0→w1→...→wkw^0\\rightarrow w^1\\rightarrow...\\rightarrow w^kw0→w1→...→wk，wkw^kwk表示第kkk次迭代找到的www，我们希望：k→∞,wk→w∗k\\rightarrow\\infty,w^k\\rightarrow w^*k→∞,wk→w∗ 我们的目的就是寻找一个步长sss用于对www进行更新，更新过程： wk+1=wk+sw^{k+1}=w^k+s wk+1=wk+s 由此我们得到了一个梯度下降过程的伪代码： 根据上述算法，我们便需要解决以下两个问题： ①如何寻找一个合理的步长sss ②如何确定我们找到了w∗w^*w∗以跳出循环 3.2 梯度下降 我们需要寻找到一个使得函数梯度下降最快的方向，并朝该方向迈出一步。 考虑一阶泰勒展开： l(wk+s)=l(wk)+g(wk)Tsl(w^k+s)=l(w^k)+g(w^k)^Ts l(wk+s)=l(wk)+g(wk)Ts 那么下降最快的方向就是−g(wk)-g(w^k)−g(wk)，因此我们在梯度下降过程中所作的就是： s=−αg(wk),α&gt;0s=-\\alpha g(w^k),\\alpha&gt;0 s=−αg(wk),α&gt;0 正确性验证：我们需要验证的是总有一些足够小的ααα使得： l(wk−αg(wk))&lt;l(wk)l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) l(wk−αg(wk))&lt;l(wk) 根据泰勒展开： l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)\\begin{aligned} &amp;l\\big(w^k-\\alpha g(w^k)\\big)=l(w^k)-\\alpha g(w^k)^Tg(w^k)+O(\\alpha^2)\\\\ &amp;\\because g(w^k)^Tg(w^k)&gt;0,\\alpha\\rightarrow0 过程中\\alpha^2\\rightarrow0的速度大于\\alpha\\\\ &amp;\\therefore 存在足够小的\\alpha，使得：l\\big(w^k-\\alpha g(w^k)\\big)&lt;l(w^k) \\end{aligned} ​l(wk−αg(wk))=l(wk)−αg(wk)Tg(wk)+O(α2)∵g(wk)Tg(wk)&gt;0,α→0过程中α2→0的速度大于α∴存在足够小的α，使得：l(wk−αg(wk))&lt;l(wk)​ ααα的设定需要合理，太小的话会导致收敛过慢，太大的话会导致发散。 3.3 Adagrad算法 对于ααα的一个选择是：设置ααα使得步长适合于所有的特征，Adagrad算法通过保持每个优化变量的平方梯度的运行平均值来实现这一点 Adagrad算法为具有大梯度的变量设置小学习率，并为具有小梯度的特征设置大学习率。 Adagrad算法的伪代码如下： 该算法的特点是：每一个维度都有不同的学习率，第jjj维的学习率维： αzj+ϵ\\frac{\\alpha}{\\sqrt{z_j+\\epsilon}} zj​+ϵ​α​ 3.4 牛顿方法 牛顿方法处理梯度下降利用二阶泰勒展开： l(wk+s)=l(wk)+g(wk)Ts+12sTH(wk)sl(w^k+s)=l(w^k)+g(w^k)^Ts+\\frac12s^TH(w^k)s l(wk+s)=l(wk)+g(wk)Ts+21​sTH(wk)s H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2 ... ∂2l(w)∂w1∂wd ... ... ... ...∂2l(w)∂wd∂w1 ∂2l(w)∂wd∂w2 ... ∂2l(w)∂wd2]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_d}\\\\ &amp;~~~~...~~~~~~~~~~...~~~~~~~~...~~~~~~~...\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_1}~~\\frac{\\partial^2l(w)}{\\partial w_d\\partial w_2}~~...~~\\frac{\\partial^2l(w)}{\\partial w_d^2}\\\\ \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​ ... ∂w1​∂wd​∂2l(w)​ ... ... ... ...∂wd​∂w1​∂2l(w)​ ∂wd​∂w2​∂2l(w)​ ... ∂wd2​∂2l(w)​​⎦⎥⎥⎥⎥⎥⎤​ 因为我们假设l(w)l(w)l(w)是凸函数，则H(w)H(w)H(w)对所有www都是半正定的，则有： sTH(wk)s≥0s^TH(w^k)s\\ge0 sTH(wk)s≥0 事实上，牛顿方法在严格的局部极小值附近有很好的性质，一旦足够接近一个解，它就会迅速收敛，为了便于分析，我们假设H(w)H(w)H(w)是正定的，即： sTH(wk)s&gt;0s^TH(w^k)s&gt;0 sTH(wk)s&gt;0 二次近似的梯度为： ∂l(wk+s)∂s=g(wk)+H(wk)s\\frac{\\partial l(w^k+s)}{\\partial s}=g(w^k)+H(w^k)s ∂s∂l(wk+s)​=g(wk)+H(wk)s 这意味着我们要找到的w∗w^*w∗满足： g(w∗)+H(w∗)s=0g(w^*)+H(w^*)s=0 g(w∗)+H(w∗)s=0 所以我们可以得到步长sss： s=H(wk)−1g(wk)s=H(w^k)^{-1}g(w^k) s=H(wk)−1g(wk) 3.4.1 Example 这有一个简单的例子清楚地说明了利用二阶泰勒展开的牛顿方法的优势： 假设损失函数实际上是严格的凸二次函数，即： l(w)=12wTAw+bTw+cl(w)=\\frac12w^TAw+b^Tw+c l(w)=21​wTAw+bTw+c 其中AAA为正定矩阵，bbb为任意向量，ccc为任意常数，在这种情况下，牛顿方法仅需一步即可收敛： l(w)在Aw+b=0时收敛l(w)在Aw+b=0时收敛 l(w)在Aw+b=0时收敛 我们不妨以二维向量为例： A=[a11 a12a21 a22],w=[w1,w2]T,b=[b1,b2]TA=\\left[ \\begin{aligned} &amp;a_{11}~~~~a_{12}\\\\ &amp;a_{21}~~~~a_{22}\\\\ \\end{aligned} \\right],w=[w_1,w_2]^T,b=[b_1,b_2]^T A=[​a11​ a12​a21​ a22​​],w=[w1​,w2​]T,b=[b1​,b2​]T 则损失函数则变为： l(w1,w2)=12(a11w12+(a12+a21)w1w2+a22w22)+(b1w1+b2w2)+cl(w_1,w_2)=\\frac12\\big(a_{11}w_1^2+(a_{12}+a_{21})w_1w_2+a_{22}w_2^2\\big)+(b_1w_1+b_2w_2)+c l(w1​,w2​)=21​(a11​w12​+(a12​+a21​)w1​w2​+a22​w22​)+(b1​w1​+b2​w2​)+c g(w)=∇l(w)=[∂l(w)∂w1,∂l(w)∂w2]=[a11w1+(a12+a21)w2+b1,a22w2+(a12+a21)w1+b2]\\begin{aligned} &amp;g(w)=\\nabla l(w)=[\\frac{\\partial l(w)}{\\partial w_1},\\frac{\\partial l(w)}{\\partial w_2}]=[a_{11}w_1+(a_{12}+a_{21})w_2+b_1,a_{22}w_2+(a_{12}+a_{21})w_1+b_2] \\end{aligned} ​g(w)=∇l(w)=[∂w1​∂l(w)​,∂w2​∂l(w)​]=[a11​w1​+(a12​+a21​)w2​+b1​,a22​w2​+(a12​+a21​)w1​+b2​]​ H(w)=[∂2l(w)∂w12 ∂2l(w)∂w1∂w2∂2l(w)∂w2∂w1 ∂2l(w)∂w22]=[ a11 a12+a21a12+a21 a22]H(w)=\\left[ \\begin{aligned} &amp;\\frac{\\partial^2l(w)}{\\partial w_1^2}~~~~\\frac{\\partial^2l(w)}{\\partial w_1\\partial w_2}\\\\ &amp;\\frac{\\partial^2l(w)}{\\partial w_2\\partial w_1}~~~~\\frac{\\partial^2l(w)}{\\partial w_2^2} \\end{aligned} \\right]=\\left[ \\begin{aligned} &amp;~~~~a_{11}~~~~a_{12}+a_{21}\\\\ &amp;a_{12}+a_{21}~~~~a_{22} \\end{aligned} \\right] H(w)=⎣⎢⎢⎢⎡​​∂w12​∂2l(w)​ ∂w1​∂w2​∂2l(w)​∂w2​∂w1​∂2l(w)​ ∂w22​∂2l(w)​​⎦⎥⎥⎥⎤​=[​ a11​ a12​+a21​a12​+a21​ a22​​] H(w)−1=[a22a11a22−(a12+a21)2 −(a12+a21)a11a22−(a12+a21)2−(a12+a21)a11a22−(a12+a21)2 a11a11a22−(a12+a21)2]=β[ a22 −(a12+a21)−(a12+a21) a11],β=1a11a22−(a12+a21)2H(w)^{-1}=\\left[ \\begin{aligned} &amp;\\frac{a_{22}}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{-(a_{12}+a_{21})}{a_{11}a_{22}-(a_{12}+a_{21})^2}~~~~\\frac{a_{11}}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right]=\\beta\\left[ \\begin{aligned} &amp;~~~~a_{22}~~~~-(a_{12}+a_{21})\\\\ &amp;-(a_{12}+a_{21})~~~~a_{11} \\end{aligned} \\right],\\beta=\\frac1{a_{11}a_{22}-(a_{12}+a_{21})^2} H(w)−1=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2a22​​ a11​a22​−(a12​+a21​)2−(a12​+a21​)​a11​a22​−(a12​+a21​)2−(a12​+a21​)​ a11​a22​−(a12​+a21​)2a11​​​⎦⎥⎥⎥⎤​=β[​ a22​ −(a12​+a21​)−(a12​+a21​) a11​​],β=a11​a22​−(a12​+a21​)21​ s=−H(w)−1g(w)=−β[(a11a22−(a12+a21)2)w1+a22b1−(a12+a21)b2(a11a22−(a12+a21)2)w2+a11b2−(a12+a21)b1]=[−w1+(a12+a21)b2−a22b1a11a22−(a12+a21)2−w2+(a12+a21)b1−a11b2a11a22−(a12+a21)2]s=-H(w)^{-1}g(w)=-\\beta\\left[ \\begin{aligned} &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_1+a_{22}b_1-(a_{12}+a_{21})b_2\\\\ &amp;(a_{11}a_{22}-(a_{12}+a_{21})^2)w_2+a_{11}b_2-(a_{12}+a_{21})b_1 \\end{aligned} \\right]\\\\ =\\left[ \\begin{aligned} &amp;-w_1+\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;-w_2+\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] s=−H(w)−1g(w)=−β[​(a11​a22​−(a12​+a21​)2)w1​+a22​b1​−(a12​+a21​)b2​(a11​a22​−(a12​+a21​)2)w2​+a11​b2​−(a12​+a21​)b1​​]=⎣⎢⎢⎢⎡​​−w1​+a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​−w2​+a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 则更新www的过程为： w=w+s=[(a12+a21)b2−a22b1a11a22−(a12+a21)2(a12+a21)b1−a11b2a11a22−(a12+a21)2]w=w+s=\\left[ \\begin{aligned} &amp;\\frac{(a_{12}+a_{21})b_2-a_{22}b_1}{a_{11}a_{22}-(a_{12}+a_{21})^2}\\\\ &amp;\\frac{(a_{12}+a_{21})b_1-a_{11}b_2}{a_{11}a_{22}-(a_{12}+a_{21})^2} \\end{aligned} \\right] w=w+s=⎣⎢⎢⎢⎡​​a11​a22​−(a12​+a21​)2(a12​+a21​)b2​−a22​b1​​a11​a22​−(a12​+a21​)2(a12​+a21​)b1​−a11​b2​​​⎦⎥⎥⎥⎤​ 为了便于讨论，我们不妨使得A=IA=IA=I，则有w=[−b1,−b2]T=−bw=[-b_1,-b_2]^T=-bw=[−b1​,−b2​]T=−b，则有Aw+b=0Aw+b=0Aw+b=0，仅通过一次更新即实现了收敛。 注意： ①H(w)H(w)H(w)是一个d×dd\\times dd×d的矩阵，它的构造代价很大，一个很好的近似方法是只计算其对角线条目 ②本质上这是梯度下降法和牛顿方法的结合，为了避免牛顿法的发散，一个好的方法是从梯度下降（甚至随机梯度下降）开始，然后完成优化牛顿法。通常，牛顿法使用的二阶近似值更可能接近最佳值 ","link":"https://2006wzt.github.io/post/机器学习实战（八）：梯度下降/"},{"title":"机器学习实战（七）：逻辑回归","content":"逻辑回归 一、基本思想 逻辑回归是一种用于分类任务的经典机器学习算法。 我们之前介绍过机器学习算法大致可以分为两类： ①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)\\begin{aligned} &amp;①生成学习：估计P(x,y)=P(x|y)P(y)\\\\ &amp;②判别学习：估计P(y|x) \\end{aligned} ​①生成学习：估计P(x,y)=P(x∣y)P(y)②判别学习：估计P(y∣x)​ 上节学习的朴素贝叶斯属于生成学习算法，而逻辑回归则属于判别学习算法，它与高斯朴素贝叶斯相对应，即判别式的高斯朴素贝叶斯。 逻辑回归的核心函数为：sigmoidsigmoidsigmoid函数，也成为激活函数： sigmoid(x)=σ(x)=11+e−xsigmoid(x)=\\sigma(x)=\\frac{1}{1+e^{-x}} sigmoid(x)=σ(x)=1+e−x1​ 我们对逻辑回归模型的建模如下： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 与感知机原理相同，我们利用升维将偏差项bbb处理到www中，则模型简化为： P(y∣x)=11+e−y(wTx)P(y|x)=\\frac{1}{1+e^{-y(w^Tx)}} P(y∣x)=1+e−y(wTx)1​ 二、逻辑回归的参数 在逻辑回归模型中，重要的参数也为www，因此我们需要用到之前学过的几个概率估计方法对其进行估计。 2.1 极大似然估计（MLE） 利用极大似然估计逻辑回归的参数：在MLEMLEMLE中，要极大化的条件数据为P(Y∣X,w)P(Y|X,w)P(Y∣X,w)，X,YX,YX,Y为训练数据，我们要找到一个合适的www使得特征向量集为XXX时，观察到标签YYY的概率最大 X：d×n维矩阵，即X=[x1→,x2→,...,xn→]∈Rd×nY：n维向量，即Y=[y1,y2,...,yn]\\begin{aligned} &amp;X：d\\times n维矩阵，即X=[\\overrightarrow {x_1},\\overrightarrow{x_2},...,\\overrightarrow{x_n}]\\in R^{d\\times n}\\\\ &amp;Y：n维向量，即Y=[y_1,y_2,...,y_n] \\end{aligned} ​X：d×n维矩阵，即X=[x1​​,x2​​,...,xn​​]∈Rd×nY：n维向量，即Y=[y1​,y2​,...,yn​]​ MLEMLEMLE的假设为： P(Y∣X,w)=∏i=1nP(yi∣xi,w)P(Y|X,w)=\\prod_{i=1}^nP(y_i|x_i,w) P(Y∣X,w)=i=1∏n​P(yi​∣xi​,w) 由此，我们对参数www进行的极大似然估计过程为： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∑i=1nlog⁡P(yi∣xi,w)=argmaxw ∑i=1nlog⁡11+e−yi(wTxi) =argmaxw −log⁡(1+e−yi(wTxi))=argminw log⁡(1+e−yi(wTxi))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~\\sum_{i=1}^n\\log P(y_i|x_i,w)=\\underset{w}{argmax}~\\sum_{i=1}^n\\log \\frac{1}{1+e^{-y_i(w^Tx_i)}}\\\\ &amp;~~~~~~~~~~~=\\underset{w}{argmax}~-\\log(1+e^{-y_i(w^Tx_i)})=\\underset{w}{argmin}~\\log(1+e^{-y_i(w^Tx_i)}) \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∑n​logP(yi​∣xi​,w)=wargmax​ i=1∑n​log1+e−yi​(wTxi​)1​ =wargmax​ −log(1+e−yi​(wTxi​))=wargmin​ log(1+e−yi​(wTxi​))​ 为了求解www，我们引入函数l(w)l(w)l(w)：在负数域上求解−l(w)-l(w)−l(w)的最大值： l(w)=∑i=1nlog⁡(1+e−yi(wTxi))l(w)=\\sum_{i=1}^n\\log(1+e^{-y_i(w^Tx_i)}) l(w)=i=1∑n​log(1+e−yi​(wTxi​)) 我们要寻找max⁡{∣−l(w)∣}\\max\\{|-l(w)|\\}max{∣−l(w)∣}所对应的www。 2.1.1 1-D Example 我们考虑一个1维问题，如下图所示：+++表示正类，ooo表示负类： 如上图所示，x0x_0x0​恒为1，数据点特征的不同仅为x1x_1x1​的不同，本质上是一个1维的问题： x=[x0,x1],w=[w0,w1],l(w)=l([w0,w1])\\begin{aligned} &amp;x=[x_0,x_1],w=[w_0,w_1],l(w)=l([w_0,w_1]) \\end{aligned} ​x=[x0​,x1​],w=[w0​,w1​],l(w)=l([w0​,w1​])​ 右图为l(w)l(w)l(w)所对应的曲面，我们可以确定：w0=1,w1=0.7w_0=1,w_1=0.7w0​=1,w1​=0.7时，∣−l(w)∣|-l(w)|∣−l(w)∣取得最大值，因此我们计算出w=[1,0.7]w=[1,0.7]w=[1,0.7] 我们也可以用热力图更直观得得到www的最佳取值： 以下是部分样例对上述图像的贡献图： 2.1.2 2-D Example 我们考虑一个2维问题，如下图所示：+++表示正类，ooo表示负类： 根据热力图，我们得到∣−l(w)∣|-l(w)|∣−l(w)∣在w=[−0.81.0.81]w=[-0.81.0.81]w=[−0.81.0.81]时取得最大值。 MLEMLEMLE计算结果如下所示，其中红色表示正类别的概率很高。黑线表示MLEMLEMLE学习的决策边界。 决策边界：P(y=−1∣x)=P(y=+1∣x)即：11+ewTx=11+e−wTx→wTx=−wTx→wTx=0w=[w1,w2]=[−0.81,0.81],x=[x1,x2]wTx=w1⋅x1+w2⋅x2=−0.81x1+0.81x2=0→x1=x2\\begin{aligned} &amp;决策边界：P(y=-1|x)=P(y=+1|x)\\\\ &amp;即：\\frac{1}{1+e^{w^Tx}}=\\frac{1}{1+e^{-w^Tx}}\\rightarrow w^Tx=-w^Tx\\rightarrow w^Tx=0\\\\ &amp;w=[w_1,w_2]=[-0.81,0.81],x=[x_1,x_2]\\\\ &amp;w^Tx=w_1\\cdot x_1+w_2\\cdot x_2=-0.81x_1+0.81x_2=0\\rightarrow x_1=x_2 \\end{aligned} ​决策边界：P(y=−1∣x)=P(y=+1∣x)即：1+ewTx1​=1+e−wTx1​→wTx=−wTx→wTx=0w=[w1​,w2​]=[−0.81,0.81],x=[x1​,x2​]wTx=w1​⋅x1​+w2​⋅x2​=−0.81x1​+0.81x2​=0→x1​=x2​​ 2.2 极大后验估计（MAP） 在极大后验估计中，我们将www视为一个随机变量，并且可以预先指定它的先验分布。我们先预先指定www的先验分布： w∼N(0,σ2)P(w)=12πσ2e−wTw2σ2w\\sim \\mathcal{N}(0,\\sigma^2)\\\\ P(w)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{w^Tw}{2\\sigma^2}} w∼N(0,σ2)P(w)=2πσ2​1​e−2σ2wTw​ 这是逻辑回归的高斯近似。 以下计算过程中我们要用到链式法则： P(A,B∣C)=P(A∣B,C)P(B∣C)P(A,B|C)=P(A|B,C)P(B|C) P(A,B∣C)=P(A∣B,C)P(B∣C) 同时需要注意XXX与www是无关的。 我们在MAPMAPMAP中的目标是找到给定数据的最可能的模型参数，即使后验值最大化的参数： w^MAP=argmaxw P(w∣D)=argmaxw P(D∣w)P(w)P(D) =argmaxw P(D∣w)P(w)=argmaxw P(Y,X∣w)P(w) =argmaxw P(Y∣X,w)P(X∣w)P(w)=argmaxw P(Y∣X,w)P(w) =argmaxw log⁡P(Y∣X,w)+log⁡P(w) =argmaxw (−∑i=1nlog⁡(1+e−yi(wTxi))+log⁡12πσ2−12σ2wTw) =argminw (∑i=1nlog⁡(1+e−yi(wTxi))+λwTw)\\begin{aligned} &amp;\\hat{w}_{MAP}=\\underset{w}{argmax}~P(w|D)=\\underset{w}{argmax}~\\frac{P(D|w)P(w)}{P(D)}\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(D|w)P(w)=\\underset{w}{argmax}~P(Y,X|w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~P(Y|X,w)P(X|w)P(w)=\\underset{w}{argmax}~P(Y|X,w)P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\log P(Y|X,w)+\\log P(w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\big(-\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}-\\frac1{2\\sigma^2}w^Tw\\big)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmin}~\\big(\\sum_{i=1}^n\\log (1+e^{-y_i(w^Tx_i)})+\\lambda w^Tw\\big) \\end{aligned} ​w^MAP​=wargmax​ P(w∣D)=wargmax​ P(D)P(D∣w)P(w)​ =wargmax​ P(D∣w)P(w)=wargmax​ P(Y,X∣w)P(w) =wargmax​ P(Y∣X,w)P(X∣w)P(w)=wargmax​ P(Y∣X,w)P(w) =wargmax​ logP(Y∣X,w)+logP(w) =wargmax​ (−i=1∑n​log(1+e−yi​(wTxi​))+log2πσ2​1​−2σ21​wTw) =wargmin​ (i=1∑n​log(1+e−yi​(wTxi​))+λwTw)​ 其中：λ=12σ2\\lambda=\\frac1{2\\sigma^2}λ=2σ21​，与MLEMLEMLE同理，我们引入函数l(w)l(w)l(w)，在负数域上求解∣−l(w)∣|-l(w)|∣−l(w)∣最大值以得到www： l(w)=∑i=1nlog⁡(1+e−y1(wTxi))+λwTwl(w)=\\sum_{i=1}^n\\log(1+e^{-y_1(w^Tx_i)})+\\lambda w^Tw l(w)=i=1∑n​log(1+e−y1​(wTxi​))+λwTw 三、更新参数 除了上述利用函数求极值的方法求解参数www的方法，我们也可以用梯度下降的方式更新www 3.1 损失函数 根据模型预测的概率为： P(y=1∣x)=h(x)=σ(wTx)=11+e−(wTx) (1)P(y=0∣x)=1−h(x) (2)\\begin{aligned} &amp;P(y=1|x)=h(x)=\\sigma(w^Tx)=\\frac{1}{1+e^{-(w^Tx)}}~~~~~~~~(1)\\\\ &amp;P(y=0|x)=1-h(x)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(2) \\end{aligned} ​P(y=1∣x)=h(x)=σ(wTx)=1+e−(wTx)1​ (1)P(y=0∣x)=1−h(x) (2)​ 合并(1)(1)(1)式(2)(2)(2)式得： P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}\\begin{aligned} &amp;P(y|x,w)=h(x)^y(1-h(x))^{1-y},y\\in\\{0,1\\} \\end{aligned} ​P(y∣x,w)=h(x)y(1−h(x))1−y,y∈{0,1}​ 我们进行极大似然估计可得： w^MLE=argmaxw P(Y∣X,w)=argmaxw ∏i=1nP(yi∣xi,w) =argmaxw ∏i=1nh(xi)yi(1−h(xi))1−yi=argmaxw ∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi)))\\begin{aligned} &amp;\\hat{w}_{MLE}=\\underset{w}{argmax}~P(Y|X,w)=\\underset{w}{argmax}~\\prod_{i=1}^nP(y_i|x_i,w)\\\\ &amp;~~~~~~~~~~=\\underset{w}{argmax}~\\prod_{i=1}^nh(x_i)^{y_i}(1-h(x_i))^{1-y_i}=\\underset{w}{argmax}~\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i))\\big)\\\\ \\end{aligned} ​w^MLE​=wargmax​ P(Y∣X,w)=wargmax​ i=1∏n​P(yi​∣xi​,w) =wargmax​ i=1∏n​h(xi​)yi​(1−h(xi​))1−yi​=wargmax​ i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)))​ 由此我们可以定义损失函数： L(w)=−∑i=1n(yiln⁡h(xi)+(1−yi)ln⁡(1−h(xi))L(w)=-\\sum_{i=1}^n\\big(y_i\\ln h(x_i)+(1-y_i)\\ln(1-h(x_i)\\big) L(w)=−i=1∑n​(yi​lnh(xi​)+(1−yi​)ln(1−h(xi​)) 我们希望损失函数最小化以得到最优的参数www，与感知机的更新过程相同，我们求导以得到更新项。 3.2 梯度下降 梯度下降的过程为遍历数据集，利用每个样本对参数www进行更新：令z=wTxz=w^Txz=wTx 每一个样本所对应的损失函数为： l(w)=−yln⁡h(x)−(1−y)ln⁡(1−h(x))l(w)=-y\\ln h(x)-(1-y)\\ln(1-h(x)) l(w)=−ylnh(x)−(1−y)ln(1−h(x)) 对www进行求导，求出更新项： ∂l(w)∂w=∂l(w)∂h(x)⋅∂h(x)∂z⋅∂z∂w,h(x)=11+e−(wTx)①∂l(w)∂h(x)=−yh(x)+1−y1−h(x)=h(x)−yh(x)(1−h(x))②∂h(x)∂z=e−x(1+e−x)=h(x)(1−h(x)) ③∂z∂w=x因此：∂l(w)∂w=h(x)−yh(x)(1−h(x))⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x\\begin{aligned} &amp;\\frac{\\partial l(w)}{\\partial w}=\\frac{\\partial l(w)}{\\partial h(x)}\\cdot \\frac{\\partial h(x)}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w},h(x)=\\frac{1}{1+e^{-(w^Tx)}}\\\\ &amp;①\\frac{\\partial l(w)}{\\partial h(x)}=-\\frac{y}{h(x)}+\\frac{1-y}{1-h(x)}=\\frac{h(x)-y}{h(x)(1-h(x))}\\\\ &amp;②\\frac{\\partial h(x)}{\\partial z}=\\frac{e^{-x}}{(1+e^{-x})}=h(x)(1-h(x))~~③\\frac{\\partial z}{\\partial w}=x\\\\ &amp;因此：\\frac{\\partial l(w)}{\\partial w}=\\frac{h(x)-y}{h(x)(1-h(x))}\\cdot h(x)(1-h(x))\\cdot x=(h(x)-y)\\cdot x \\end{aligned} ​∂w∂l(w)​=∂h(x)∂l(w)​⋅∂z∂h(x)​⋅∂w∂z​,h(x)=1+e−(wTx)1​①∂h(x)∂l(w)​=−h(x)y​+1−h(x)1−y​=h(x)(1−h(x))h(x)−y​②∂z∂h(x)​=(1+e−x)e−x​=h(x)(1−h(x)) ③∂w∂z​=x因此：∂w∂l(w)​=h(x)(1−h(x))h(x)−y​⋅h(x)(1−h(x))⋅x=(h(x)−y)⋅x​ 则梯度下降的更新过程为： w→w−η∑i=1n(h(xi)−yi)⋅x=w−η∑i=1n(σ(wTxi)−yi)⋅xiw\\rightarrow w-\\eta\\sum_{i=1}^n(h(x_i)-y_i)\\cdot x=w-\\eta\\sum_{i=1}^n(\\sigma(w^Tx_i)-y_i)\\cdot x_i w→w−ηi=1∑n​(h(xi​)−yi​)⋅x=w−ηi=1∑n​(σ(wTxi​)−yi​)⋅xi​ 四、模型实现 我们将通过手动实现与调库的方式去构造逻辑回归模型。 4.1 数据集 本次我们仍使用乳腺癌数据集，进行逻辑回归二分类器的实现，数据集详细内容可通过如下代码查看，不再过多赘述。 4.2 手动实现模型 我们利用梯度下降的方式对模型进行更新： 4.3 调库实现模型 调库用到的函数为sklearn所提供的LogisticRegression函数，其函数原型如下： 可以发现该模型并不是用梯度下降的方式去更新模型的，而是通过参数的估计，要用到优化求解器，其相关参数如下表所示： 模型参数 Parameter含义 备注 penalty 正则化项 广义线性模型的正则项，可选值包括L1正则项'l1'、L2正则项'l2'、复合正则'elasticnet'和无正则项None，默认值为'l2'。值得注意的是，正则项的选择应与优化求解器相匹配(详见solver参数)。 l1_ratio 正则权重系数 L1正则和L2正则的权重系数，取值空间为0-1。若为0，则相当于为L2正则；若为1，则为L1正则，否则为Elasticnet正则。 dual 对偶问题 默认值False，可设为True将问题转换为对偶问题（详见本博客SVM问题中原问题-对偶问题的推导），仅适用于采用L2正则化且求解器为'liblinear’的情况。当样本数少于特征数时，推荐为True。 tol 迭代阈值 求解器迭代求解时，停止迭代的目标函数改变阈值。 C 正则化系数倒数 注意，其值与正则化强度相反，即C值越小，正则化程度越大。其值必须为正，且默认值为1 fit_intercept 是否预设偏置 控制广义线性模型中是否预设偏置值 intercept_scaling 预设偏置值 广义线性模型中预设的偏置值，仅当求解器为'liblinear'同时fit_intercept时生效。注意：该偏置值会作为新的特征计算其系数，因此也会计入L1和L2正则。 class_weight 样本权重 用于处理样本不均衡问题。默认值为None，即各类别样本权重一样，可通过设置字典定义权重系数，或设为'balanced'，即根据样本数自动计算权重。若在fit函数中设置sample_weight参数，两者作用会叠加。 random_state 随机状态 LR模型中的随机性主要体现在求解器迭代时对样本的随机选取，适用于当求解器为'liblinear'或'sag'时。 solver 求解器 sklearn中共提供了5种优化求解器，分别为'liblinear'、'sag'、'saga'、'newton-cg'和'lbfgs'。各求解器的适用条件不同，具体见后文。默认值为'liblinear'。 max_iter 最大迭代步数 ‘newton-cg’、'sag'和'lbfgs' 求解器所需要的最大迭代步数 multi_class 多分类策略 取值可为'ovr'、'multinomial'和'auto'。'ovr'即采用'one vs rest'策略对二分类模型进行集成；'multinomial'即采用'multinomial loss'直接求解多分类问题。默认为'auto'，其会在两分类问题或求解器为'liblinear'时选择'ovr'，而其它情况下选择'multinomial'。默认值为'auto' 在此我们不对参数作过多解释，具体调参过程就问题而论。 ","link":"https://2006wzt.github.io/post/机器学习实战（七）：逻辑回归/"},{"title":"机器学习实战（六）：朴素贝叶斯","content":"朴素贝叶斯 一、基本思想 在机器学习中，朴素贝叶斯分类器是在强独立假设下基于贝叶斯定理的一系列简单概率分类器，强独立假设为： 对于训练数据：D={(x1,y1),(x2,y2),...,(xn,yn)}\\begin{aligned} &amp;对于训练数据：D=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\} \\end{aligned} ​对于训练数据：D={(x1​,y1​),(x2​,y2​),...,(xn​,yn​)}​ 我们假设它是从未知分布中抽取的独立同分布，则有： P(D)=P((x1,y1),(x2,y2),...,(xn,yn))=∏α=1nP(xα,yα)P(D)=P((x_1,y_1),(x_2,y_2),...,(x_n,y_n))=\\prod_{\\alpha=1}^nP(x_\\alpha,y_\\alpha) P(D)=P((x1​,y1​),(x2​,y2​),...,(xn​,yn​))=α=1∏n​P(xα​,yα​) 如果我们有足够的数据，我们可以估算P(X,Y)P(X,Y)P(X,Y)，类似于上一节的硬币例子，我们想象一个巨大的骰子，每个可能的(X,Y)(X,Y)(X,Y)值都有一面。我们可以通过计数来估计某一特定方面出现的概率。 P^(x,y)=∑i=1nI(xi=x∩yi=y)nxi=x且yi=y时，I(xi=x∩yi=y)=1，否则为0\\hat{P}(x,y)=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{n}\\\\ x_i=x且y_i=y时，I(x_i=x\\cap y_i=y)=1，否则为0 P^(x,y)=n∑i=1n​I(xi​=x∩yi​=y)​xi​=x且yi​=y时，I(xi​=x∩yi​=y)=1，否则为0 当然，如果我们主要感兴趣的是从特征xxx预测标签yyy，我们可以直接估计P(y∣x)P(y | x)P(y∣x)，而不是P(x，y)P(x，y)P(x，y)。我们可以使用贝叶斯最优分类器对P(y∣x)P(y | x)P(y∣x)进行预测： P^(y∣x)=P^(x∣y)P^(y)P(x)=P^(x,y)P(x)=∑i=1nI(xi=x∩yi=y)/n∑i=1nI(xi=x)/n=∑i=1nI(xi=x∩yi=y)∑i=1nI(xi=x)\\hat{P}(y|x)=\\frac{\\hat{P}(x|y)\\hat{P}(y)}{P(x)}=\\frac{\\hat{P}(x,y)}{P(x)}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)/n}{\\sum_{i=1}^nI(x_i=x)/n}=\\frac{\\sum_{i=1}^nI(x_i=x\\cap y_i=y)}{\\sum_{i=1}^nI(x_i=x)} P^(y∣x)=P(x)P^(x∣y)P^(y)​=P(x)P^(x,y)​=∑i=1n​I(xi​=x)/n∑i=1n​I(xi​=x∩yi​=y)/n​=∑i=1n​I(xi​=x)∑i=1n​I(xi​=x∩yi​=y)​ 如上图韦恩图所示，我们最终的预测结果可以表示为： P^(y∣x)=∣C∣∣B∣\\hat{P}(y|x)=\\frac{|C|}{|B|} P^(y∣x)=∣B∣∣C∣​ 二、朴素贝叶斯算法 2.1 贝叶斯规则 朴素贝叶斯算法的核心是贝叶斯公式： P(y∣x)=P(x∣y)P(y)P(x)P(y|x)=\\frac{P(x|y)P(y)}{P(x)} P(y∣x)=P(x)P(x∣y)P(y)​ 根据上述贝叶斯公式我们可以知道：如果我们可以预测P(x∣y)P(x|y)P(x∣y)和P(y)P(y)P(y)，那么我们即可预测P(y∣x)P(y|x)P(y∣x) ①预测P(y)P(y)P(y)很容易，假如yyy取离散的值，我们只需要记录观察到结果为yyy的次数即可： P^(y=c)=∑i=1nI(yi=c)n=π^c\\hat{P}(y=c)=\\frac{\\sum_{i=1}^nI(y_i=c)}{n}=\\hat{\\pi}_c P^(y=c)=n∑i=1n​I(yi​=c)​=π^c​ ②但是预测P(x∣y)P(x|y)P(x∣y)并不容易，因此我们需要引入朴素贝叶斯假设。 2.2 朴素贝叶斯假设 朴素贝叶斯假设为： P(x∣y)=∏α=1dP(xα∣y)x=[x1,x2,...,xd],xα是d维特征向量x在第α维度上的值P(x|y)=\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\\\ x=[x_1,x_2,...,x_d],x_\\alpha是d维特征向量x在第\\alpha维度上的值 P(x∣y)=α=1∏d​P(xα​∣y)x=[x1​,x2​,...,xd​],xα​是d维特征向量x在第α维度上的值 即：对于给定的标签，其特征值是独立的，这是一个非常大胆的假设 经常使用朴素贝叶斯分类器的一个例子是垃圾邮件的过滤，此处数据为电子邮件，标签为是垃圾邮件还是非垃圾邮件； 朴素贝叶斯假设意味着电子邮件中的单词在条件上是独立的，因为我们知道电子邮件是否是垃圾邮件，显然这不是真的。无论是垃圾邮件还是非垃圾邮件都不是独立随机抽取的。然而，即使违反了这一假设，由此产生的分类器在实践中也可以很好地工作。 由此我们可以对P(x∣y)P(x|y)P(x∣y)进行估计：假设朴素贝叶斯假设成立，则贝叶斯分类器定义如下： h(x)=argmaxy P(y∣x)=argmaxy P(x∣y)P(y)P(x)=argmaxy P(x∣y)P(y) =argmaxy (∏α=1dP(xα∣y))P(y)=argmaxy (∑α=1dlog⁡P(xα∣y))+log⁡P(y)\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~P(y|x)=\\underset{y}{argmax}~\\frac{P(x|y)P(y)}{P(x)}=\\underset{y}{argmax}~P(x|y)P(y)\\\\ &amp;~~~~~~~~~=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^dP(x_\\alpha|y)\\big)P(y)=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log P(x_\\alpha|y)\\big)+\\log P(y) \\end{aligned} ​h(x)=yargmax​ P(y∣x)=yargmax​ P(x)P(x∣y)P(y)​=yargmax​ P(x∣y)P(y) =yargmax​ (α=1∏d​P(xα​∣y))P(y)=yargmax​ (α=1∑d​logP(xα​∣y))+logP(y)​ 估计P(xα∣y)P(x_\\alpha|y)P(xα​∣y)和P(y)P(y)P(y)都很容易，因此我们则可实现贝叶斯分类器。 三、估算P(xα∣y)P(x_\\alpha|y)P(xα​∣y) 3.1 Case#1：分类特征 分类特征：对于ddd维特征向量xxx，它的第α\\alphaα维特征xαx_\\alphaxα​有KαK_\\alphaKα​个取值，即分类问题的特征 例如：年龄、性别、省份等特征，特们都有固定个数的KαK_\\alphaKα​个取值，xα∈{f1,f2,...,fKα}x_\\alpha\\in\\{f_1,f_2,...,f_{K_\\alpha} \\}xα​∈{f1​,f2​,...,fKα​​} P(xα=fj∣y=c)=[θjc]α,∑j=1Kαθjc=1P(x_\\alpha=f_j|y=c)=[\\theta_{jc}]_\\alpha,\\sum_{j=1}^{K_\\alpha}\\theta_{jc}=1 P(xα​=fj​∣y=c)=[θjc​]α​,j=1∑Kα​​θjc​=1 [θjc]α[θ_{jc}]_α[θjc​]α​是特征ααα在假设标签是ccc时，具有值fjf_jfj​的概率。约束表明xαx_αxα​必须具有一个类别{1，…，Kα}\\{1，…，K_α\\}{1，…，Kα​} 下面我们对[θjc]α[\\theta_{jc}]_\\alpha[θjc​]α​进行估计： [θjc]^α=∑i=1nI(xi=fj∩yi=c)+l∑i=1nI(yi=c)+lKα\\hat{[\\theta_{jc}]}_\\alpha=\\frac{\\sum_{i=1}^nI(x_i=f_j\\cap y_i=c)+l}{\\sum_{i=1}^nI(y_i=c)+lK_\\alpha} [θjc​]^​α​=∑i=1n​I(yi​=c)+lKα​∑i=1n​I(xi​=fj​∩yi​=c)+l​ l ~l~ l 是一个平滑参数： ①l=0l=0l=0时，我们得到MLEMLEMLE估计量，l&gt;0l&gt;0l&gt;0时，我们得到MAPMAPMAP估计量 ②l=1l=1l=1时，我们得到拉普拉斯平滑 最终我们得到的模型为： h(x)=argmaxy (∏α=1d[θjc]^α)π^c=argmaxy (∑α=1dlog⁡[θjc]α^)+log⁡π^c\\begin{aligned} &amp;h(x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d\\hat{[\\theta_{jc}]}_\\alpha\\big)\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^d\\log\\hat{[\\theta_{jc}]_\\alpha}\\big)+\\log\\hat\\pi_c \\end{aligned} ​h(x)=yargmax​ (α=1∏d​[θjc​]^​α​)π^c​=yargmax​ (α=1∑d​log[θjc​]α​^​)+logπ^c​​ 3.2 Case#2：多项式特征 多项式特征：特征值不是诸如男女之类的分类特征，而是计数值，即回归问题的特征，但是计数值是有限的 例如：垃圾邮件过滤的例子中，各个特征是不同的单词，维度ddd即为单词表的大小，特征值是一个单词出现的次数，比如某个单词ααα出现十次，即xαx_αxα​=10，可能意味着该邮件为垃圾邮件。 xα∈{0,1,2,...,m},m=∑α=1dxαx_\\alpha\\in\\{0,1,2,...,m\\},m=\\sum_{\\alpha=1}^dx_\\alpha xα​∈{0,1,2,...,m},m=α=1∑d​xα​ 新的估计方式如下：P(x∣m,y=c)P(x|m,y=c)P(x∣m,y=c)表示标签y=cy=cy=c时，一个文本长度为mmm的邮件中特征向量为xxx的概率 P^(x∣m,y=c)=m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θαc)xα\\hat{P}(x|m,y=c)=\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha} P^(x∣m,y=c)=x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θαc​)xα​ θαc\\theta_{\\alpha c}θαc​是在标签y=cy=cy=c时，选中特征向量中的xαx_\\alphaxα​的概率，有∑α=1dθαc=1\\sum_{\\alpha=1}^d\\theta_{\\alpha c}=1∑α=1d​θαc​=1，以垃圾邮件为例，θαc\\theta_{\\alpha c}θαc​即为单词xαx_\\alphaxα​在文本中所占的比例。 下面我们对θαc\\theta_{\\alpha c}θαc​进行估计： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ xiαx_{i\\alpha}xiα​是第iii个电子邮件的特征向量的第α\\alphaα维度的值，mi=∑α=1dxiαm_i=\\sum_{\\alpha=1}^dx_{i\\alpha}mi​=∑α=1d​xiα​即第iii个电子邮件中的文本总数 最终我们得到的模型为： h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 3.3 Case#3：连续特征 连续特征：计数值是连续的值，例如身高体重，连续特征所对应的朴素贝叶斯也称为高斯朴素贝叶斯 xα∈Rx_\\alpha\\in R xα​∈R 新的估计方式如下： P^(xα∣y=c)=N(μαc,σαc2)=12πσαce−12(xα−μαcσαc)2\\hat{P}(x_\\alpha|y=c)=\\mathcal{N}(\\mu_{\\alpha c},\\sigma_{\\alpha c}^2)=\\frac1{\\sqrt{2\\pi}\\sigma_{\\alpha c}}e^{-\\frac12(\\frac{x_\\alpha-\\mu_{\\alpha c}}{\\sigma_{\\alpha c}})^2} P^(xα​∣y=c)=N(μαc​,σαc2​)=2π​σαc​1​e−21​(σαc​xα​−μαc​​)2 注意，上面指定的模型基于我们对数据的假设，即每个特征α来自一类条件高斯分布。 对参数进行估计： μ^αc=1nc∑i=1nI(yi=c)xiασ^αc=1nc∑i=1nI(yi=c)(xiα−μαc)2nc=∑i=1nI(yi=c)\\hat\\mu_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}\\\\\\hat\\sigma_{\\alpha c}=\\frac1{n_c}\\sum_{i=1}^nI(y_i=c)(x_{i\\alpha}-\\mu_{\\alpha c})^2\\\\ n_c=\\sum_{i=1}^nI(y_i=c) μ^​αc​=nc​1​i=1∑n​I(yi​=c)xiα​σ^αc​=nc​1​i=1∑n​I(yi​=c)(xiα​−μαc​)2nc​=i=1∑n​I(yi​=c) 高斯朴素贝叶斯分类器本质上为逻辑回归模型： P(y∣x)=11+e−y(wTx+b)P(y|x)=\\frac{1}{1+e^{-y(w^Tx+b)}} P(y∣x)=1+e−y(wTx+b)1​ 四、朴素贝叶斯分类器 朴素贝叶斯分类器为一个线性分类器： 对于一个多项式特征的数据集，其进行分类的过程如下：假设yi∈{−1,+1}y_i\\in\\{-1,+1\\}yi​∈{−1,+1}且特征都是多项式特征，有： h(x)=argmaxc P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x∣y=+1)P(y=+1)P(x)&gt;P(x∣y=−1)P(y=−1)P(x)即：P(y=+1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(+1))xα&gt;P(y=−1)⋅m!x1!⋅x2!⋅⋅⋅⋅⋅⋅xd!∏α=1d(θα(−1))xα即：log⁡P(y=+1)+∑α=1dxαlog⁡θα(+1)&gt;log⁡P(y=−1)+∑α=1dxαlog⁡θα(−1)即：[log⁡P(y=+1)−log⁡P(y=−1)]+∑α=1dxα(log⁡θα(+1)−log⁡θα(−1))&gt;0\\begin{aligned} &amp;h(x)=\\underset{c}{argmax}~P(y=c|x)\\\\ &amp;设h(x)=+1,则P(y=+1|x)&gt;P(y=-1|x)\\\\ &amp;根据贝叶斯公式，则有：\\frac{P(x|y=+1)P(y=+1)}{P(x)}&gt;\\frac{P(x|y=-1)P(y=-1)}{P(x)}\\\\ &amp;即：P(y=+1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (+1)})^{x_\\alpha}&gt;P(y=-1)\\cdot\\frac{m!}{x_1!\\cdot x_2!\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot x_d!}\\prod_{\\alpha=1}^d(\\theta_{\\alpha (-1)})^{x_\\alpha}\\\\ &amp;即：\\log P(y=+1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(+1)}&gt;\\log P(y=-1)+\\sum_{\\alpha=1}^dx_\\alpha\\log \\theta_{\\alpha(-1)}\\\\ &amp;即：\\big[\\log P(y=+1)-\\log P(y=-1)\\big]+\\sum_{\\alpha=1}^dx_\\alpha(\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)})&gt;0 \\end{aligned} ​h(x)=cargmax​ P(y=c∣x)设h(x)=+1,则P(y=+1∣x)&gt;P(y=−1∣x)根据贝叶斯公式，则有：P(x)P(x∣y=+1)P(y=+1)​&gt;P(x)P(x∣y=−1)P(y=−1)​即：P(y=+1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(+1)​)xα​&gt;P(y=−1)⋅x1​!⋅x2​!⋅⋅⋅⋅⋅⋅xd​!m!​α=1∏d​(θα(−1)​)xα​即：logP(y=+1)+α=1∑d​xα​logθα(+1)​&gt;logP(y=−1)+α=1∑d​xα​logθα(−1)​即：[logP(y=+1)−logP(y=−1)]+α=1∑d​xα​(logθα(+1)​−logθα(−1)​)&gt;0​ 我们可以发现上述正确分类的形式与感知机很像，我们可以进行如下变换： 令：b=log⁡P(y=+1)−log⁡P(y=−1),wα=log⁡θα(+1)−log⁡θα(−1)则：h(x)=+1↔wTx+b&gt;0\\begin{aligned} &amp;令：b=\\log P(y=+1)-\\log P(y=-1),w_\\alpha=\\log \\theta_{\\alpha(+1)}-\\log \\theta_{\\alpha(-1)}\\\\ &amp;则：h(x)=+1\\leftrightarrow w^Tx+b&gt;0 \\end{aligned} ​令：b=logP(y=+1)−logP(y=−1),wα​=logθα(+1)​−logθα(−1)​则：h(x)=+1↔wTx+b&gt;0​ 与感知机的形式完全相同，最终我们将朴素贝叶斯分类器等效为了感知机。 五、算法实现 我们将通过手动实现与调库的方式去构造朴素贝叶斯模型。 5.1 数据集 本次我们使用的数据集为垃圾邮件数据集，其下载链接为：UCI Machine Learning Repository: SMS Spam Collection Data Set 下载后的SMSSpamCollection中的内容如下图所示，显然我们需要对文件进行处理整合，数据处理过程如下： 上述数据集预处理过程中有几点需要注意： ①nltk报错问题，不妨尝试运行以下两部分代码完善nltk库的安装 ②TF-IDF特征矩阵：TF-IDF是Term Frequency - Inverse Document Frequency的缩写，即“词频——逆文本频率”。它由两部分组成，TF和IDF，也就是这两部分的乘积。TF指的就是常用的词频，即某个单词在当前文本中出现的频率。IDF，即“逆文本频率”，反映了一个单词在当前文本中的重要性 TF(t,D)=单词t在文本D中出现的次数文本D的总单词数IDF(t)=log⁡语料库文本总数+1包含词语t的文本总数+1TF−IDF(t,D)=TF(t,D)⋅IDF(t)\\begin{aligned} &amp;TF(t,D)=\\frac{单词t在文本D中出现的次数}{文本D的总单词数}\\\\ &amp;IDF(t)=\\log\\frac{语料库文本总数+1}{包含词语t的文本总数+1}\\\\ &amp;TF-IDF(t,D)=TF(t,D)\\cdot IDF(t) \\end{aligned} ​TF(t,D)=文本D的总单词数单词t在文本D中出现的次数​IDF(t)=log包含词语t的文本总数+1语料库文本总数+1​TF−IDF(t,D)=TF(t,D)⋅IDF(t)​ 我们处理后得到的 vectorizer.fit_transform(X) 输出的特征矩阵形式为(A,B) C(A,B)~C(A,B) C，AAA为文件索引，BBB为特定词的向量索引，CCC为文件AAA中单词BBB的TF−IDFTF-IDFTF−IDF分数 5.2 手动实现模型 我们选择多项式特征的数据集，即在上述数据处理的基础上将文本转为统计单词数量的矩阵，数据预处理与存储过程如下： 参考多项式特征的模型，我们可以构造我们的训练代码： θ^αc=∑i=1nI(yi=c)xiα+l∑i=1nI(yi=c)mi+l⋅d\\hat{\\theta}_{\\alpha c}=\\frac{\\sum_{i=1}^nI(y_i=c)x_{i\\alpha}+l}{\\sum_{i=1}^nI(y_i=c)m_i+l\\cdot d} θ^αc​=∑i=1n​I(yi​=c)mi​+l⋅d∑i=1n​I(yi​=c)xiα​+l​ h(x)=argmaxy P(y=c∣x)=argmaxy (∏α=1d(θαc)xα)⋅π^c=argmaxy (∑α=1dxαlog⁡θαc)+log⁡π^ch(x)=\\underset{y}{argmax}~P(y=c|x)=\\underset{y}{argmax}~\\big(\\prod_{\\alpha=1}^d(\\theta_{\\alpha c})^{x_\\alpha}\\big)\\cdot\\hat\\pi_c=\\underset{y}{argmax}~\\big(\\sum_{\\alpha=1}^dx_\\alpha\\log\\theta_{\\alpha c}\\big)+\\log\\hat\\pi_c h(x)=yargmax​ P(y=c∣x)=yargmax​ (α=1∏d​(θαc​)xα​)⋅π^c​=yargmax​ (α=1∑d​xα​logθαc​)+logπ^c​ 5.3 调库实现模型 调库实现模型时，我们可以使用另一种方法，即上面提到过的TF-IDF特征矩阵，即连续特征的数据集，数据预处理与存储过程如下： 我们直接利用sklearn所提供的朴素贝叶斯分类器，观察分类情况，可以发现调库的准确率达到了0.9596412556053812，也较为准确。 ","link":"https://2006wzt.github.io/post/机器学习实战（六）：朴素贝叶斯/"},{"title":"机器学习实战（五）：概率估计","content":"概率估计 一、分布预测分类 我们在KNNKNNKNN算法中已经学习过了贝叶斯最优分类器： 如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx=arg⁡max⁡yP(y∣x)\\begin{aligned} &amp;如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签~label_x=\\arg\\max_{y}P(y|x) \\end{aligned} ​如果我们已知分布P(X,Y),我们可以预测x的标签为概率最高的那个标签 labelx​=argymax​P(y∣x)​ 如果我们可以根据训练集得到一个大致的分布P(X,Y)，则可以利用贝叶斯最优分类器进行分类，对分布进行预测的学习分为两类： ①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)\\begin{aligned} &amp;①生成学习：预测P(X,Y)=P(X|Y)P(Y)\\\\ &amp;②判别学习：直接预测P(Y|X) \\end{aligned} ​①生成学习：预测P(X,Y)=P(X∣Y)P(Y)②判别学习：直接预测P(Y∣X)​ 二、极大似然估计 Maximum Likelihood Estimation (MLE) 2.1 简单场景：掷硬币 我们投十次硬币，假设投掷结果为：D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T}，则我们一般会进行以下预测： nH=4,nT=6,P(H)=θ≈nHnH+nT=0.4n_H=4,n_T=6,P(H)=\\theta\\approx\\frac{n_H}{n_H+n_T}=0.4 nH​=4,nT​=6,P(H)=θ≈nH​+nT​nH​​=0.4 2.2 形式化定义 上述掷硬币的例子就是极大似然估计的过程，对于MLEMLEMLE，一般分为两步： ①对分布类型进行明确的建模假设 ②设置分布中所涉及的参数 对于掷硬币问题的分布，我们易知这是一个经典的二项分布，他有两个参数：抛硬币次数nnn，某个事件（例如：硬币正面朝上）发生的概率θ\\thetaθ，我们不妨假设 P(H)=θ ~P(H)=\\theta~ P(H)=θ ，则有： P(D∣θ)=CnH+nTnH⋅θnH⋅(1−θ)nT\\begin{aligned} &amp;P(D|\\theta)=C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}\\\\ \\end{aligned} ​P(D∣θ)=CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​​ P(D∣θ) P(D|\\theta)~P(D∣θ) 表示θ\\thetaθ为某个值时，抛硬币结果为DDD的概率，比如D={H,T,T,H,H,H,T,T,T,T}D=\\{H, T, T, H, H, H, T, T, T, T\\}D={H,T,T,H,H,H,T,T,T,T} MLEMLEMLE的规则：对于固定的事件DDD，找到一个θ\\thetaθ使得P(D∣θ)P(D|\\theta)P(D∣θ)最大： θ^MLE=argmaxθ P(D∣θ)=argmaxθ CnH+nTnH⋅θnH⋅(1−θ)nT=argmaxθ (nHln⁡θ+nTln⁡(1−θ))\\begin{aligned} &amp;\\hat{\\theta}_{MLE}=\\underset{\\theta}{argmax}~P(D|\\theta)=\\underset{\\theta}{argmax}~C^{n_H}_{n_H+n_T}\\cdot \\theta^{n_H}\\cdot(1-\\theta)^{n_T}=\\underset{\\theta}{argmax}~(n_H\\ln\\theta+n_T\\ln(1-\\theta)) \\end{aligned} ​θ^MLE​=θargmax​ P(D∣θ)=θargmax​ CnH​+nT​nH​​⋅θnH​⋅(1−θ)nT​=θargmax​ (nH​lnθ+nT​ln(1−θ))​ 我们对函数进行求导即可得到θ\\thetaθ的极大似然估计值： 令f(θ)=nHln⁡θ+nTln⁡(1−θ)df(θ)dθ=nHθ−nT1−θ=0→θ=nHnH+nT,即θ^MLE=nHnH+nT\\begin{aligned} &amp;令f(\\theta)=n_H\\ln\\theta+n_T\\ln(1-\\theta)\\\\ &amp;\\frac{df(\\theta)}{d\\theta}=\\frac{n_H}{\\theta}-\\frac{n_T}{1-\\theta}=0\\rightarrow \\theta=\\frac{n_H}{n_H+n_T},即\\hat{\\theta}_{MLE}=\\frac{n_H}{n_H+n_T} \\end{aligned} ​令f(θ)=nH​lnθ+nT​ln(1−θ)dθdf(θ)​=θnH​​−1−θnT​​=0→θ=nH​+nT​nH​​,即θ^MLE​=nH​+nT​nH​​​ 可以发现极大似然估计的预测结果与我们的直观预测相同，这就是MLEMLEMLE 三、先验估计 3.1 简单场景：掷硬币 我们仍可以用掷硬币的场景去理解先验估计：假设我们预感θ\\thetaθ接近0.50.50.5。但我们的样本量很小，所以我们对此估计并不确信，可以作如下处理： θ^=nH+mnH+nT+2m\\hat{\\theta}=\\frac{n_H+m}{n_H+n_T+2m} θ^=nH​+nT​+2mnH​+m​ 当nnn很大时，该处理对θ\\thetaθ的影响微不足道；但是当nnn较小时，该处理可以使得θ\\thetaθ更接近我们的猜测。 3.2 形式化定义 假设θθθ是根据分布P(θ)P(θ)P(θ)得到的一个随机值，DDD为一个事件，则有如下贝叶斯公式： P(θ∣D)=P(D∣θ)P(θ)P(D)=P(D,θ)P(D)P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D,\\theta)}{P(D)} P(θ∣D)=P(D)P(D∣θ)P(θ)​=P(D)P(D,θ)​ ①P(θ)P(θ)P(θ)是我们在看到数据之前θθθ的先验分布 ②P(D∣θ)P(D|\\theta)P(D∣θ)是对于给定的参数θ\\thetaθ，事件DDD发生的可能性 ③P(θ∣D)P(\\theta|D)P(θ∣D)是我们观察数据后得到的θ\\thetaθ的后验分布 我们常常利用BetaBetaBeta分布得到θθθ的先验分布： P(θ)=θα−1(1−θ)β−1B(α,β)(其中B(α,β)=Γ(α)Γ(β)Γ(α+β),其目的是对P(θ)进行归一化)P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\ (其中B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)},其目的是对P(\\theta)进行归一化) P(θ)=B(α,β)θα−1(1−θ)β−1​(其中B(α,β)=Γ(α+β)Γ(α)Γ(β)​,其目的是对P(θ)进行归一化) P(θ∣D)∝P(D∣θ)P(θ)∝θnH+α−1(1−θ)nT+β−1P(\\theta|D)\\propto P(D|\\theta)P(\\theta)\\propto\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1} P(θ∣D)∝P(D∣θ)P(θ)∝θnH​+α−1(1−θ)nT​+β−1 四、极大后验估计 在我们知道关于θ\\thetaθ的分布，可以利用极大后验估计得到θ\\thetaθ的估计值 MAPMAPMAP规则：对于一个固定的事件DDD，找到一个θ\\thetaθ，使得P(θ∣D)P(\\theta|D)P(θ∣D)最大： θ^MAP=argmaxθ P(θ∣D)=argmaxθ P(D∣θ)P(θ)P(D)=argmaxθ P(D∣θ)P(θ) =argmaxθ (θnH+α−1(1−θ)nT+β−1)=argmaxθ (nH+α−1)ln⁡θ+(nT+β−1)ln⁡(1−θ) =nH+α−1nH+nT+α+β−2\\begin{aligned} &amp;\\hat\\theta_{MAP}=\\underset{\\theta}{argmax}~P(\\theta|D)=\\underset{\\theta}{argmax}~\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\underset{\\theta}{argmax}~P(D|\\theta)P(\\theta)\\\\ &amp;~~~~~~~~~~~=\\underset{\\theta}{argmax}~(\\theta^{n_H+\\alpha-1}(1-\\theta)^{n_T+\\beta-1})=\\underset{\\theta}{argmax}~(n_H+\\alpha-1)\\ln\\theta+(n_T+\\beta-1)\\ln(1-\\theta)\\\\ &amp;~~~~~~~~~~~~=\\frac{n_H+\\alpha-1}{n_H+n_T+\\alpha+\\beta-2} \\end{aligned} ​θ^MAP​=θargmax​ P(θ∣D)=θargmax​ P(D)P(D∣θ)P(θ)​=θargmax​ P(D∣θ)P(θ) =θargmax​ (θnH​+α−1(1−θ)nT​+β−1)=θargmax​ (nH​+α−1)lnθ+(nT​+β−1)ln(1−θ) =nH​+nT​+α+β−2nH​+α−1​​ 当n→∞n\\rightarrow\\inftyn→∞时，α−1\\alpha-1α−1和β−2\\beta-2β−2与nHn_HnH​和nTn_TnT​相比可以忽略，即θ^MAP→θ^MLE\\hat\\theta_{MAP}\\rightarrow\\hat\\theta_{MLE}θ^MAP​→θ^MLE​ 五、总结 在监督学习中有一个数据集DDD，我们运用它来训练模型，它有一个参数 θθθ，利用这个模型我们希望对测试点xtx_txt​进行预测，则有如下方法： MLE:P(y∣xt;θ) learning:θ^MLE=arg⁡max⁡θP(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt,θ) learning:θ^MAP=arg⁡max⁡θP(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt,θ)=∫θP(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的\\begin{aligned} &amp;MLE:P(y|x_t;\\theta)~learning:\\hat\\theta_{MLE}=\\arg\\max_\\theta P(D|\\theta),此处的\\theta为一个模型参数\\\\ &amp;MAP:P(y|x_t,\\theta)~learning:\\hat\\theta_{MAP}=\\arg\\max_\\theta P(\\theta|D)\\propto P(D|\\theta)P(\\theta)，此处的\\theta为随机变量\\\\ &amp;True~Bayesian:P(y|x_t,\\theta)=\\int_\\theta P(y|\\theta)P(\\theta|D)d\\theta,此处的\\theta是考虑所有可能模型积分出来的 \\end{aligned} ​MLE:P(y∣xt​;θ) learning:θ^MLE​=argθmax​P(D∣θ),此处的θ为一个模型参数MAP:P(y∣xt​,θ) learning:θ^MAP​=argθmax​P(θ∣D)∝P(D∣θ)P(θ)，此处的θ为随机变量True Bayesian:P(y∣xt​,θ)=∫θ​P(y∣θ)P(θ∣D)dθ,此处的θ是考虑所有可能模型积分出来的​ ","link":"https://2006wzt.github.io/post/机器学习实战（五）：概率估计/"},{"title":"机器学习实战（四）：感知机算法","content":"感知机算法 一、形式化定义 在机器学习中，感知机是一种用于处理监督学习的二分类问题的分类器，它是一种线性分类器，即一种基于线性预测函数（将一组权重与特征向量相结合）进行预测的分类算法，该模型有着如下的假设： ①处理的问题是二分类问题：e.g. yi∈{−1,+1}②数据是线性可分的\\begin{aligned} &amp;①处理的问题是二分类问题：e.g.~~y_i\\in\\{-1,+1\\}\\\\ &amp;②数据是线性可分的 \\end{aligned} ​①处理的问题是二分类问题：e.g. yi​∈{−1,+1}②数据是线性可分的​ 感知机模型最终所要求解的是权重向量www，其形式化定义如下： 权重向量：w=[w1,w2,...,wd]T特征向量：x=[x1,x2,...,xd]T解空间：H={h(x)=wTx+b=0}\\begin{aligned} &amp;权重向量：w=[w_1,w_2,...,w_d]^T\\\\ &amp;特征向量：x=[x_1,x_2,...,x_d]^T\\\\ &amp;解空间：\\mathcal{H}=\\{h(x)=w^Tx+b=0\\} \\end{aligned} ​权重向量：w=[w1​,w2​,...,wd​]T特征向量：x=[x1​,x2​,...,xd​]T解空间：H={h(x)=wTx+b=0}​ 二、感知机实现 2.1 权重向量 根据形式化定义，我们知道我们的求解目标是一个权重向量www，最终得到模型函数：h(x)=wTx+bh(x)=w^Tx+bh(x)=wTx+b 即我们求解得到www对应的是多维空间中的一个超平面wTx+b=0w^Tx+b=0wTx+b=0，该平面在多维空间中将数据点分为两部分 而我们最后的分类只需要根据h(x)h(x)h(x)的正负判断数据点在哪一侧即可。 权重向量www即为所要求解的超平面的法向量 2.2 偏差项 我们注意到模型函数中还有一项常数bbb，我们称之为偏差项，如果没有偏差项，www所定义的超平面一定经过原点。 为了便于后续处理，我们通过升维操作将偏差项bbb吸收到www中去： 目标超平面：wTx+b=0令x′=[x 1],w=[w′ b]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}\\begin{aligned} &amp;目标超平面：w^Tx+b=0\\\\ &amp;令x&#x27;=\\left[ \\begin{aligned} &amp;x\\\\ &amp;~1 \\end{aligned} \\right],w=\\left[ \\begin{aligned} &amp;w&#x27;\\\\ &amp;~b \\end{aligned} \\right]，则有w&#x27;^Tx&#x27;=w^Tx+b=0\\\\ &amp;由此解空间变为：\\mathcal{H}=\\{h(x)=w^Tx=0\\} \\end{aligned} ​目标超平面：wTx+b=0令x′=[​x 1​],w=[​w′ b​]，则有w′Tx′=wTx+b=0由此解空间变为：H={h(x)=wTx=0}​ 通过升维后超平面一定是经过原点的，则有： 设yi∈{−1,+1},h(xi)&gt;0↔ yi=+1,h(xi)&lt;0↔ yi=−1可得：yi⋅h(xi)=yi⋅(wTxi)&gt;0↔xi分类正确\\begin{aligned} &amp;设y_i\\in\\{-1,+1\\},h(x_i)&gt;0\\leftrightarrow~y_i=+1,h(x_i)&lt;0\\leftrightarrow~y_i=-1\\\\ &amp;可得：y_i\\cdot h(x_i)=y_i\\cdot(w^Tx_i)&gt;0\\leftrightarrow x_i分类正确 \\end{aligned} ​设yi​∈{−1,+1},h(xi​)&gt;0↔ yi​=+1,h(xi​)&lt;0↔ yi​=−1可得：yi​⋅h(xi​)=yi​⋅(wTxi​)&gt;0↔xi​分类正确​ 注意我们尽量将二分类问题的标签设置为{−1,+1}\\{-1,+1\\}{−1,+1}，如果设置为{0,+1}\\{0,+1\\}{0,+1}则没有上述结论。 2.3 算法实现 首先我们先看感知机算法的伪码： mmm用于记录在训练集上分类错误的次数，如果出错则m=m+1m=m+1m=m+1，同时对权重向量www进行调整，调整方式为w=w+yxw=w+yxw=w+yx，该调整方式的几何解释如下图所示： 如上图所示，图一为初始的权重向量以及其所对应的超平面，我们以一个标签为−1-1−1的数据点为例，该超平面错误得将该数据点分到了+1+1+1的一侧（即yi⋅(wTxi)≤0y_i\\cdot(w^Tx_i)\\le0yi​⋅(wTxi​)≤0），则要进行调整：w=w+yx=w−xw=w+yx=w-xw=w+yx=w−x，则得到了如图三所示的调整后的超平面。 数学解释如下，w和bw和bw和b的更新是一个梯度下降的过程，而我们用到的损失函数为： L(w,b)=−∑xi∈Dyi(wTxi+b)L(w,b)=-\\sum_{x_i\\in D}y_i(w^Tx_i+b) L(w,b)=−xi​∈D∑​yi​(wTxi​+b) 当分类错误时会有 yi⋅(wTxi+b)≤0 ~y_i\\cdot(w^Tx_i+b)\\le0~ yi​⋅(wTxi​+b)≤0 ，自然就会使得该损失函数变大，该函数对www和bbb求偏导得： ∂L(w,b)∂w=−∑xi∈Dyixi∂L(w,b)∂b=−∑xi∈Dyi\\frac{\\partial L(w,b)}{\\partial w}=-\\sum_{x_i\\in D}y_ix_i\\\\ \\frac{\\partial L(w,b)}{\\partial b}=-\\sum_{x_i\\in D}y_i ∂w∂L(w,b)​=−xi​∈D∑​yi​xi​∂b∂L(w,b)​=−xi​∈D∑​yi​ 由此我们得到更新过程： w→w+yixib→b+yiw\\rightarrow w+y_ix_i\\\\ b\\rightarrow b+y_i w→w+yi​xi​b→b+yi​ 三、感知机的收敛 感知机可以说是第一个具有强大形式保证的算法。如果有数据集合是线性可分的，感知机将在有限更新次数中找到一个可以分离两类数据点的超平面，如果数据不是线性可分的，它将永远循环。 假设∃w∗有:对于∀(xi,yi)∈D，yi⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi∈D,∣∣w∗∣∣=∑i=1n(wi∗)2=1,∣∣xi∣∣≤1对于w∗对应的超平面，令γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣，即离超平面最近的点对应的距离\\begin{aligned} &amp;假设∃w^∗有:对于∀(x_i, y_i) ∈ D，y_i\\cdot(x^Tw^∗) &gt; 0\\\\ &amp;对每个数据进行等比例收敛使得：对于\\forall x_i\\in D,||w^*||=\\sqrt{\\sum_{i=1}^n(w^*_i)^2}=1,||x_i||\\le1\\\\ &amp;对于w^*对应的超平面，令\\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||，即离超平面最近的点对应的距离 \\end{aligned} ​假设∃w∗有:对于∀(xi​,yi​)∈D，yi​⋅(xTw∗)&gt;0对每个数据进行等比例收敛使得：对于∀xi​∈D,∣∣w∗∣∣=i=1∑n​(wi∗​)2​=1,∣∣xi​∣∣≤1对于w∗对应的超平面，令γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣，即离超平面最近的点对应的距离​ 由此，我们得到一个关于更新次数和γ\\gammaγ之间关系的定理： 定理 3-1 感知机算法最多对超平面进行1γ2次调整感知机算法最多对超平面进行\\frac1{\\gamma^2}次调整 感知机算法最多对超平面进行γ21​次调整 此处的调整次数（更新次数）即为上述伪码中for循环中执行 w=w+yx ~w=w+yx~ w=w+yx 的次数，对上述定理的证明如下： γ\\gammaγ是基于上述w∗w^*w∗所对应的超平面得来的，证明过程主要讨论wTw∗w^Tw^*wTw∗和wTww^TwwTw的大小关系： 1)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw∗=(w+yixi)Tw∗=(wT+xiTyiT)w∗=wTw∗+yi⋅(xiTw∗)∵γ=min⁡(xi,yi)∈D∣∣xiTw∗∣∣ 且 yi⋅(xiTw∗)&gt;0 ∴yi⋅(xiTw∗)≥γ∴wTw∗=wTw∗+yi⋅(xiTw∗)≥wTw∗+γ2)如果yi⋅(wTxi)≤0,根据算法：w=w+yixi则 wTw=(w+yixi)T(w+yixi)=wTw+2yi⋅(wTxi)+yi2xiTxi∵yi∈{−1,+1} 且 yi⋅(wTxi)≤0,数据收敛后∣∣xi∣∣2≤1 ∴ yi2=1,xiTxi≤1∴ wTw=wTw+2yi⋅(wTxi)+yi2xiTxi≤wTw+1\\begin{aligned} 1)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw^*=(w+y_ix_i)^Tw^*=(w^T+x_i^Ty_i^T)w^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\\\ &amp;\\because \\gamma=\\min_{(x_i,y_i)\\in D}||x_i^Tw^*||~且~ y_i\\cdot(x_i^Tw^*)&gt;0~\\therefore y_i\\cdot(x_i^Tw^*)\\ge \\gamma\\\\ &amp;\\therefore w^Tw^*=w^Tw^*+y_i\\cdot(x_i^Tw^*)\\ge w^Tw^*+\\gamma\\\\ 2)&amp;如果y_i\\cdot(w^Tx_i)\\le0,根据算法：w=w+y_ix_i\\\\ &amp;则~w^Tw=(w+y_ix_i)^T(w+y_ix_i)=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\\\ &amp;\\because y_i\\in\\{-1,+1\\}~且~y_i\\cdot(w^Tx_i)\\le0,数据收敛后||x_i||_2\\le1~\\therefore~y_i^2=1,x_i^Tx_i\\le 1\\\\ &amp;\\therefore~w^Tw=w^Tw+2y_i\\cdot(w^Tx_i)+y_i^2x_i^Tx_i\\le w^Tw+1 \\end{aligned} 1)2)​如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw∗=(w+yi​xi​)Tw∗=(wT+xiT​yiT​)w∗=wTw∗+yi​⋅(xiT​w∗)∵γ=(xi​,yi​)∈Dmin​∣∣xiT​w∗∣∣ 且 yi​⋅(xiT​w∗)&gt;0 ∴yi​⋅(xiT​w∗)≥γ∴wTw∗=wTw∗+yi​⋅(xiT​w∗)≥wTw∗+γ如果yi​⋅(wTxi​)≤0,根据算法：w=w+yi​xi​则 wTw=(w+yi​xi​)T(w+yi​xi​)=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​∵yi​∈{−1,+1} 且 yi​⋅(wTxi​)≤0,数据收敛后∣∣xi​∣∣2​≤1 ∴ yi2​=1,xiT​xi​≤1∴ wTw=wTw+2yi​⋅(wTxi​)+yi2​xiT​xi​≤wTw+1​ 调整的过程就是www向w∗w^*w∗趋近的过程，在算法的forforfor循环中，如果进行了调整，则wTw∗w^Tw^*wTw∗至少增加了γ\\gammaγ，wTww^TwwTw至多增加了1 设总共调整了M次，则wTw≤M≤wTw∗γ∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw≤M∴M≤1γ2\\begin{aligned} &amp;设总共调整了M次，则w^Tw\\le M\\le\\frac{w^Tw^*}{\\gamma}\\\\ &amp;\\because ||w^*||=1~\\therefore~\\gamma M\\le w^Tw^*\\le||w^T||\\cdot||w^*||=||w||=\\sqrt{w^Tw}\\le\\sqrt{M}\\\\ &amp;\\therefore M\\le\\frac1{\\gamma^2} \\end{aligned} ​设总共调整了M次，则wTw≤M≤γwTw∗​∵∣∣w∗∣∣=1 ∴ γM≤wTw∗≤∣∣wT∣∣⋅∣∣w∗∣∣=∣∣w∣∣=wTw​≤M​∴M≤γ21​​ 四、算法实现 根据上述伪码，我们将通过手动实现与调库的方式，分别实现状态机。 4.1 数据集 在本次算法实战中，我们将使用到的数据集为sklearn所提供的乳腺癌数据集，我们首先先来了解一下数据集的内容： 最终我们得到的数据集如下图所示，总共有569个数据，每条数据有30个特征，标签0，1分别代表是否患癌症。 4.2 手动实现模型 参考伪码，我们可以实现感知机的fitfitfit函数，在此基础上我们引入了两个新的量lr和max\\text{_}iter，学习率lrlrlr控制每次对超平面的更新程度，最大训练轮数max\\text{_}iter避免数据线性不可分时出现死循环。 上述的参数得到的准确率为0.9035087719298246，我们可以通过调参过程使得准确率趋于最优。 4.3 调库实现模型 模型的训练主要用到sklearn所提供的库函数Perceptron，它的函数原型如下： ①penalty：正则化项，l2l2l2、l1l1l1或弹性网络。 ②alpha：正则化项系数，如果使用正则化项，则在正则化项前乘上该系数 ③fit_intercept：是否需要计算截距 ④max_iter：最大训练轮数 ⑤tol：训练的停止标准，训练将在loss&gt;previous\\text{_}loss-tol时停止 ⑥shuffle：是否在每轮训练后对训练数据进行随机排列 ⑦eta0：学习率 4.4 可视化 我们不妨仅选取两个特征对模型进行训练，观察感知机模型最终得到的超平面划分是什么样的，我们利用make_classification创造具有两个特征的用于二分类的数据集，调用库函数进行预测： ","link":"https://2006wzt.github.io/post/机器学习实战（四）：感知机算法/"},{"title":"机器学习实战（三）：KNN算法","content":"K-NN算法 一、形式化定义 K-NN算法（k-nearest neighbor）是一种基本的分类与回归算法，在本节我们主要以分类问题为例介绍K-NN算法。 K-NN模型是一个非参数化模型，参数数量随着训练数据的规模增长，更加的灵活，但是却又较高的计算成本。 ①模型假设：相似的输入有着相似的输出 ②分类规则：对于测试输入x，在其k个最相似的训练输入之间分配最常见的标签 ③回归规则：输出是对象的属性值，该值是k个最近邻值的平均值。 设训练集为DDD，输入的测试点的特征向量为xxx，xxx的kkk个近邻点表示为集合SxS_xSx​，SxS_xSx​有如下性质： ①Sx∈D②∣Sx∣=k③∀(x′,y′)∈D−Sx,dist(x,x′)≥max⁡(x′′,y′′)∈Sxdist(x,x′′)\\begin{aligned} &amp;①S_x\\in D\\\\ &amp;②|S_x|=k\\\\ &amp;③\\forall (x&#x27;,y&#x27;)\\in D-S_x,\\text{dist}(x,x&#x27;)\\ge \\max_{(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x}\\text{dist}(x,x&#x27;&#x27;) \\end{aligned} ​①Sx​∈D②∣Sx​∣=k③∀(x′,y′)∈D−Sx​,dist(x,x′)≥(x′′,y′′)∈Sx​max​dist(x,x′′)​ 根据分类规则，我们可以得到模型函数： h(x)=mode({y′′:(x′′,y′′)∈Sx})mode返回集合中出现频率最高的标签y′′h(x)=\\text{mode}(\\{y&#x27;&#x27;:(x&#x27;&#x27;,y&#x27;&#x27;)\\in S_x\\})\\\\ \\text{mode}返回集合中出现频率最高的标签y&#x27;&#x27; h(x)=mode({y′′:(x′′,y′′)∈Sx​})mode返回集合中出现频率最高的标签y′′ 根据上述定义，我们可以通过下图更直观得认识KNN算法，当K=1时，新的数据点被分类为正方形，当K=3时，新的数据点被分为三角形 二、参数选择 2.1 距离函数 K-NN分类器基本上依赖于距离函数的选择，距离度量越能反映标签的相似性，分类器的性能就越好。 最为常用的距离函数为闵可夫斯基距离（Minkowski distance），设数据点的特征向量为ddd维向量，则(x,y)(x,y)(x,y)和(x′,y′)(x&#x27;,y&#x27;)(x′,y′)之间的距离为： dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ 其中x=(x1,x2,...,xd),x′=(x1′,x2′,...,xd′)x=(x_1,x_2,...,x_d),x&#x27;=(x&#x27;_1,x&#x27;_2,...,x&#x27;_d)x=(x1​,x2​,...,xd​),x′=(x1′​,x2′​,...,xd′​)，当ppp取值不同时，得到不同的距离函数。 ①p=2p=2p=2时，为欧氏距离： dist(x,x′)=∑r=1d∣xr−xr′∣2\\text{dist}(x,x&#x27;)=\\sqrt{\\sum_{r=1}^d|x_r-x&#x27;_r|^2} dist(x,x′)=r=1∑d​∣xr​−xr′​∣2​ 欧几里得度量（Euclidean Metric）是一个通常采用的距离定义，指在d维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离），在二维和三维空间中的欧氏距离就是两点之间的实际距离。 ②p=1p=1p=1时，为曼哈顿距离： dist(x,x′)=∑r=1d∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\sum_{r=1}^d|x_r-x&#x27;_r| dist(x,x′)=r=1∑d​∣xr​−xr′​∣ 想象你在城市道路里，要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。 实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源，曼哈顿距离也称为城市街区距离（City Block distance）。 ③p=∞p=\\inftyp=∞时，为切比雪夫距离： dist(x,x′)=max⁡r∣xr−xr′∣\\text{dist}(x,x&#x27;)=\\max_r|x_r-x&#x27;_r| dist(x,x′)=rmax​∣xr​−xr′​∣ 二个点之间的切比雪夫距离定义是其各坐标数值差绝对值的最大值。 国际象棋棋盘上二个位置间的切比雪夫距离是指王要从一个位子移至另一个位子需要走的步数。由于王可以往斜前或斜后方向移动一格,因此可以较有效率的到达目的的格子。上图是棋盘上所有位置距f6f6f6位置的切比雪夫距离。 2.2 K参数 我们应该如何选择一个合适的K值使得分类器的效果达到最优？ 1）一个较小的K值： ①减少在学习过程中的近似误差，即减小在训练集上的误差 ②扩大在学习过程中的估计误差，即增大在测试集上的误差 ③会使得模型更加复杂，容易导致过拟合 2）一个较大的K值： ①减少在学习过程中的估计误差，即减小在测试集上的误差 ②扩大在学习过程中的近似误差，即增大在训练集上的误差 ③会使得模型更加简单 因此我们需要对K值进行一个权衡，这取决于所提供的数据集。通常，较大的k值会减少噪声对分类的影响，但会使类之间的边界不那么明显，在二分类问题中，选择k为奇数有助于避免并列。 三、特殊的K-NN分类器 3.1 1-NN分类器 当K=1K=1K=1时，我们得到的分类器便是1-NN分类器，这是一个最为直观的K-NN分类器，选取离得最近的那个数据点的标签作为预测标签。 1-NN边界划分的方法：选定每一个点最近的那个邻居，作它们连线间的中垂线，中垂线相连形成边界，得到的分类边界如下图所示 3.2 贝叶斯最优分类器 假如我们知道了任何xxx对应的yyy，即知道分布P(y∣x)P(y|x)P(y∣x)（这在现实中不可能），这样对于一个xxx你就可以简单地预测最有可能的标签，Bayes optimal classifier预测为： y^=hopt(x)=argmaxy P(y∣x)\\hat y=h_{opt}(x)=\\underset{y}{argmax}~P(y|x) y^​=hopt​(x)=yargmax​ P(y∣x) 即：对于一个xxx，取概率最高的标签作为预测的标签，Bayes optimal classifier 仍然可能出错，它的出错率即为： ϵBayesOpt=1−P(hopt(x)∣x)=1−P(y^∣x)\\epsilon_{BayesOpt}=1-P(h_{opt}(x)|x)=1-P(\\hat y|x) ϵBayesOpt​=1−P(hopt​(x)∣x)=1−P(y^​∣x) 例：假设一个二分类问题，标签仅有+1和-1两种，并且对于某个xxx，有： P(+1∣x)=0.8,P(−1∣x)=0.2P(+1|x)=0.8,P(-1|x)=0.2 P(+1∣x)=0.8,P(−1∣x)=0.2 因此根据Bayes optimal classifier可得： y^=+1,ϵBayesOpt=0.2\\hat y=+1,\\epsilon_{BayesOpt}=0.2 y^​=+1,ϵBayesOpt​=0.2 由此我们可以知道贝叶斯最优分类器的错误率是在给定数据分布时的最小可实现错误率。同时我们也能得到如下定理： 定理 3-1 证明如下： 如上图所示，在数据集趋近于无穷大时，对于任何一个数据点，它与K个近邻的距离趋近于0，即： 设测试点为xt，它的近邻为xNN,当数据集大小n→∞时，dist(xt,xNN)→0xt→ xNN,因此xt预测的标签将为xNN的标签\\begin{aligned} &amp;设测试点为x_t，它的近邻为x_{NN},当数据集大小n\\rightarrow\\infty时，\\text{dist}(x_t,x_{NN})\\rightarrow0\\\\ &amp;x_t\\rightarrow~x_{NN},因此x_t预测的标签将为x_{NN}的标签 \\end{aligned} ​设测试点为xt​，它的近邻为xNN​,当数据集大小n→∞时，dist(xt​,xNN​)→0xt​→ xNN​,因此xt​预测的标签将为xNN​的标签​ 如上图所示，我们以垃圾邮件分类为例，SpamSpamSpam为垃圾邮件，HamHamHam为正常邮件，假设xNNx_{NN}xNN​的标签为SpamSpamSpam，则有： ϵBayesOpt=1−P(s∣x)ϵ1−NN=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt\\begin{aligned} &amp;\\epsilon_{BayesOpt}=1-P(s|x)\\\\ &amp;\\epsilon_{1-NN}=P(s|x)(1-P(s|x))+(1-P(s|x))P(s|x)=2(1-P(s|x))P(s|x)&lt;2(1-P(s|x))=2\\epsilon_{BayesOpt} \\end{aligned} ​ϵBayesOpt​=1−P(s∣x)ϵ1−NN​=P(s∣x)(1−P(s∣x))+(1−P(s∣x))P(s∣x)=2(1−P(s∣x))P(s∣x)&lt;2(1−P(s∣x))=2ϵBayesOpt​​ 四、维度灾难 当向量的维度很高时，KNN模型将变得非常不稳定。 4.1 数据点之间的距离 首先让我们想象一个ddd维的单位超立方体[0,1]d[0,1]^d[0,1]d，所有的数据都在此超立方体内均匀采样，即对于任意的xix_ixi​，都有xi∈[0,1]dx_i\\in[0,1]^dxi​∈[0,1]d。接着我们不妨考虑一下K=10K=10K=10时，对于一个测试点它的近邻所在的超立方体，如下图所示： 设 l ~l~ l 是包含KKK个近邻的最小超立方体的边长， n ~n~ n 是取样的数据点数，因为数据集是在该单位超立方体中均匀取样，则有： ld=kn,l=(kn)1dl^d=\\frac{k}{n},l=(\\frac{k}{n})^{\\frac1d} ld=nk​,l=(nk​)d1​ 对于K=10,n=1000K=10,n=1000K=10,n=1000时的情况，则有： d l 2 0.1 10 0.63 100 0.955 1000 0.9954 因此，当维度很高（d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0）时，几乎需要整个空间才能找到某个测试点的10个近邻。 从而导致最近的KKK个邻居并不比训练集中其他的点离测试点更近。 这是因为，在ddd维空间中随机分布的点之间的距离逐渐趋近于一个很小的范围内，如下图所示： 4.2 到超平面的距离 对于二分类问题，我们往往是在ddd维空间中寻找一个超平面，将空间一分为二，超平面两侧的数据即为不同的标签。 如上图所示，维度灾难对于两点之间的距离和点与超平面之间的距离有不同的影响。 同一维度中的移动不能增加或减少到超平面的距离，点只是四处移动并与超平面保持相同的距离。但随着维度的升高，成对点之间的距离变得非常大，到超平面的距离变得相对较小。 维数灾难的一个后果是，大多数数据点往往非常接近这些超平面，并且通常可能轻微的扰动就会更改分类。 4.3 数据降维 为了解决维度灾难，最好的方法便是对数据进行降维操作，我们以图像识别为例：虽然一张脸的图像可能需要1800万像素，但是我们可能只需要用少于50个属性（例如男性/女性、金发/深色头发等）就可以描述一个人，这些属性会随着脸的变化而变化。 如上图所示，这是从底层二维流形绘制的三维数据集示例。蓝色点位于粉色表面之上，而粉色表面则嵌入在三维空间之中，粉色表面映射为二维平面，进而将数据集从三维映射到二维，实现数据降维。 五、算法实战 本次我们使用Scikit-learn所提供的鸢尾花数据集，通过调库的方式实现K-NN算法。 5.1 数据集 我们首先了解一下该数据集的特点，读取数据并将其存储为csv文件的方式如下： 我们最终得到的数据集如下图所示，总共有150个数据，每条数据有4个特征，分别为花萼长度、花萼宽度、花瓣长度、花瓣宽度，相应的不同特征所对应的标签共有3种，分别为setosa、versicolor、virginica，即鸢尾花的三个种类。 5.2 模型训练 模型的训练主要用到sklearn所提供的库函数KNeighborsClassifier，它的函数原型如下： ①n_neighbors：即K值的大小，默认为5 ②weights：用于预测的权重函数。可选参数如下: ③algorithm：计算最近邻居用的算法。可选参数如下： 球树与kd树将会在后续文章中进行介绍，这是一种优化KNN算法的方式。 ④leaf_size：传入BallTree或者KDTree算法的叶子数量，此参数会影响构建、查询BallTree或者KDTree的速度，以及存储BallTree或者KDTree所需要的内存大小。 ⑤p：用于Minkowski metric的超参数，即当我们用闵可夫斯基距离作为度量时，p值的大小。 dist(x,x′)=(∑r=1d∣xr−xr′∣p)1p\\text{dist}(x,x&#x27;)=(\\sum_{r=1}^d|x_r-x&#x27;_r|^p)^{\\frac1p} dist(x,x′)=(r=1∑d​∣xr​−xr′​∣p)p1​ ⑥metric：计算距离的方式，默认为闵可夫斯基距离。 我们不妨先用简单的欧几里得距离构建模型，观察模型的预测效果： 在这里准确率的计算方法比较简单，即Accuracy=预测准确的数据点总测试点数Accuracy=\\frac{预测准确的数据点}{总测试点数}Accuracy=总测试点数预测准确的数据点​ 上述模型中，我们最终在测试集上得到的准确率为0.9666666666666667，可以看出模型的效果已经非常好了。 5.3 可视化 接下来让我们进行一些调参过程，观察能否让准确率再高一些，首先调整KKK值，对于每个KKK值我们进行十折交叉验证： 如上图所示，是在KKK在[1,30][1,30][1,30]上取不同值时十折交叉验证的准确率，因为是随机对数据集进行划分，所以每次所得的曲线图是不同的，不过其大致趋势是相同的，我们最终可以得到KKK取到[12,18][12,18][12,18]能得到最高准确率的概率最高。 而其他参数的调参过程完全可以效仿上述K值调参可视化的过程，只需要修改需要遍历的取值范围以及参数名即可，即这部分代码： 5.4 特征重要性 因为在鸢尾花数据集中仅有4个特征，所以我们在此讨论特征重要性其实是没必要的，但是对于具有很多特征的数据集我们则需要依据特征的重要性对特征进行筛选，一方面可以提高预测的准确性，另一方面可以提高模型的效率，我们不妨先观察鸢尾花数据集的4个特征对分类准确性的影响。 我们可以通过各个特征对所对应的标签分布直观得判断特征的重要性，可视化代码如下： 我们任取两个特征，观察特征对所对应的数据分布即可判断特征的重要性，如上图所示，是我们选择petal_length,petal_width两个特征可视化的结果，可以发现三种标签的分布并没有过多的交织，说明这两个特征对于分类是较为重要的。 4个特征所对应的6个特征对的标签分布如下图所示，由此我们可以大致判断特征的重要性： 六、总结 1）如果距离可以可靠得反映相异性，则KNN是一个简单高效的模型。 2）在数据集大小n→∞n\\rightarrow\\inftyn→∞时，KNNKNNKNN模型将变得非常精确，但是也会变得非常缓慢。 3）当数据维度d&gt;&gt;0d&gt;&gt;0d&gt;&gt;0时，会发生维度灾难 4）优点：①没有关于数据的假设，例如：对非线性数据非常有用 ②算法简单，易于理解 ③具有较高的精度，但是与更好的监督学习模型相比没有竞争力 ④用途广泛，适用于分类以及回归问题 5）缺点：①具有较高的计算成本，因为算法要用到所有的训练数据 ②具有较高的内存要求，需要存储几乎所有的训练数据 ③对无关特征和数据规模较为敏感，存在维度灾难 ","link":"https://2006wzt.github.io/post/机器学习实战（三）：KNN算法/"},{"title":"机器学习实战（二）：监督学习","content":"监督学习 一、形式化定义 在监督学习的过程中，我们输入的训练数据是成对输入的(x,y)(x,y)(x,y)，x∈Rdx\\in R_dx∈Rd​是输入实例，yyy是其标签，整个训练数据表示为： D={(x1,y1),...,(xn,yn)}⊆Rd×CD = \\{(x_1, y_1), . . . ,(x_n, y_n)\\} ⊆ R_d × C D={(x1​,y1​),...,(xn​,yn​)}⊆Rd​×C 其中RdR_dRd​是ddd维特征空间，CCC是标签空间，xix_ixi​是第iii个样本的特征向量，yiy_iyi​是第i个样本的标签 数据点(xi,yi)(x_i,y_i)(xi​,yi​)来自一些未知的分布P(X,Y)P(X,Y)P(X,Y) 最终我们希望学习出一个模型函数hhh，对于一个新的数据点(x,y)(x,y)(x,y)，我们有较高概率的 h(x)=y 或 h(x)≈y~h(x)=y~或~h(x)\\approx y h(x)=y 或 h(x)≈y 监督学习的标签空间决定了问题的类型，典型的标签空间如下： Type Lable Space E.g. 二分类 C={0,1} or C={−1,+1}C = \\{0, 1\\}~or~C = \\{−1, +1\\}C={0,1} or C={−1,+1} E.g. 垃圾邮件过滤问题. 一个邮件要么是垃圾邮件(-1)要么不是(+1) 多分类 C={1,2,⋅⋅⋅,K}(K≥2)C = \\{1, 2, · · · , K\\} (K ≥ 2)C={1,2,⋅⋅⋅,K}(K≥2) E.g. 人脸识别问题.一个人的身份可以是K个身份中的一个 回归 C=RC = RC=R E.g.预测某一天的温度或者某个人的身高 二、损失函数 2.1 定义 什么是损失函数？顾名思义，它是一个用于评估模型对于数据集拟合的损失程度的函数，我们希望预测的效果越差，损失函数的值越大，这也为我们在搭建模型算法的过程中提供了方向：我们要尽可能得使得模型的损失函数输出最小化，以达到较好的拟合效果。 同时在优化过程中，损失函数输出的值变化也可以说明我们的在模型优化上的进展。 事实上，我们可以设计一个非常基本的损失函数来进一步解释它是如何工作的。对于我们所做的每个预测，我们的损失函数将简单地测量预测值和实际值之间的绝对差，即： L(h)=1n∑i=1n∣h(xi)−yi∣L(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| L(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 其中LLL是损失函数，hhh是我们训练出的模型函数，nnn是验证集的样本数量，h(xi)h(x_i)h(xi​)是特征向量xix_ixi​的预测标签，yiy_iyi​则是实际的标签，显然当我们预测完全准确时L(h)=0L(h)=0L(h)=0，预测值与实际值差别越大，损失函数的值越大，这满足损失函数的特性。 2.2 经典的损失函数 2.2.1 Zero-one loss 0-1损失函数的形式化定义如下： L1/0(h)=1n∑i=1nδh(xi)=yi对于分类问题：δh(xi)=yi={1,if h(xi)=yi0,o.w.对于回归问题：δh(xi)=yi={1,if ∣h(xi)−yi∣&gt;t0,o.w.L_{1/0}(h)=\\frac1n\\sum_{i=1}^n\\delta_{h(x_i)\\not=y_i}\\\\ 对于分类问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~h(x_i)\\not=y_i\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right.\\\\ 对于回归问题：\\delta_{h(x_i)\\not=y_i}= \\left\\{ \\begin{aligned} &amp;1,if~|h(x_i)-y_i|&gt;t\\\\ &amp;0,o.w.\\\\ \\end{aligned} \\right. L1/0​(h)=n1​i=1∑n​δh(xi​)​=yi​​对于分类问题：δh(xi​)​=yi​​={​1,if h(xi​)​=yi​0,o.w.​对于回归问题：δh(xi​)​=yi​​={​1,if ∣h(xi​)−yi​∣&gt;t0,o.w.​ 根据δ\\deltaδ函数的定义，当我们的预测值准确时，会给损失函数加上0，对于不准确的预测值则会给损失函数加上1 0-1损失函数也有明显的缺点，对于分类问题影响并不大，但是对于回归问题，ttt是某个自定义的阈值，δ\\deltaδ函数对于所有的误差大于阈值的惩罚相同，即错误的预测所带来的惩罚都为1，对于一些很离谱的预测（比如1预测成了10000）并没有额外的惩罚。 2.2.2 Squared loss 平方损失函数的形式化定义如下： Lsq(h)=1n∑i=1n(h(xi)−yi)2L_{sq}(h)=\\frac1n\\sum_{i=1}^n(h(x_i)-y_i)^2 Lsq​(h)=n1​i=1∑n​(h(xi​)−yi​)2 平方损失函数常用于分类问题，它有两个主要特点： ①损失函数的值永远是非负的 ②损失函数的值与预测的绝对误差呈二次关系 显然平方损失函数规避了0-1损失函数的缺点，对于预测误差较大的样本，所带来的惩罚也越大，但同时对于预测较准确的点，所带来的惩罚也会变得更小，对于噪声数据的处理会使得模型的效果变差。 2.2.3 Absolute loss 绝对损失函数的形式化定义如下： Labs(h)=1n∑i=1n∣h(xi)−yi∣L_{abs}(h)=\\frac1n\\sum_{i=1}^n|h(x_i)-y_i| Labs​(h)=n1​i=1∑n​∣h(xi​)−yi​∣ 绝对损失函数的值随着预测失误而线性增长，因此更适合于噪声数据 三、泛化能力 学习方法的泛化能力，是指由该学习方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。 对于给定的一个损失函数LLL，我们可以得到一个使得损失函数最小化的模型hhh，即： h=argminh∈H L(h)h=\\underset{h\\in H}{argmin}~L(h) h=h∈Hargmin​ L(h) 机器学习的很大一部分集中在这个问题上，即如何有效地进行最小化。如果我们得到一个模型函数h(⋅)h(·)h(⋅)，它在我们的训练数据DDD上的损失很小，我们该如何确定它在DDD之外的数据上的损失也很小呢？这就是泛化的问题，一个泛化能力较差的模型如下： Bad example： h(x)={yi,(xi,yi)∈D∩x=xi0 ,o.w.h(x)=\\left\\{ \\begin{aligned} &amp;y_i,(x_i,y_i)\\in D \\cap x=x_i\\\\ &amp;0~,o.w. \\end{aligned} \\right. h(x)={​yi​,(xi​,yi​)∈D∩x=xi​0 ,o.w.​ 对于这个模型函数，我们在训练的数据集DDD上的误差为0，但是对于数据集DDD外的数据点，显然将会有很大的误差，这就是过拟合问题。 四、过拟合 什么是过拟合？ 在模型的监督学习过程中，我们往往是在训练集上设计模型，在测试集上评估模型的准确性，如上图所示，图1是欠拟合的情况，即模型在训练集上也没有很好的准确性，因此在测试集上的准确性也不会高；图2是较为合适的拟合，它规避了噪声的影响，较为准确得对两类点进行了划分；图3则是过拟合的情况，它最大化了模型在训练集上的准确度而忽略了噪声点的影响，使得模型在训练集上表现得很好，但是在测试集上却表现不佳。 在西瓜书中也有一个比较形象的例子： 五、训练与测试 5.1 定义 训练是使得模型可以学习的过程，而测试则是模型进行预测的过程，训练集的标签是已知的，即可观察的，而测试集的正确标签则是未知的，我们的模型要从已知的特征与标签的对应关系中进行学习，然后对未知的测试集进行预测。 **No free lunch rule：**一个模型hah_aha​即使在某些问题上比另一个模型hbh_bhb​好，也必然存在另一些问题使得hbh_bhb​效果比hah_aha​好 5.2 数据集的划分 对于所提供的数据集，为了训练模型，我们往往需要对数据集DDD进行划分，一般划分成训练集DTRD_{TR}DTR​、验证集DVAD_{VA}DVA​、测试集DTED_{TE}DTE​三部分。 一般的划分比例为80%（DTRD_{TR}DTR​），10%（DVAD_{VA}DVA​），10%（DTED_{TE}DTE​），对于不同的数据集可以进行调整。 为什么我们需要验证集DVAD_{VA}DVA​？ DVAD_{VA}DVA​用于检查从DTRD_{TR}DTR​中获得的模型函数h(⋅)h(\\cdot)h(⋅)是否存在过拟合的问题，也就是我们所追求的目标不只是hhh在训练集DTRD_{TR}DTR​上的损失最小化，也要兼顾在验证集DVAD_{VA}DVA​上的损失最小化，如果h(⋅)h(\\cdot)h(⋅)在DVAD_{VA}DVA​上的损失很大，h(⋅)h(·)h(⋅)将根据DTRD_{TR}DTR​进行修订，并在DVAD_{VA}DVA​上再次验证。该过程将不断来回，直到在DVAD_{VA}DVA​上产生低损失，在验证集上的误差是接近泛化误差的。 在DTRD_{TR}DTR​和DVAD_{VA}DVA​的大小之间有一个权衡：对于较大的DTRD_{TR}DTR​，训练结果会更好，但如果DVAD_{VA}DVA​较大，验证会更可靠（噪音更少）。 对于监督学习实际的应用问题，我们一般只需要在提供的已知标签的数据集上进行训练集与验证集的划分，根据所提供的数据集的特点，有如下的划分规则： ①含有时间成分的数据集：一定要遵循过去预测未来的规则，不能利用未来的数据去预测过去的数据。 ②不含有时间成分的数据集：可以均匀随机得进行划分 利用验证集进行模型评估的常用方法为k折交叉验证，其原理如下图所示： 六、总结 1）学习的过程： Learning:h(⋅)=argminh∈H 1∣DTR∣∑(x,y)∈DTRL(x,y∣h(⋅))Learning:h(\\cdot)=\\underset{h\\in \\mathcal{H}}{argmin}~\\frac1{|D_{TR}|}\\sum_{(x,y)\\in D_{TR}}L(x,y|h(\\cdot)) Learning:h(⋅)=h∈Hargmin​ ∣DTR​∣1​(x,y)∈DTR​∑​L(x,y∣h(⋅)) 2）评估的过程： Evaluation:ϵTE=1∣DTE∣∑(x,y)∈DTEL(x,y∣h(⋅))Evaluation:\\epsilon_{TE}=\\frac{1}{|D_{TE}|}\\sum_{(x,y)\\in D_{TE}}L(x,y|h(\\cdot)) Evaluation:ϵTE​=∣DTE​∣1​(x,y)∈DTE​∑​L(x,y∣h(⋅)) 根据监督学习的上述基本原理，我们将在后续文章中开始相关算法的实战！ ","link":"https://2006wzt.github.io/post/机器学习实战（二）：监督学习/"},{"title":"机器学习实战（一）：机器学习导论","content":"机器学习导论 一、什么是机器学习 机器学习是人工智能的一个分支，涉及算法的设计和开发，允许计算机根据经验数据进化行为。由于智能需要知识，计算机必须获取知识。Tom Mtichell于1997年对机器学习的定义：一个计算机程序A，从经验E中学习关于某个任务T的性能度量P，E有助于提高计算机在任务T上的性能表现，这是基于统计和优化的，而不是基于逻辑的。 二、ML VS CS 机器学习的核心是程序，但是最终目标是结果。以垃圾邮件分类为例，我们所要研究的是进行分类的算法，但是我们的最终目标是邮件的类型。而传统的计算机科学则是根据一定的逻辑规则由输入得到正确的输出。 三、基本分类 3.1 监督学习 在监督学习（supervised learning）的过程中，算法从我们所提供的数据集中进行学习。 监督学习与非监督学习的最大区别在于我们知道用于训练的数据集的正确结果或期望输出，算法对验证集进行预测，由监督者对预测结果进行评估与纠正，进而优化算法，当算法达到可接受的性能水平时，则停止学习。 监督学习有两种主要类型：分类与回归 3.1.1 分类问题 顾名思义，就是根据特征对事物进行分类，一些经典的分类问题例子： ①垃圾邮件过滤问题：根据邮件内容对邮件是否为垃圾邮件进行分类，这是一个二分类问题。 ②人脸识别问题：显然这就是一个多分类问题，根据照片中所包含的数据判断出人脸所对应的身份 3.1.2 回归问题 回归用于预测输入和输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值也随之发生变化，经典的回归问题如下： ①房价预测：根据所提供的相关特征对某个楼盘的房价进行预测 ②天气预测：根据相关指标对未来的天气，例如PM2.5值进行预测 3.2 非监督学习 无监督学习（unsupervised learning）是一类用于在数据中寻找模式的机器学习技术。无监督学习算法使用的输入数据都是没有标注过的，这意味着数据只给出了输入变量（自变量 X）而没有给出相应的输出变量（因变量y）。 在无监督学习中，算法本身将发掘数据中有趣的结构。人工智能研究的领军人物 Yann LeCun，解释道：无监督学习能够自己进行学习，而不需要被显式地告知他们所做的一切是否正确。 非监督学习最为经典的例子便是聚类问题。 3.2.1 聚类问题 顾名思义，聚类就是根据所提供的相关数据，对具有相似特征的事物进行分类，比如照片的类型、网页主题的聚类。 在后期的学习中我们主要关注监督学习的相关算法。 3.3 强化学习 强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优化行为策略的机器学习问题，强化学习的本质是学习最优的序贯决策。 智能系统与环境的互动如下图所示，在每一步t，智能系统从环境中观测到一个状态（state）sts_tst​与一个奖励rtr_trt​，采取一个动作（action）ata_tat​。环境根据只能系统选择的动作，决定下一步t+1的状态st+1s_{t+1}st+1​与奖励rt+1r_{t+1}rt+1​。要学习的策略表示为给定的状态下采取的动作。 智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化，强化学习的过程中系统不断试错，以达到学习最优策略的目的。 3.4 半监督学习 半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。通常有少量标注数据、大量未标注数据，因为标注数据的构成往往需要人工，成本较高，未标注的数据的收集则不需要太多成本，半监督学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。 四、经典的机器学习过程 如下图所示，这是一个属于监督学习的二分类问题的机器学习过程，通过所提供的训练数据使用学习算法训练出模型，再将我们所要预测的不带标签（label）的新数据样本输入到模型中去，进而预测出该样本所属的类别。 在后续的博客中，我将会介绍机器学习中的重要概念与相关算法，还请大家多多支持与关注😄！ ","link":"https://2006wzt.github.io/post/机器学习实战（一）：机器学习导论/"},{"title":"Python入门与实战","content":"Python 一.Python初探 1.Python语言的基本要素 （1）符号和注释 ①符号：需要用英文字符,程序前面不能随便加空格 ②注释：单行注释用#开头，选中部分Ctrl+/可以代码和注释间相互转化 （2）变量 变量有名字，其值可以存储数据 （3）赋值语句 变量=表达式，使变量的值变得跟表达式一样 2.初步认识字符串 （1）字符串初步 字符串可以且必须用单引号，双引号和三引号括起来；当字符串太长时，可以用\\进行分行输出 三双引号字符串中可以包含换行符，制表符等其他字符，双引号中也可以有\\n 有n个字符的字符串，编号从左向右为0-&gt;n-1，从右往左编号为-1-&gt;-n 字符串可以用+号连接 注意字符串不可以修改，在初始化后不可以修改该字符串中任何一个字符 可以用in和not in判断子串 （2）字符串和数的转换 3.简单的输入输出 （1）输入输出初步 输出语句print()，可以输出一项或多项，输出多项时用，隔开，输出效果为用空格隔开，输出后自动换行 有时不想换行可以在print的括号最后加上end=&quot;&quot;,缺省的情况下为end=&quot;\\n&quot; 输入语句x=input(y)，先输出y再等待输入，最后将输入的内容以字符串的形式赋值给x。 要求y一定要是字符串，且赋值输入字符串给x时不带最后的回车 4.初步认识列表 （1）列表初步 列表可以有0到多个元素，并且可以通过下标访问，下标的概念与字符串相同，列表中的元素类型可以不同 可以用in判断列表中是否有某个元素 输入两个整数并求和 二.基本运算、条件分支和输出格式控制 1.算术运算、逻辑运算和分支语句 （1）算术运算 +，-，*，/结果为小数，%取模，//求商结果为整数，**求幂 （2）关系运算符，逻辑运算符，逻辑表达式 ①关系运算符：==,&lt;,&lt;=,&gt;,&gt;=,!=，比较结果为bool值，True或者False，字符串按照字典排序进行比较 ②逻辑运算符：and（与），or（或），not（非），False等于0，True等于1，优先级not&gt;and&gt;or 0，&quot;&quot;(空字符串)，[]（空表）都相当于False；非0数，非空字符串，非空列表都相当于True ③逻辑表达式：逻辑表达式是短路计算的，当表达式的值可以确定时则会停止计算 （3）条件分支语句 字符串切片：若s为一个字符串，则s[x:y]为从下标x到下标y-1的子串 2.输出格式控制 输出格式控制符与C语言的大致相同，且输出格式控制符仅能作用在字符串的输出中 三.循环语句 1.for循环语句 forin: ​ 令variable的值依次是sequence里面的每一个元素，并对每个variable值进行操作statements1 2.break、continue语句 break语句用于跳出循环，continue语句用于直接进入下一层循环，其用法与C语言类似 下为一些for循环语句的例题: 3.while循环语句 4.异常处理 常见的异常有： （1）不合适的转换：int(&quot;abc&quot;),float(&quot;abc&quot;),int(12.34) （2）输入已经结束后还执行input() （3）除法的除数为0 （4）整数和字符串相加 （5）列表下标越界 5.循环综合例题 四.函数和递归 1.函数的概念和用法 函数中的变量：在函数中定义的变量不在函数外起到作用，如果函数内的变量x与全局变量x重名，则如果在函数中未对变量x进行赋值，则认为x为全局变量，反之则为函数内部的变量，在函数内可以用global x声明x为全局变量 python还有很多内置函数，除了前面所讲，还有abs(x)求x绝对值，max(x),min(x)求列表x中元素的的最大最小值，max(x1,x2...),min(x1,x2...)等 2.递归的概念 五.字符串和元组 1.Python变量的指针本质 isinstance函数： python中的变量都是指针，赋值的过程即为指针指向某个内存的过程，列表中的元素也可以进行赋值，因此列表中的每个元素都是指针 is运算符和==运算符的区别： ①a is b 为True代表a，b指向同一个内存 ②a==b代表a，b指向内存中的值相等 2.字符串详解 转义字符：\\与其后面的一个字符一同构成，\\n为换行符，\\t为制表符等等 字符串切片：s[x:y]代表下标x到y-1的子串，字符串切片的方法也适用于元组和列表 字符串的分割：s.split(x)为以x为分隔符，将字符串s分割成一个列表，列表中的每个元素为字符串的子串 字符串的函数： 字符串编码：字符串的编码在内存中的编码是unicode的 字符串的格式化： 3.元组 元组：一个元组由数个逗号分隔的值组成，前后的括号可加可不加 元组与列表的区别在于：元组不可修改，不可增删元素，不可对元素进行赋值，不可修改元素的排序，对元组进行处理的速度比列表快 注意元组的元素的内容可以被修改。 元组的元素本质上是指针，元组的元素不可修改指的是元组元素的指向不可被改变，但指向的内容可修改 元组的下标访问、切片规则以及运算法则与字符串完全相同 元组的运算、迭代和赋值 元组比大小：类似于字符串按照字典顺序和数字大小一一对应进行比较，但是数字于字符串不能比大小，此时会报错 我们可以用列表或元组取代复杂的分支结构 六.列表 1.列表的基本操作 列表是可以增删元素的，并且列表中的元素也可以进行修改 列表的运算 列表的切片 2.列表的排序 选择排序：基础的排序算法，但是效率较低 python自带的排序函数： 对于不同的需求，我们可以自定义比较函数用在sort函数上 3.复杂列表的自定义排序 （1）lambda表达式 lambda x:x[2]，表示一个函数，参数是x，返回值是x[2]，相当于是一个匿名函数 （2）自定义排序 （3）元组的排序 元组的元素不能被修改，因此没有相应的sort函数，但是有sorted函数可以作用于元组，但是返回值是一个列表 4.列表和元组的高级用法 列表的相关函数： 列表映射： 列表过滤： 列表生成式： 二维列表的定义： 列表的拷贝： 列表的深拷贝： 元组和列表的互转： 元组列表和字符串的互转： 5.例题 七.字典和集合 1.字典的基本概念 字典中的每个元素由“键：值”两部分组成，每个元素的键是唯一且不与其他元素的键相同的，可以根据键对元素进行快速查找 键必须是不可变的数据类型，如字符串，整数，小数，元组；而列表，集合，字典等可变数据类型不能作为字典元素的键 字典的增删元素： 字典元素不能重复，如果重复则保留后一个出现的元素： 字典的构造： 2.字典的相关函数 遍历字典： 字典的浅拷贝与深拷贝： 3.字典例题 4.集合 集合的定义同数学上的集合，集合中元素类型可以不同，不会有重复的元素，可以增删元素，集合中的元素类型应为可变数据类型如：字符串，整数，小数，复数，元组，而不能为列表、字典和集合 集合的作用是可以快速地判断某个元素是否在一堆元素里面 集合的构造： 集合常用的函数： 集合的逻辑运算： 集合的比较： 八.文件读写和文件夹操作和数据库 1.文本文件的读写 创建文本文件并写入内容： 读取现有文件： 在文件中添加内容： 2.文本文件的编码 常见文件的编码有gbk和utf-8两种，ANSI对应gbk，创建和读写文件时都可以指定编码，如果不指定则默认为缺省的编码 .py文件必须存成utf-8格式才能运行，如果存成ansi格式（即为gbk格式），应在文件开头写#coding =gbk 3.文件的路径 相对路径：文件路径没有包含盘符 当前文件夹：一般来说.py文件所在的文件夹就是当前文件夹 绝对路径：文件路径指明了盘符 4.文件夹操作 python的文件夹和文件操作函数： 有时文件夹非空但也想要删除文件夹，则不能调用os.rmdir(x)，可以自己定义删除文件夹的函数： 获取文件夹总大小也可以同理自己定义函数： 5.命令行参数 Win+R并输入cmd即可打开控制台，输入python ×××.py即可运行×××.py，注意需要先将控制台转到.py当前文件夹下 当.py文件需要输入参数时，我们希望可以直接在命令行直接输入，可进行如下操作： 6.文件处理实例 7.数据库和SQL语言简介 （1）数据库： 数据库可以存放大量数据，并且提供了方便快捷的检索手段，一个数据库可以是一个文件 一个数据库中可以有多张表，每张表又又不同的字段，字段又有其相应的类型 （2）SQL语言： SQL命令是用于进行数据库操作的标准语句 用sqlite3.exe打开.db文件时，输入select * from 表名;即可显示表中信息，注意一定要加分号 （3）数据库的查询和修改 数据库的查询： 数据库的修改： （4）数据库二进制字段处理 九.正则表达式 1.正则表达式的概念 正则表达式是某些字符有特殊含义的字符串，可以用相关函数用来判断其他字符串是否能与正则表达式进行匹配 正则表达式中的功能字符：\\d等不是转义字符，都是两个字符 正则表达式中的特殊字符：. + ? * ( ) [ ] { } ^ \\ $，如果要在正则表达式中表示这几个字符本身，应该在前面加\\ 2.字符范围和量词 字符的复合： 正则表达式示例： 3.正则表达式的函数 使用正则表达式要import re (1)re.match(pattern,string,flags=0)：从string的起始位匹配一个模式pattern，flags标志位用于控制模式串的匹配方式，如果匹配的上则返回一个匹配对象，否则返回None (2)re.search(pattern,string,flags=0)：查找字符串中第一个可以匹配成功的子串，查找成功返回匹配对象，否则返回None (3)re.findall(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个列表，如果匹配失败则返回空表 (4)re.finditer(pattern,string,flags=0)：查找字符串中所有可以匹配成功的子串，返回一个序列，序列中的每个元素都是一个匹配对象 4.边界符号 边界符号本身不与任何字符匹配 5.分组 括号中的一个表达式就是分组，多个分组按照左括号，从左到右从1开始编号 在分组的右边可以通过分组的编号引用该分组所匹配的子串 分组作为一个整体，后面可以跟量词，且不一定需要匹配相同的字符串 当正则表达式中没有分组时，re.findall返回所有匹配子串构成的列表 当正则表达式有且仅有一个分组时，re.findall返回的是一个子串的列表，每个元素是一个匹配子串中分组对应的内容 当正则表达式中有超过一个分组时，re.findall返回的是一个元组的列表，元组中的每个元素依次是各个分组的内容 6.|的用法 |表示或，如果没有放在分组中，|的起作用范围是直到整个正则表达式开头或结尾的另一个”|“ |从左到右短路匹配，当匹配到左边的后不会再匹配右边 |也可以用在分组中，起作用的范围就仅限于分组 7.贪婪模式和懒惰模式 （1）贪婪模式 量词+,*,?,{m,n}都默认匹配尽可能长的子串，存在很明显的弊端 （2）懒惰模式 即为非贪婪模式，在量词+,*,?,{m,n}后面加上？则匹配尽可能短的子串 8.匹配对象的函数 十.玩转Python生态 1.用datatime库处理日期、时间 处理日期： 处理时刻： 2.用random库处理随机事务 设置随机数种子：如果没有设置则是按照系统时间作为随机数种子，随机数种子设定后每次随机结果都是唯一的 random库应用实例： 3.用jieba库进行分词和中文词频统计 不仅可以将一个词加入jieba的字典，也可以将一个文件中的词加入jieba的字典 4.用openpyxl库处理excel文档 （1）用openpyxl库读取excel文档 有时候单元格中的内容是公式，希望打开文档的时候自动计算公式的值，则可进行如下操作： （2）用openpyxl库创建excel文档 （3）用openpyxl库设定excel文档单元格样式 4.用Pillow处理图像 （1）图像基本常识： 图像由像素构成：屏幕上每个像素点由三个距离非常近的点构成，分别显示红绿蓝三种颜色，每个像素可以由一个元组表示(r,g,b)， r，g，b通常是不大于255的整数，分别表示红绿蓝三原色的深浅程度 图像模式： ①RGB：一个像素有红绿蓝三个分量 ②RGBA：一个像素有红绿蓝三个分量以及透明分量 ③CYMK：一个像素有青色（Cyan），洋红色（Magenta），黄色（Yellow），黑色（K代表黑）四个分量构成，每个像素用元组(c,y,m,k)表示，对应于彩色打印机或者印刷机的四种颜色墨水 L：黑白图像，像素就是一个整数，代表灰度 （2）图像的基本操作： ①图像的缩放： ②图像的旋转、翻转和滤镜效果： ③图像的裁剪： ④图像素描化： ⑤为图像添加水印： 原理：在将一个图像粘贴到另一个图像上时，会用到paste函数，paste函数还有一个参数mask称为“掩膜”指定img的每个像素粘贴过去的透明度，如果透明度为0则完全透明，如果透明度为255则完全遮盖原图像的像素 mask本质上是一个模式为“L”的图片即一个Image对象 十一.数据分析和展示 1.numpy库的使用 numpy是一个多维数组库，创建多维数组很方便，可以代替多维列表，速度比多维列表快，支持向量和矩阵的各种数学运算，所有元素类型必须相同 （1）用numpy库创建数组 （2）numpy常用的属性与函数 （3）numpy数组元素增删 numpy数组一旦生成，则不能增删元素，此处的增删指的是增删之后生成一个新数组，原数组保持不变 numpy增添数组元素： numpy删除数组元素： （4）在numpy数组中查找元素 （5）numpy数组的数学运算 （6）numpy数组的切片 不同于列表的切片，numpy数组的切片是“视图”，即为原数组的一部分，而非原数组一部分的拷贝，切片改变原数组也改变 2.数据分析库pandas pandas的核心功能是在二维表格上做各种操作，需要numpy库的支持，在openpyxl库的支持下还可以读取excel文档 pandas中最为关键的类是DataFrame，用于表示二维表格 （1）pandas的重要类Series Series为一维表格，每个元素带有标签与下标，类似于列表和字典的结合 （2）DataFrame的构造和访问 DataFrame是带行列标签的二维表格，它的每一行都是一个Series （3）DataFrame的切片、增删和统计 ①DataFrame的切片： ②DataFrame的分析统计： ③DataFrame的修改和增删： 修改和增加： 删除： （4）用pandas库读取excel和csv文档 需要依赖于openpyxl库 ①用pandas读excel文档：读取的每张工作表都是一个DataFrame ②用pandas写excel文档 3.用matplotlib进行数据展示 在此感慨：清华镜像源yyds！！！ （1）绘制直方图 （2）绘制堆叠直方图 （3）绘制对比直方图 （4）绘制折线图和散点图 （5）绘制饼图 （6）绘制热力图 热力图用于展示二维数据： （7）绘制雷达图 （8）一个窗口绘制多幅子图 十二.网络爬虫设计 1.爬虫的用途和原理 1）爬虫的用途： ①在网络上搜集数据（搜索引擎） ②模拟浏览器快速操作（如抢票，选课） ③模拟浏览器操作，替代填表等重复操作 2）最基本的爬虫写法：数据获取型爬虫的本质就是自动获取网页并抽取其中的内容 ①手工找出合适的url（网址） ②用浏览器手工查找url对应的网页，并查看网页源码，找出包含想要内容的字符串的模式 ③在程序中获取url对应的网页 ④在程序中用正则表达式或BeautifulSoup库抽取网页中想要的内容并保存 上述代码原来if语句写的是： if not(x.lower().endswith(&quot;.jpg&quot;) or x.lower().endswith(&quot;.jpeg&quot;) or x.lower().endswith(&quot;.png&quot;)): continue#只取.png和.jpg图片 但是发现爬取失败，经过查看各个图片网址可以发现，各个图片网址格式为：u=623374478,4175757280&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG (500×658) (baidu.com) 因此并不能正确得爬取图片，因此对if语句进行了修改，查找字符串中得jpeg，jpg和png用于判断是否进行爬取 同时文件名的处理也做了相应的修改，最终爬取成功！！！ 局限是requests库获取网络资源很容易被反爬虫，且不能获取JavaScript编写的动态网页源代码，可以尝试pyppeteer库，速度快且暂未被许多网站反爬 2.用pyppeteer库获取网页 pyppeteer的工作原理： ①启动一个浏览器Chromium，用浏览器装入网页 ②从浏览器可以获取网页源代码，如果网页有JavaScript程序，获取到的是JavaScript被浏览器执行后的网页源代码 ③可以向浏览器发出命令，模拟用户在浏览器上的键盘输入、鼠标点击等操作，让浏览器转到其他网页 十三.面向对象程序设计 1.类和对象的概念 类：类是用来代表事物的，对于一种事物，可以用一个类来概括其属性 对象：类的实例称为对象，类主要用于将数据和操作数据的函数捆绑在一起，便于当作一个整体使用 2.对象的比较 python中所有的类都有eq,ne等方法用于比较（注意双下划线） 自定义对象的比较： 3.继承与派生 定义一个新类B时，如果发现B类具有A类的全部特点，则可以在定义B类时以A为基类，定义B类为A类的派生类 object类：python中所有类都是object类的派生类，因而具有object类的各种属性和方法 4.静态属性和静态方法 静态属性：静态属性被所有对象所共享，一共只有一份 静态方法：静态方法不是作用在具体的某个对象上，因此不能访问非静态属性 静态属性和静态方法这种机制存在的目的就是少写全局变量和全局函数 5.对象作为集合元素或字典的键 （1）可哈希：可哈希的东西才可以作为字典的键和集合的元素，hash(x)有定义即hash(x)有返回值，说明x是可哈希的 （2）哈希值和字典、集合的关系 字典和集合都是一种称为”哈希表“的数据结构，根据元素的哈希值为元素寻找存放的槽，哈希值可以看作是槽编号，一个槽中可以放多个哈希值相同的元素。 两个对象的哈希值不同可以作为同一集合的不同元素和同一字典的不同键，当hash(a)==hash(b)但是a!=b时，a，b也可以作为同一集合的不同元素和同一字典的不同键，自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址），而非对象的值，可以重写自定义类的hash方法 若 dt 是个字典，dt[x] 计算过程如下： 根据hash(x)去找x应该在的槽的编号 如果该槽没有元素，则认为dt中没有键为x的元素 如果该槽中有元素，则试图在槽中找一个元素y，使得 y的键 == x。 如果找到，则dt[x] 即为 y的值，如果找不到，则dt[x]没定义，即 认为dt中不存在键为x的元素 （3）自定义类的对象是否可哈希 自定义类的对象在默认情况下哈希值的计算是根据对象的id计算的（即存储地址） 如果为自定义的类重写了eq(self,other)成员函数，则其 hash成员函数会被自动设置为None。这种情况下，该类就变成不可哈希的 一个自定义类，只有在重写了eq方法却没有重写hash方法的情况下，才是不可哈希的。 十四.tkinter图形界面程序设计 1.控件概述 控件(widgets)： 按钮、列表框、单选框、多选框、编辑框.... 布局 ：如何将控件摆放在窗口上合适的位置 事件响应： 对鼠标点击、键盘敲击、控件被点击等操作进行响应 对话框 ：弹出一个和用户交互的窗口接受一些输入 tkinter的扩展控件： from tkinter import ttk，tk中的控件ttk都有且更加美观，并且ttk中还有一些tk不具有的控件 2.布局基础 用grid进行布局，grid布局在窗口上布置网格，控件放在网格单元里面居中摆放 默认情况下的grid规则： 1）一个单元格只能放一个控件，控件在单元格中居中摆放。 2）不同控件高宽可以不同，因此网格不同行可以不一样高，不同列也 可以不一样宽。但同一行的单元格是一样高的，同一列的单元格也 是一样宽的。 3）一行的高度，以该行中包含最高控件的那个单元格为准。单元格的 高度，等于该单元格中摆放的控件的高度（控件如果有上下留白， 还要加上留白的高度）。列宽度也是类似的处理方式。 4）若不指定窗口的大小和显示位置，则窗口大小和网格的大小一样， 即恰好能包裹所有控件；显示位置则由Python自行决定。 3.使用Frame控件进行布局 1）控件多了，要算每个控件行、列、rowspan,columnspan很麻烦 2）Frame控件上面还可以摆放控件，可以当作底板使用 3）可以在Frame控件上面设置网格进行Grid布局，摆放多个控件 4.控件属性和事件响应 （1）控件属性 1）有的控件有函数可以用来设置和获取其属性，或以字典下标的形式获取和设置其属性 2）有的控件必须和一个变量相关联，取变量值或设置变量值，就是取或设置该控件的属性 （2）事件响应 1）创建有些控件时，可以用command参数指定控件的事件响应函数 2）可以用控件的bind函数指定事件响应函数 示例： 5.Python实例：火锅店点菜系统 十五.Python游戏设计 外星人入侵主要依靠pygame库进行设计，在此仅附上代码 1.aline_invasion.py 2.game_stats.py 3.scoreboard.py 4.button.py 5.aline.py 6.ship.py 7.settings.py 8.bullet.py 十六.用opencv进行人脸识别 1.读取图片、灰度转换、修改尺寸 2.绘制图形 3.人脸检测 详解detectMultiScale： 此方法的任务是检测不同大小的对象，并返回矩形的列表。 第1个参数img需要是灰度图片 第2个参数scaleFactor，很重要 第3个参数minNeighbors，很重要 第4个参数flag=0即可 第5，6个参数minSize和maxSize 设置检测对象的最大最小值，低于minSize和高于maxSize的话就不会检测出来。参数类型为二元组，指定矩形的最小范围和最大范围 scaleFactor默认为1.1，Haar cascade的工作原理是一种“滑动窗口”的方法，通过在图像中不断的“滑动检测窗口”来匹配人脸。 因为图像的像素有大有小，图像中的人脸因为远近不同也会有大有小，所以需要通过scaleFactor参数设置一个缩小的比例，对图像进行逐步缩小来检测，这个参数设置的越大，计算速度越快，但可能会错过了某个大小的人脸。可以根据图像的像素值来设置此参数，像素大缩小的速度就可以快一点，通常在1~1.5之间。 minNeighbors默认为3，指定每个候选矩形有多少个“邻居”，指定每个候选矩形有多少个“邻居”，也可以理解为检测次数，只有检测minNeighbors次，某处都识别为人脸才会显示出来矩形 4.视频人脸检测 5.人脸录入 6.数据训练 7.人脸识别 十七.scikit-learn实现机器学习 1.机器学习介绍及其原理 ①人工智能：就其本质而言，是及其对人思维信息过程的模拟，让它能像人一样去思考 人工智能可以根据输入信息进行模型结构，权重更新，实现最终优化 特点：信息处理，自我学习，优化升级 ②人工智能的核心方法：机器学习，深度学习 机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术 机器学习：使用算法来解析数据，从中学习，然后对真实世界中的事件进行决策和预测 深度学习：模仿人类的神经网络，建立模型，进行数据分析 ③机器学习分类： 监督式学习：基于数据及结果进行预测 非监督式学习：从数据中挖掘关联性 强化学习：强调如何基于环境而行动以获取最大利益 ④监督式学习核心步骤： 1.使用标签数据训练机器学习模型 ●&quot;标签数据&quot; 是指由输入数据对应的正确的输出结果 ●&quot;机器学习模型&quot;将学习输入数据与之对应的输出结果间的函数关系 2.调用训练好的机器学习模型,根据新的输入数据预测对应的结果 相比于监督式学习,非监督式学习不需要标签数据,而是通过引入预先设定的优化准则进行模型训练,比如自动将数据分为三类 2.机器学习开发环境部署 1）Scikit-learn：是python语言中专门针对机器学习应用而发展起来的开源框架，可以实现数据预处理，分类，回归，降维，模型选择等常用的机器学习算法 2）Jupyter notebook 3）Anaconda 3.机器学习实现之数据预处理 1）Iris数据集： 2）使用scikit-learn进行数据处理的四个关键点 ①区分开属性数据和结果数据 ②属性数据和结果数据都是量化的 ③运算过程中，属性数据与结果数据的类型都是numpy数组 ④属性数据与结果数据的维度是对应的，即行数应该对应 4.机器学习实现之模型训练 1）分类：根据数据集目标的特征或属性，划分到已有的类别中 2）常用的算法：K近邻（KNN），逻辑回归，决策树，朴素贝叶斯 3）K近邻分类模型（KNN）：给定一个训练数据集，对新的输入实例，在训练的数据集中找到与该实例最邻近的K个实例，这个K实例的多数属于某一类，就把该输入实例归到这个类中 4）使用scikit-learn进行建模的四个步骤： ①调用需要使用的模型类 ②模型初始化：创建一个模型实例 ③模型训练 ④模型预测 5.机器学习实现之模型评估 1）为了对模型的准确率进行评估，我们不能用全部的数据进行训练，因为这样无法判断模型的准确性，不妨将数据集分为训练集与测试集两部分，一部分用于训练，一部分用于测试，这样便于对模型进行评估，下面是不进行数据分离的结果，显然K=1时，正确率100% 2）下面进行数据分离然后进行模型评估 3）确定最合适的K值，遍历所有可取的K值 K值越小，模型越复杂，并且因为每次数据分离是随机的，训练出的结果也不相同 6.逻辑回归模型 1）逻辑回归模型：用于解决分类问题的一种模型。根据数据特征或属性,计算其归属于某一类别的概率P(x) ,根据概率数值判断其所属类别。主要应用场景: 二分类问题。 2）皮马印第安人糖尿病数据集 注意数据集的特点是有缺失数据 3）使用准确率进行模型评估的局限性：没有体现数据的实际分布情况，没有体现模型错误预测的类型 空准确率：当模型总是预测比较高的类别，其预测准确率的数值，即比例较高的类别在模型中所占的比例 4）混淆矩阵 比F1更具一般形式的Fβ′:Fβ′=(1+β2)×P×R(β2×P)+R①β=1:标准的F1②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2\\begin{aligned} &amp;比F_1更具一般形式的F_{\\beta&#x27;}:\\\\ &amp;F_{\\beta&#x27;}=\\frac{(1+\\beta^2)\\times P\\times R}{(\\beta^2\\times P)+R}\\\\ &amp;①\\beta=1:标准的F_1\\\\ &amp;②\\beta&lt;1:偏精确率P\\\\ &amp;③\\beta&gt;1:偏召回率R\\\\ &amp;\\beta最常用的值为0.5，1，2 \\end{aligned} ​比F1​更具一般形式的Fβ′​:Fβ′​=(β2×P)+R(1+β2)×P×R​①β=1:标准的F1​②β&lt;1:偏精确率P③β&gt;1:偏召回率Rβ最常用的值为0.5，1，2​ 计算混淆矩阵进行模型评估 ","link":"https://2006wzt.github.io/post/Python入门与实战/"}]}